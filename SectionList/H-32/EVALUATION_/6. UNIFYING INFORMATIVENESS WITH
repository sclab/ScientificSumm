INTERESTINGNESS
We have thus far been comparing the Human Interest Model
against the Soft-Pattern model in order to understand the 
differences between interesting and informative nuggets. However from
the perspective of a human reader, both informative and interesting
nuggets are useful and definitional. Informative nuggets present a
general overview of the topic while interesting nuggets give 
readers added depth and insight by providing novel and unique aspects
about the topic. We believe that a good definitional question 
answering system should provide the reader with a combined mixture
of both nugget types as a definitional answer set.
Rank PERSON ORG THING EVENT
Baseline
Unigram Weighting Scheme, N+W+S+M
0.3279 0.3630 0.2551 0.2644
1
N+S+M W+S W+M N+M
0.3584 0.3709 0.2688 0.2905
2
N+S N+W+S W+S+M N+S+M
0.3469 0.3702 0.2665 0.2745
3
N+M N+W+S+M W+S N+S
0.3431 0.3680 0.2616 0.2690
Table 2: Top 3 runs using different web resources for each 
entity class
We now have two very different experts at identifying 
definitions. The Soft Pattern Bigram Model proposed by Cui et al. is
an expert in identifying informative nuggets. The Human 
Interest Model we have described in this paper on the other hand is an
expert in finding interesting nuggets. We had initially hoped to
unify the two separate definitional question answering systems by
applying an ensemble learning method [5] such as voting or 
boosting in order to attain a good mixture of informative and interesting
nuggets in our answer set. However, none of the ensemble 
learning methods we attempted could outperform our Human Interest
Model.
The reason is that both systems are picking up very different
sentences as definitional answers. In essence, our two experts are
disagreeing on which sentences are definitional. In the top 10 
sentences from both systems, only 4.4% of these sentences appeared in
both answer sets. The remaining answers were completely 
different. Even when we examined the top 500 sentences generated by
both systems, the agreement rate was still an extremely low 5.3%.
Yet, despite the low agreement rate between both systems, each
individual system is still able to attain a relatively high F3 score.
There is a distinct possibility that each system may be selecting
different sentences with different syntactic structures but actually
have the same or similar semantic content. This could result in both
systems having the same nuggets marked as correct even though the
source answer sentences are structurally different. Unfortunately,
we are unable to automatically verify this as the evaluation software
we are using does not report correctly identified answer nuggets.
To verify if both systems are selecting the same answer nuggets,
we randomly selected a subset of 10 topics from the TREC 2005
question set and manually identified correct answer nuggets (as 
defined by TREC accessors) from both systems. When we compared
the answer nuggets found by both system for this subset of topics,
we found that the nugget agreement rate between both systems was
16.6%. While the nugget agreement rate is higher than the 
sentence agreement rate, both systems are generally still picking up
different answer nuggets. We view this as further indication that
definitions are indeed made up of a mixture of informative and 
interesting nuggets. It is also indication that in general, interesting
and informative nuggets are quite different in nature.
There are thus rational reasons and practical motivation in 
unifying answers from both the pattern based and corpus based 
approaches. However, the differences between the two systems also
cause issues when we attempt to combine both answer sets. 
Currently, the best approach we found for combining both answer sets
is to merge and re-rank both answer sets with boosting agreements.
We first normalize the top 1,000 ranked sentences from each
system, to obtain the Normalized Human Interest Model score,
him(s), and the Normalized Soft Pattern Bigram Model score,
sp(s), for every unique sentence, s. For each sentence, the two 
separate scores for are then unified into a single score using Equation 5.
When only one system believes that the sentence is definitional, we
simply retain that system"s normalized score as the unified score.
When both systems agree agree that the sentence is definitional,
the sentence"s score is boosted by the degree of agreement between
between both systems.
Score(s) = max(shim, ssp)1âˆ’min(shim,ssp)
(5)
In order to maintain a diverse set of answers as well as to 
ensure that similar sentences are not given similar ranking, we 
further re-rank our combined list of answers using Maximal Marginal
Relevance or MMR [2]. Using the approach described here, we
achieve a F3 score of 0.3081. This score is equivalent to the initial
Human Interest Model score of 0.3031 but fails to outperform the
optimized Human Interest Model model.
