The relationship between frequencies, probabilities and
information theory (entropy) has been the focus of many
researchers. In this background section, we focus on work
that investigates the application of the Poisson distribution
in IR since a main part of the work presented in this paper
addresses the underlying assumptions of Poisson.
[4] proposes a 2-Poisson model that takes into account
the different nature of relevant and non-relevant documents,
rare terms (content words) and frequent terms (noisy terms,
function words, stopwords). [9] shows experimentally that
most of the terms (words) in a collection are distributed
according to a low dimension n-Poisson model. [10] uses a
2-Poisson model for including term frequency-based 
probabilities in the probabilistic retrieval model. The non-linear
scaling of the Poisson function showed significant 
improvement compared to a linear frequency-based probability. The
Poisson model was here applied to the term frequency of a
term in a document. We will generalise the discussion by
pointing out that document frequency and term frequency
are dual parameters in the collection space and the 
document space, respectively. Our discussion of the Poisson 
distribution focuses on the document frequency in a collection
rather than on the term frequency in a document.
[7] and [6] address the deviation of idf and Poisson, and
apply Poisson mixtures to achieve better Poisson-based 
estimates. The results proved again experimentally that a 
onedimensional Poisson does not work for rare terms, therefore
Poisson mixtures and additional parameters are proposed.
[3], section 3.3, illustrates and summarises 
comprehensively the relationships between frequencies, probabilities
and Poisson. Different definitions of idf are put into 
context and a notion of noise is defined, where noise is viewed
as the complement of idf . We use in our paper a different
notion of noise: we consider a frequency-based noise that
corresponds to the document frequency, and we consider a
term noise that is based on the independence of document
events.
[11], [12], [8] and [1] link frequencies and probability 
estimation to information theory. [12] establishes a framework
in which information retrieval models are formalised based
on probabilistic inference. A key component is the use of a
space of disjoint events, where the framework mainly uses
terms as disjoint events. The probability of being 
informative defined in our paper can be viewed as the probability
of the disjoint terms in the term space of [12].
[8] address entropy and bibliometric distributions. 
Entropy is maximal if all events are equiprobable and the 
frequency-based Lotka law (N/iλ
is the number of scientists
that have written i publications, where N and λ are 
distribution parameters), Zipf and the Pareto distribution are 
related. The Pareto distribution is the continuous case of the
Lotka and Lotka and Zipf show equivalences. The Pareto
distribution is used by [2] for term frequency normalisation.
The Pareto distribution compares to the Poisson 
distribution in the sense that Pareto is fat-tailed, i. e. Pareto 
assigns larger probabilities to large numbers of events than
Poisson distributions do. This makes Pareto interesting
since Poisson is felt to be too radical on frequent events.
We restrict in this paper to the discussion of Poisson, 
however, our results show that indeed a smoother distribution
than Poisson promises to be a good candidate for improving
the estimation of probabilities in information retrieval.
[1] establishes a theoretical link between tf-idf and 
information theory and the theoretical research on the meaning
of tf-idf clarifies the statistical model on which the different
measures are commonly based. This motivation matches
the motivation of our paper: We investigate theoretically
the assumptions of classical idf and Poisson for a better
understanding of parameter estimation and combination.
