In this section we illustrate how our cooperative, context aware
approach can combine the technical building blocks described in
the previous section to realize live server migration across a wide
area network. We demonstrate how the coordination of server 
virtualization and migration technologies, the storage replication 
subsystem and the network can achieve live migration of the entire data
center across the WAN. We utilize different scenarios to 
demonstrate our approach. In Section 3.1 we outline how our approach
can be used to achieve the safe live migration of a data center when
planned maintenance events are handled. In Section 3.2 we show
the use of live server migration to mitigate the effects of unplanned
outages or failures.
3.1 Maintenance Outages
We deal with maintenance outages in two parts. First, we 
consider the case where the service has no (or very limited) storage
requirements. This might for example be the case with a network
element such as a voice-over-IP (VoIP) gateway. Second, we deal
with the more general case where the service also requires the 
migration of data storage to the new data center.
Without Requiring Storage to be Migrated: Without storage to
be replicated, the primary components that we need to coordinate
are the server migration and network mobility. Figure 1 shows
the environment where the application running in a virtual server
VS has to be moved from a physical server in data center A to a
physical server in data center B.
Prior to the maintenance event, the coordinating migration 
management system (MMS) would signal to both the server 
management system as well as the network that a migration is imminent.
The server management system would initiate the migration of the
virtual server from physical server a (¢¤£¦¥ ) to physical server b
(¢¤£¦§ ). After an initial bulk state transfer as preparation for 
migration, the server management system will mirror any state changes
between the two virtual servers.
Similarly, for the network part, based on the signal received
from the MMS, the service provider edge (¢©¨ ) router will 
initiate a number of steps to prepare for the migration. Specifically,
as shown in Figure 1(b), the migration system will cause the 
network to create a tunnel between ¢©¨ and ¢©¨ which will be used
subsequently to transfer data destined to VS to data center B.
When the MMS determines a convenient point to quiesce the VS,
another signal is sent to both the server management system and
the network. For the server management system, this signal will
indicate the final migration of the VS from data center A to data
center B, i.e., after this the VS will become active in data center B.
For the network, this second signal enables the network data path to
switchover locally at ¢©¨©¥ to the remote data center. Specifically,
from this point in time, any traffic destined for the virtual server
address that arrives at ¢©¨©¥ will be switched onto the tunnel to
¢©¨©§ for delivery to data center B.
Note that at this point, from a server perspective the migration is
complete as the VS is now active in data center B. However, traffic
is sub-optimally flowing first to ¢©¨©¥ and then across the tunnel
to ¢©¨¤§ . To rectify this situation another networking step is 
involved. Specifically, ¢©¨©§ starts to advertise a more preferred route
to reach VS, than the route currently being advertised by ¢©¨¤¥ . In
this manner, as ingress PEs to the network (¢©¨¤ to ¢©¨¤ in 
Figure 1) receive the more preferred route, traffic will start to flow to
¢©¨©§ directly and the tunnel between ¢©¨©¥ and ¢©¨©§ can be torn
down leading to the final state shown in Figure 1(c).
Requiring Storage Migration: When storage has to also be 
replicated, it is critical that we achieve the right balance between 
performance (impact on the application) and the recovery point or
data loss when the switchover occurs to the remote data center. To
achieve this, we allow the storage to be replicated asynchronously,
prior to any initiation of the maintenance event, or, assuming the
amount of data to be transfered is relatively small, asynchronous
replication can be started in anticipation of a migration that is 
expected to happen shortly. Asynchronous replication during this
initial phase allows for the application to see no performance 
impact. However, when the maintenance event is imminent, the MMS
would signal to the replication system to switch from asynchronous
replication to synchronous replication to ensure that there is no
loss of data during migration. When data is being replicated 
synchronously, there will be a performance impact on the application.
264
Figure 1: Live server migration across a WAN
This requires us to keep the exposure to the amount of time we
replicate on a synchronous basis to a minimum.
When the MMS signals to the storage system the requirement
to switch to synchronous replication, the storage system completes
all the pending asynchronous operations and then proceeds to 
perform all the subsequent writes by synchronously replicating it to
the remote data center. Thus, between the server migration and
synchronous replication, both the application state and all the 
storage operations are mirrored at the two environments in the two data
centers. When all the pending write operations are copied over,
then as in the previous case, we quiesce the application and the
network is signaled to switch traffic over to the remote data center.
From this point on, both storage and server migration operations
are complete and activated in data center B. As above, the network
state still needs to be updated to ensure optimal data flow directly
to data center B.
Note that while we have described the live server migration 
process as involving the service provider for the networking part, it
is possible for a data center provider to perform a similar set of
functions without involving the service provider. Specifically, by
creating a tunnel between the customer edge (CE) routers in the
data center, and performing local switching on the appropriate CE,
rather than on the PE, the data center provider can realize the same
functionality.
3.2 Unplanned Outages
We propose to also use cooperative, context aware migration to
deal with unplanned data center outages. There are multiple 
considerations that go into managing data center operations to plan
and overcome failures through migration. Some of these are: (1)
amount of overhead under normal operation to overcome 
anticipated failures; (2) amount of data loss affordable (recovery point
objective - RPO); (3) amount of state that has to be migrated; and
(4) time available from anticipated failure to occurrence of event.
At the one extreme, one might incur the overhead of completely
mirroring the application at the remote site. This has the 
consequence of both incurring processing and network overhead under
normal operation as well as impacting application performance 
(latency and throughput) throughout. The other extreme is to only 
ensure data recovery and to start a new copy of the application at the
remote site after an outage. In this case, application memory state
such as ongoing sessions are lost, but data stored on disk is 
replicated and available in a consistent state. Neither this hot standby
nor the cold standby approach described are desirable due to the
overhead or the loss of application memory state.
An intermediate approach is to recover control and essential state
of the application, in addition to data stored on disk, to further 
minimize disruptions to users. A spectrum of approaches are possible.
In a VoIP server, for instance, session-based information can be
mirrored without mirroring the data flowing through each session.
More generally, this points to the need to checkpoint some 
application state in addition to mirroring data on disk. Checkpointing 
application state involves storing application state either periodically
or in an application-aware manner like databases do and then 
copying it to the remote site. Of course, this has the consequence that
the application can be restarted remotely at the checkpoint 
boundary only. Similarly, for storage one may use asynchronous 
replication with a periodic snapshot ensuring all writes are up-to-date
at the remote site at the time of checkpointing. Some data loss
may occur upon an unanticipated, catastrophic failure, but the 
recovery point may be fairly small, depending on the frequency of
checkpointing application and storage state. Coordination between
265
the checkpointing of the application state and the snapshot of 
storage is key to successful migration while meeting the desired RPOs.
Incremental checkpointing of application and storage is key to 
efficiency, and we see existing techniques to achieve this [4, 3, 11].
For instance, rather than full application mirroring, a virtualized
replica can be maintained as a warm standby-in dormant or
hibernating state-enabling a quick switch-over to the previously
checkpointed state. To make the switch-over seamless, in addition
to replicating data and recovering state, network support is needed.
Specifically, on detecting the unavailability of the primary site, the
secondary site is made active, and the same mechanism described
in Section 3.1 is used to switch traffic over to reach the secondary
site via the pre-established tunnel. Note that for simplicity of 
exposition we assume here that the PE that performs the local switch
over is not affected by the failure. The approach can however, 
easily be extended to make use of a switchover at a router deeper in
the network.
The amount of state and storage that has to be migrated may vary
widely from application to application. There may be many 
situations where, in principle, the server can be stateless. For example,
a SIP proxy server may not have any persistent state and the 
communication between the clients and the proxy server may be using
UDP. In such a case, the primary activity to be performed is in
the network to move the communication over to the new data 
center site. Little or no overhead is incurred under normal operation to
enable the migration to a new data center. Failure recovery involves
no data loss and we can deal with near instantaneous, catastrophic
failures.
As more and more state is involved with the server, more 
overhead is incurred to checkpoint application state and potentially
to take storage snapshots, either periodically or upon application
prompting. It also means that the RPO is a function of the 
interval between checkpoints, when we have to deal with instantaneous
failures. The more advanced information we have of an impending
failure, the more effective we can be in having the state migrated
over to the new data center, so that we can still have a tighter RPO
when operations are resumed at the new site.
