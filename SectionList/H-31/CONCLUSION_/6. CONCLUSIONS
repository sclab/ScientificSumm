We present a new family of query generation language
models for retrieval based on Poisson distribution. We 
derive several smoothing methods for this family of models,
including single-stage smoothing and two-stage smoothing.
We compare the new models with the popular multinomial
retrieval models both analytically and experimentally. Our
analysis shows that while our new models and multinomial
models are equivalent under some assumptions, they are
generally different with some important differences. In 
particular, we show that Poisson has an advantage over 
multinomial in naturally accommodating per-term smoothing. We
exploit this property to develop a new per-term smoothing
algorithm for Poisson language models, which is shown to
outperform term-independent smoothing for both Poisson
and multinomial models. Furthermore, we show that a 
mixture background model for Poisson can be used to improve
the performance and robustness over the standard Poisson
background model. Our work opens up many interesting 
directions for further exploration in this new family of models.
Further exploring the flexibilities over multinomial language
models, such as length normalization and pseudo-feedback
could be good future work. It is also appealing to find 
robust methods to learn the per-term smoothing coefficients
without additional computation cost.
