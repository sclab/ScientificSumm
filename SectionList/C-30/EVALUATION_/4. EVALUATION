We have evaluated Bullet"s performance in real Internet
environments as well as the ModelNet [37] IP emulation
framework. While the bulk of our experiments use 
ModelNet, we also report on our experience with Bullet on the
PlanetLab Internet testbed [31]. In addition, we have 
implemented a number of underlying overlay network trees
upon which Bullet can execute. Because Bullet performs
well over a randomly created overlay tree, we present 
results with Bullet running over such a tree compared against
an oﬄine greedy bottleneck bandwidth tree algorithm using
global topological information described in Section 4.1.
All of our implementations leverage a common 
development infrastructure called MACEDON [33] that allows for
the specification of overlay algorithms in a simple 
domainspecific language. It enables the reuse of the majority of
common functionality in these distributed systems, 
including probing infrastructures, thread management, message
passing, and debugging environment. As a result, we 
believe that our comparisons qualitatively show algorithmic
differences rather than implementation intricacies. Our 
implementation of the core Bullet logic is under 1000 lines of
code in this infrastructure.
Our ModelNet experiments make use of 50 2Ghz 
Pentium4"s running Linux 2.4.20 and interconnected with 100 Mbps
and 1 Gbps Ethernet switches. For the majority of these
experiments, we multiplex one thousand instances (overlay
participants) of our overlay applications across the 50 Linux
nodes (20 per machine). In ModelNet, packet transmissions
are routed through emulators responsible for accurately 
emulating the hop-by-hop delay, bandwidth, and congestion of
a network topology. In our evaluations, we used four 1.4Ghz
Pentium III"s running FreeBSD-4.7 as emulators. This 
platform supports approximately 2-3 Gbps of aggregate 
simultaneous communication among end hosts. For most of our
ModelNet experiments, we use 20,000-node INET-generated
topologies [10]. We randomly assign our participant nodes
to act as clients connected to one-degree stub nodes in the
topology. We randomly select one of these participants to
act as the source of the data stream.
Propagation delays in the network topology are calculated
based on the relative placement of the network nodes in the
plane by INET. Based on the classification in [8], we 
classify network links as being Client-Stub, Stub-Stub, 
TransitStub, and Transit-Transit depending on their location in
the network. We restrict topological bandwidth by setting
the bandwidth for each link depending on its type. Each
type of link has an associated bandwidth range from which
the bandwidth is chosen uniformly at random. By changing
these ranges, we vary bandwidth constraints in our 
topologies. For our experiments, we created three different ranges
corresponding to low, medium, and high bandwidths relative
to our typical streaming rates of 600-1000 Kbps as 
specified in Table 1. While the presented ModelNet results are
restricted to two topologies with varying bandwidth 
constraints, the results of experiments with additional 
topologies all show qualitatively similar behavior.
We do not implement any particular coding scheme for
our experiments. Rather, we assume that either each 
sequence number directly specifies a particular data block and
the block offset for each packet, or we are distributing data
within the same block for LT Codes, e.g., when distributing
a file.
4.1 Offline Bottleneck Bandwidth Tree
One of our goals is to determine Bullet"s performance 
relative to the best possible bandwidth-optimized tree for a
given network topology. This allows us to quantify the 
possible improvements of an overlay mesh constructed using
Bullet relative to the best possible tree. While we have not
yet proven this, we believe that this problem is NP-hard.
Thus, in this section we present a simple greedy oﬄine 
algorithm to determine the connectivity of a tree likely to deliver
a high level of bandwidth. In practice, we are not aware of
any scalable online algorithms that are able to deliver the
bandwidth of an oﬄine algorithm. At the same time, trees
constructed by our algorithm tend to be long and skinny
making them less resilient to failures and inappropriate for
delay sensitive applications (such as multimedia streaming).
In addition to any performance comparisons, a Bullet mesh
has much lower depth than the bottleneck tree and is more
resilient to failure, as discussed in Section 4.6.
289
Topology 
classification
Client-Stub Stub-Stub Transit-Stub Transit-Transit
Low bandwidth 300-600 500-1000 1000-2000 2000-4000
Medium bandwidth 800-2800 1000-4000 1000-4000 5000-10000
High bandwidth 1600-5600 2000-8000 2000-8000 10000-20000
Table 1: Bandwidth ranges for link types used in our topologies expressed in Kbps.
Specifically, we consider the following problem: given 
complete knowledge of the topology (individual link latencies,
bandwidth, and packet loss rates), what is the overlay tree
that will deliver the highest bandwidth to a set of 
predetermined overlay nodes? We assume that the throughput of
the slowest overlay link (the bottleneck link) determines the
throughput of the entire tree. We are, therefore, trying to
find the directed overlay tree with the maximum bottleneck
link. Accordingly, we refer to this problem as the overlay
maximum bottleneck tree (OMBT). In a simplified case, 
assuming that congestion only exists on access links and there
are no lossy links, there exists an optimal algorithm [23].
In the more general case of contention on any physical link,
and when the system is allowed to choose the routing path
between the two endpoints, this problem is known to be
NP-hard [12], even in the absence of link losses. For the
purposes of this paper, our goal is to determine a good
overlay streaming tree that provides each overlay 
participant with substantial bandwidth, while avoiding overlay
links with high end-to-end loss rates.
We make the following assumptions:
1. The routing path between any two overlay participants
is fixed. This closely models the existing overlay 
network model with IP for unicast routing.
2. The overlay tree will use TCP-friendly unicast 
connections to transfer data point-to-point.
3. In the absence of other flows, we can estimate the
throughput of a TCP-friendly flow using a steady-state
formula [27].
4. When several (n) flows share the same bottleneck link,
each flow can achieve throughput of at most c
n
, where
c is the physical capacity of the link.
Given these assumptions, we concentrate on estimating
the throughput available between two participants in the
overlay. We start by calculating the throughput using the
steady-state formula. We then route the flow in the 
network, and consider the physical links one at a time. On
each physical link, we compute the fair-share for each of the
competing flows. The throughput of an overlay link is then
approximated by the minimum of the fair-shares along the
routing path, and the formula rate. If some flow does not
require the same share of the bottleneck link as other 
competing flows (i.e., its throughput might be limited by losses
elsewhere in the network), then the other flows might end
up with a greater share than the one we compute. We do
not account for this, as the major goal of this estimate is
simply to avoid lossy and highly congested physical links.
More formally, we define the problem as follows:
Overlay Maximum Bottleneck Tree (OMBT).
Given a physical network represented as a graph G = (V, E),
set of overlay participants P ⊂ V , source node (s ∈ P),
bandwidth B : E → R+
, loss rate L : E → [0, 1], 
propagation delay D : E → R+
of each link, set of possible
overlay links O = {(v, w) | v, w ∈ P, v = w}, routing table
RT : O × E → {0, 1}, find the overlay tree T = {o | o ∈ O}
(|T| = |P| − 1, ∀v ∈ P there exists a path ov
= s ❀ v) that
maximizes
min
o|o∈T
(min(f(o), min
e|e∈o
b(e)
|{p | p ∈ T, e ∈ p}|
))
where f(o) is the TCP steady-state sending rate, 
computed from round-trip time d(o) =
Èe∈o d(e) +
Èe∈o d(e)
(given overlay link o = (v, w), o = (w, v)), and loss rate
l(o) = 1 −
Ée∈o (1 − l(e)). We write e ∈ o to express that
link e is included in the o"s routing path (RT(o, e) = 1).
Assuming that we can estimate the throughput of a flow,
we proceed to formulate a greedy OMBT algorithm. This
algorithm is non-optimal, but a similar approach was found
to perform well [12].
Our algorithm is similar to the Widest Path Heuristic
(WPH) [12], and more generally to Prim"s MST algorithm [32].
During its execution, we maintain the set of nodes already in
the tree, and the set of remaining nodes. To grow the tree,
we consider all the overlay links leading from the nodes in
the tree to the remaining nodes. We greedily pick the node
with the highest throughput overlay link. Using this overlay
link might cause us to route traffic over physical links 
traversed by some other tree flows. Since we do not re-examine
the throughput of nodes that are already in the tree, they
might end up being connected to the tree with slower 
overlay links than initially estimated. However, by attaching the
node with the highest residual bandwidth at every step, we
hope to lessen the effects of after-the-fact physical link 
sharing. With the synthetic topologies we use for our emulation
environment, we have not found this inaccuracy to severely
impact the quality of the tree.
4.2 Bullet vs. Streaming
We have implemented a simple streaming application that
is capable of streaming data over any specified tree. In our
implementation, we are able to stream data through 
overlay trees using UDP, TFRC, or TCP. Figure 6 shows 
average bandwidth that each of 1000 nodes receives via this
streaming as time progresses on the x-axis. In this 
example, we use TFRC to stream 600 Kbps over our oﬄine 
bottleneck bandwidth tree and a random tree (other random
trees exhibit qualitatively similar behavior). In these 
experiments, streaming begins 100 seconds into each run. While
the random tree delivers an achieved bandwidth of under 100
Kbps, our oﬄine algorithm overlay delivers approximately
400 Kbps of data. For this experiment, bandwidths were
set to the medium range from Table 1. We believe that any
degree-constrained online bandwidth overlay tree algorithm
would exhibit similar (or lower) behavior to our 
bandwidth290
0
200
400
600
800
1000
0 50 100 150 200 250 300 350 400
Bandwidth(Kbps)
Time (s)
Bottleneck bandwidth tree
Random tree
Figure 6: Achieved bandwidth over time for TFRC
streaming over the bottleneck bandwidth tree and
a random tree.
optimized overlay. Hence, Bullet"s goal is to overcome this
bandwidth limit by allowing for the perpendicular reception
of data and by utilizing disjoint data flows in an attempt to
match or exceed the performance of our oﬄine algorithm.
To evaluate Bullet"s ability to exceed the bandwidth 
achievable via tree distribution overlays, we compare Bullet 
running over a random overlay tree to the streaming behavior
shown in Figure 6. Figure 7 shows the average bandwidth
received by each node (labeled Useful total) with standard
deviation. The graph also plots the total amount of data
received and the amount of data a node receives from its
parent. For this topology and bandwidth setting, Bullet
was able to achieve an average bandwidth of 500 Kbps,
fives times that achieved by the random tree and more than
25% higher than the oﬄine bottleneck bandwidth algorithm.
Further, the total bandwidth (including redundant data) 
received by each node is only slightly higher than the useful
content, meaning that Bullet is able to achieve high 
bandwidth while wasting little network resources. Bullet"s use
of TFRC in this example ensures that the overlay is TCP
friendly throughout. The average per-node control overhead
is approximately 30 Kbps. By tracing certain packets as
they move through the system, we are able to acquire link
stress estimates of our system. Though the link stress can
be different for each packet since each can take a different
path through the overlay mesh, we average link stress due
to each traced packet. For this experiment, Bullet has an
average link stress of approximately 1.5 with an absolute
maximum link stress of 22.
The standard deviation in most of our runs is fairly high
because of the limited bandwidth randomly assigned to some
Client-Stub and Stub-Stub links. We feel that this is 
consistent with real Internet behavior where clients have widely
varying network connectivity. A time slice is shown in 
Figure 8 that plots the CDF of instantaneous bandwidths that
each node receives. The graph shows that few client nodes
receive inadequate bandwidth even though they are 
bandwidth constrained. The distribution rises sharply starting at
approximately 500 Kbps. The vast majority of nodes receive
a stream of 500-600 Kbps.
We have evaluated Bullet under a number of bandwidth
constraints to determine how Bullet performs relative to the
0
200
400
600
800
1000
0 50 100 150 200 250 300 350 400 450 500
Bandwidth(Kbps)
Time (s)
Raw total
Useful total
From parent
Figure 7: Achieved bandwidth over time for Bullet
over a random tree.
0
0.2
0.4
0.6
0.8
1
0 100 200 300 400 500 600 700 800
Percentageofnodes
Bandwidth(Kbps)
Figure 8: CDF of instantaneous achieved bandwidth
at time 430 seconds.
available bandwidth of the underlying topology. Table 1 
describes representative bandwidth settings for our streaming
rate of 600 Kbps. The intent of these settings is to show a
scenario where more than enough bandwidth is available to
achieve a target rate even with traditional tree streaming,
an example of where it is slightly not sufficient, and one in
which the available bandwidth is quite restricted. Figure 9
shows achieved bandwidths for Bullet and the bottleneck
bandwidth tree over time generated from topologies with
bandwidths in each range.
In all of our experiments, Bullet outperforms the 
bottleneck bandwidth tree by a factor of up to 100%, depending on
how much bandwidth is constrained in the underlying 
topology. In one extreme, having more than ample bandwidth,
Bullet and the bottleneck bandwidth tree are both able to
stream at the requested rate (600 Kbps in our example). In
the other extreme, heavily constrained topologies allow 
Bullet to achieve twice the bandwidth achievable via the 
bottleneck bandwidth tree. For all other topologies, Bullet"s
benefits are somewhere in between. In our example, Bullet
running over our medium-constrained bandwidth topology
is able to outperform the bottleneck bandwidth tree by a
factor of 25%. Further, we stress that we believe it would
291
0
200
400
600
800
1000
0 50 100 150 200 250 300 350 400
Bandwidth(Kbps)
Time (s)
Bullet - High Bandwidth
Bottleneck tree - High Bandwidth
Bullet - Medium Bandwidth
Bottleneck tree - Medium Bandwidth
Bullet - Low Bandwidth
Bottleneck tree - Low Bandwidth
Figure 9: Achieved bandwidth for Bullet and 
bottleneck tree over time for high, medium, and low
bandwidth topologies.
be extremely difficult for any online tree-based algorithm to
exceed the bandwidth achievable by our oﬄine bottleneck
algorithm that makes use of global topological information.
For instance, we built a simple bandwidth optimizing overlay
tree construction based on Overcast [21]. The resulting 
dynamically constructed trees never achieved more than 75%
of the bandwidth of our own oﬄine algorithm.
4.3 Creating Disjoint Data
Bullet"s ability to deliver high bandwidth levels to nodes
depends on its disjoint transmission strategy. That is, when
bandwidth to a child is limited, Bullet attempts to send
the correct portions of data so that recovery of the lost
data is facilitated. A Bullet parent sends different data to
its children in hopes that each data item will be readily
available to nodes spread throughout its subtree. It does
so by assigning ownership of data objects to children in a
manner that makes the expected number of nodes holding
a particular data object equal for all data objects it 
transmits. Figure 10 shows the resulting bandwidth over time
for the non-disjoint strategy in which a node (and more 
importantly, the root of the tree) attempts to send all data to
each of its children (subject to independent losses at 
individual child links). Because the children transports throttle the
sending rate at each parent, some data is inherently sent 
disjointly (by chance). By not explicitly choosing which data
to send its child, this approach deprives Bullet of 25% of its
bandwidth capability, when compared to the case when our
disjoint strategy is enabled in Figure 7.
4.4 Epidemic Approaches
In this section, we explore how Bullet compares to data
dissemination approaches that use some form of epidemic
routing. We implemented a form of gossiping, where a
node forwards non-duplicate packets to a randomly chosen
number of nodes in its local view. This technique does not
use a tree for dissemination, and is similar to lpbcast [14] 
(recently improved to incorporate retrieval of data objects [13]).
We do not disseminate packets every T seconds; instead we
forward them as soon as they arrive.
0
200
400
600
800
1000
0 50 100 150 200 250 300 350 400 450 500
Bandwidth(Kbps)
Time (s)
Raw total
Useful total
From parent
Figure 10: Achieved bandwidth over time using 
nondisjoint data transmission.
We also implemented a pbcast-like [2] approach for 
retrieving data missing from a data distribution tree. The
idea here is that nodes are expected to obtain most of their
data from their parent. Nodes then attempt to retrieve any
missing data items through gossiping with random peers.
Instead of using gossiping with a fixed number of rounds for
each packet, we use anti-entropy with a FIFO Bloom filter
to attempt to locate peers that hold any locally missing data
items.
To make our evaluation conservative, we assume that nodes
employing gossip and anti-entropy recovery are able to 
maintain full group membership. While this might be difficult in
practice, we assume that RanSub [24] could also be applied
to these ideas, specifically in the case of anti-entropy 
recovery that employs an underlying tree. Further, we also
allow both techniques to reuse other aspects of our 
implementation: Bloom filters, TFRC transport, etc. To reduce
the number of duplicate packets, we use less peers in each
round (5) than Bullet (10). For our configuration, we 
experimentally found that 5 peers results in the best performance
with the lowest overhead. In our experiments, increasing
the number of peers did not improve the average bandwidth
achieved throughout the system. To allow TFRC enough
time to ramp up to the appropriate TCP-friendly sending
rate, we set the epoch length for anti-entropy recovery to 20
seconds.
For these experiments, we use a 5000-node INET topology
with no explicit physical link losses. We set link bandwidths
according to the medium range from Table 1, and randomly
assign 100 overlay participants. The randomly chosen root
either streams at 900 Kbps (over a random tree for 
Bullet and greedy bottleneck tree for anti-entropy recovery), or
sends packets at that rate to randomly chosen nodes for 
gossiping. Figure 11 shows the resulting bandwidth over time
achieved by Bullet and the two epidemic approaches. As 
expected, Bullet comes close to providing the target bandwidth
to all participants, achieving approximately 60 percent more
then gossiping and streaming with anti-entropy. The two
epidemic techniques send an excessive number of duplicates,
effectively reducing the useful bandwidth provided to each
node. More importantly, both approaches assign equal 
significance to other peers, regardless of the available 
band292
0
500
1000
1500
2000
0 50 100 150 200 250 300
Bandwidth(Kbps)
Time (s)
Push gossiping raw
Streaming w/AE raw
Bullet raw
Bullet useful
Push gossiping useful
Streaming w/AE useful
Figure 11: Achieved bandwidth over time for Bullet
and epidemic approaches.
width and the similarity ratio. Bullet, on the other hand,
establishes long-term connections with peers that provide
good bandwidth and disjoint content, and avoids most of
the duplicates by requesting disjoint data from each node"s
peers.
4.5 Bullet on a Lossy Network
To evaluate Bullet"s performance under more lossy 
network conditions, we have modified our 20,000-node 
topologies used in our previous experiments to include random
packet losses. ModelNet allows the specification of a packet
loss rate in the description of a network link. Our goal by
modifying these loss rates is to simulate queuing behavior
when the network is under load due to background network
traffic.
To effect this behavior, we first modify all non-transit links
in each topology to have a packet loss rate chosen uniformly
random from [0, 0.003] resulting in a maximum loss rate of
0.3%. Transit links are likewise modified, but with a 
maximum loss rate of 0.1%. Similar to the approach in [28],
we randomly designated 5% of the links in the topologies as
overloaded and set their loss rates uniformly random from
[0.05, 0.1] resulting in a maximum packet loss rate of 10%.
Figure 12 shows achieved bandwidths for streaming over
Bullet and using our greedy oﬄine bottleneck bandwidth
tree. Because losses adversely affect the bandwidth 
achievable over TCP-friendly transport and since bandwidths are
strictly monotonically decreasing over a streaming tree, 
treebased algorithms perform considerably worse than Bullet
when used on a lossy network. In all cases, Bullet delivers
at least twice as much bandwidth than the bottleneck 
bandwidth tree. Additionally, losses in the low bandwidth 
topology essentially keep the bottleneck bandwidth tree from 
delivering any data, an artifact that is avoided by Bullet.
4.6 Performance Under Failure
In this section, we discuss Bullet"s behavior in the face
of node failure. In contrast to streaming distribution trees
that must quickly detect and make tree transformations to
overcome failure, Bullet"s failure resilience rests on its ability
to maintain a higher level of achieved bandwidth by virtue
of perpendicular (peer) streaming. While all nodes under a
failed node in a distribution tree will experience a temporary
0
200
400
600
800
1000
0 50 100 150 200 250 300 350 400
Bandwidth(Kbps)
Time (s)
Bullet - High Bandwidth
Bullet - Medium Bandwidth
Bottleneck tree - High Bandwidth
Bottleneck tree - Medium Bandwidth
Bullet - Low Bandwidth
Bottleneck tree - Low Bandwidth
Figure 12: Achieved bandwidths for Bullet and 
bottleneck bandwidth tree over a lossy network 
topology.
disruption in service, Bullet nodes are able compensate for
this by receiving data from peers throughout the outage.
Because Bullet, and, more importantly, RanSub makes
use of an underlying tree overlay, part of Bullet"s failure
recovery properties will depend on the failure recovery 
behavior of the underlying tree. For the purposes of this 
discussion, we simply assume the worst-case scenario where an
underlying tree has no failure recovery. In our failure 
experiments, we fail one of root"s children (with 110 of the total
1000 nodes as descendants) 250 seconds after data 
streaming is started. By failing one of root"s children, we are able
to show Bullet"s worst-case performance under a single node
failure.
In our first scenario, we disable failure detection in 
RanSub so that after a failure occurs, Bullet nodes request data
only from their current peers. That is, at this point, 
RanSub stops functioning and no new peer relationships are 
created for the remainder of the run. Figure 13 shows Bullet"s
achieved bandwidth over time for this case. While the 
average achieved rate drops from 500 Kbps to 350 Kbps, most
nodes (including the descendants of the failed root child) are
able to recover a large portion of the data rate.
Next, we enable RanSub failure detection that recognizes
a node"s failure when a RanSub epoch has lasted longer
than the predetermined maximum (5 seconds for this test).
In this case, the root simply initiates the next distribute
phase upon RanSub timeout. The net result is that nodes
that are not descendants of the failed node will continue to
receive updated random subsets allowing them to peer with
appropriate nodes reflecting the new network conditions. As
shown in Figure 14, the failure causes a negligible disruption
in performance. With RanSub failure detection enabled,
nodes quickly learn of other nodes from which to receive
data. Once such recovery completes, the descendants of the
failed node use their already established peer relationships
to compensate for their ancestor"s failure. Hence, because
Bullet is an overlay mesh, its reliability characteristics far
exceed that of typical overlay distribution trees.
4.7 PlanetLab
This section contains results from the deployment of 
Bullet over the PlanetLab [31] wide-area network testbed. For
293
0
200
400
600
800
1000
0 50 100 150 200 250 300 350 400
Bandwidth(Kbps)
Time (s)
Bandwidth received
Useful total
From parent
Figure 13: Bandwidth over time with a worst-case
node failure and no RanSub recovery.
0
200
400
600
800
1000
0 50 100 150 200 250 300 350 400
Bandwidth(Kbps)
Time (s)
Bandwidth received
Useful total
From parent
Figure 14: Bandwidth over time with a worst-case
node failure and RanSub recovery enabled.
our first experiment, we chose 47 nodes for our deployment,
with no two machines being deployed at the same site. Since
there is currently ample bandwidth available throughout the
PlanetLab overlay (a characteristic not necessarily 
representative of the Internet at large), we designed this experiment
to show that Bullet can achieve higher bandwidth than an
overlay tree when the source is constrained, for instance in
cases of congestion on its outbound access link, or of 
overload by a flash-crowd.
We did this by choosing a root in Europe connected to
PlanetLab with fairly low bandwidth. The node we selected
was in Italy (cs.unibo.it) and we had 10 other overlay
nodes in Europe. Without global knowledge of the topology
in PlanetLab (and the Internet), we are, of course, unable
to produce our greedy bottleneck bandwidth tree for 
comparison. We ran Bullet over a random overlay tree for 300
seconds while attempting to stream at a rate of 1.5 Mbps.
We waited 50 seconds before starting to stream data to 
allow nodes to successfully join the tree. We compare the
performance of Bullet to data streaming over multiple 
handcrafted trees. Figure 15 shows our results for two such trees.
The good tree has all nodes in Europe located high in the
tree, close to the root. We used pathload [20] to measure the
0
200
400
600
800
1000
1200
0 50 100 150 200 250
Bandwidth(Kbps)
Time (s)
Bullet
Good Tree
Worst Tree
Figure 15: Achieved bandwidth over time for Bullet
and TFRC streaming over different trees on 
PlanetLab with a root in Europe.
available bandwidth between the root and all other nodes.
Nodes with high bandwidth measurements were placed close
to the root. In this case, we are able to achieve a bandwidth
of approximately 300 Kbps. The worst tree was created
by setting the root"s children to be the three nodes with the
worst bandwidth characteristics from the root as measured
by pathload. All subsequent levels in the tree were set in
this fashion.
For comparison, we replaced all nodes in Europe from
our topology with nodes in the US, creating a topology that
only included US nodes with high bandwidth characteristics.
As expected, Bullet was able to achieve the full 1.5 Mbps
rate in this case. A well constructed tree over this 
highbandwidth topology yielded slightly lower than 1.5 Mbps,
verifying that our approach does not sacrifice performance
under high bandwidth conditions and improves performance
under constrained bandwidth scenarios.
