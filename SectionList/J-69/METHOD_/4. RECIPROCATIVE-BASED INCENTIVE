TECHNIQUES
In this section we present our incentives techniques and evaluate
their behavior by simulation. To make the exposition clear we group
our techniques by the challenges they address: large populations
and high turnover (Section 4.1), collusions (Section 4.2), zero-cost
identities (Section 4.3), and traitors (Section 4.4).
4.1 Large Populations and High Turnover
The large populations and high turnover of P2P systems makes it
less likely that repeat interactions will occur with a familiar entity.
Under these conditions, basing decisions only on private history
(records about interactions the peer has been directly involved in)
is not effective. In addition, private history does not deal well with
asymmetry of interest. For example, if player B has cooperated with
others but not with player A himself in the past, player A has no
indication of player B"s generosity, thus may unduly defect on him.
We propose two mechanisms to alleviate the problem of few repeat
transactions: server-selection and shared history.
4.1.1 Server Selection
A natural way to increase the probability of interacting with familiar
peers is by discriminating server selection. However, the 
asymmetry of transactions challenges selection mechanisms. Unlike in the
prisoner"s dilemma payoff matrix, where players can benefit one
another within a single transaction, transactions in GPD are 
asymmetric. As a result, a player who selects her donor for the second
time without contributing to her in the interim may face a defection.
In addition, due to untraceability of defections, it is impossible to
maintain blacklists to avoid interactions with known defectors.
In order to deal with asymmetric transactions, every player holds
(fixed size) lists of both past donors and past recipients, and selects
a server from one of these lists at random with equal 
probabilities. This way, users approach their past recipients and give them a
chance to reciprocate.
In scenarios with selective users we omit the complete availability
assumption to prevent players from being clustered into a lot of
very small groups; thus, we assume that every player can perform
the requested service with probability p (for the results presented in
this section, p = .3). In addition, in order to avoid bias in favor of
the selective players, all players (including the non-discriminative
ones) select servers for games.
Figure 6 demonstrates the effectiveness of the proposed selection
mechanism in scenarios with large population sizes. We fix the 
initial ratio of Reciprocative in the population (33%) while varying
the population size (between 24 to 1000) (Notice that while in 
Figures 4(a) and (b), the data points demonstrates the evolution of the
system over time, each data point in this figure is the result of an
entire simulation for a specific scenario). The figure shows that the
Reciprocative decision function using private history in conjunction
with selective behavior can scale to large populations.
In Figure 7 we fix the population size and vary the turnover rate.
It demonstrates that while selective behavior is effective for low
turnover rates, as turnover gets higher, selective behavior does not
scale. This occurs because selection is only effective as long as
players from the past stay alive for long enough such that they can
be selected for future games.
4.1.2 Shared history
In order to mitigate asymmetry of interest and scale to higher
turnover rate, there is a need in shared history. Shared history means
that every peer keeps records about all of the interactions that 
occur in the system, regardless of whether he was directly involved
in them or not. It allows players to leverage off of the experiences
of others in cases of few repeat transactions. It only requires that
someone has interacted with a particular player for the entire 
population to observe it, thus scales better to large populations and high
turnovers, and also tolerates asymmetry of interest. Some examples
of shared history schemes are [20] [23] [28].
Figure 7 shows the effectiveness of shared history under high
turnover rates. In this figure, we fix the population size and vary the
turnover rate. While selective players with private history can only
tolerate a moderate turnover, shared history scales to turnovers of
up to approximately 0.1. This means that 10% of the players leave
the system at the end of each round. In Figure 6 we fix the turnover
and vary the population size. It shows that shared history causes
the system to converge to optimal cooperation and performance,
regardless of the size of the population.
These results show that shared history addresses all three 
challenges of large populations, high turnover, and asymmetry of 
transactions. Nevertheless, shared history has two disadvantages. First,
106
0
1
2
3
4
5
6
0
50
100
150
200
250
300
350
400
MeanOverallScore/Round
NumPlayers
Shared Non-Sel
Private Non-Sel
Private Selective
Figure 6: Private vs. Shared History as a function of population size.
0
1
2
3
4
5
6
0.0001 0.001 0.01 0.1
MeanOverallScore/Round
Turnover
Shared Non-Sel
Private Non-Sel
Private Selective
Figure 7: Performance of selection mechanism under turnover. The
x-axis is the turnover rate. The y-axis is the mean overall per round
score.
while a decentralized implementation of private history is 
straightforward, implementation of shared-history requires communication
overhead or centralization. A decentralized shared history can be
implemented, for example, on top of a DHT, using a peer-to-peer
storage system [36] or by disseminating information to other 
entities in a similar way to routing protocols. Second, and more 
fundamental, shared history is vulnerable to collusion. In the next section
we propose a mechanism that addresses this problem.
4.2 Collusion and Other Shared History
Attacks
4.2.1 Collusion
While shared history is scalable, it is vulnerable to collusion. 
Collusion can be either positive (e.g. defecting entities claim that other
defecting entities cooperated with them) or negative (e.g. entities
claim that other cooperative entities defected on them). Collusion
subverts any strategy in which everyone in the system agrees on the
reputation of a player (objective reputation). An example of 
objective reputation is to use the Reciprocative decision function with
shared history to count the total number of cooperations a player
has given to and received from all entities in the system; another
example is the Image strategy [28]. The effect of collusion is 
magnified in systems with zero-cost identities, where users can create
fake identities that report false statements.
Instead, to deal with collusion, entities can compute reputation 
subjectively, where player A weighs player B"s opinions based on how
much player A trusts player B. Our subjective algorithm is based
on maxflow [24] [32]. Maxflow is a graph theoretic problem, which
given a directed graph with weighted edges asks what is the greatest
rate at which material can be shipped from the source to the target
without violating any capacity constraints. For example, in figure 8
each edge is labeled with the amount of traffic that can travel on it.
The maxflow algorithm computes the maximum amount of traffic
that can go from the source (s) to the target (t) without violating the
constraints. In this example, even though there is a loop of high 
capacity edges, the maxflow between the source and the target is only
2 (the numbers in brackets represent the actual flow on each edge
in the solution).
100(0)
1(1)
5(1)
s t
10(1)
100(1)
1(1)
100(1)
20(0)
Figure 8: Each edge in the graph is labeled with its capacity and the
actual flow it carries in brackets. The maxflow between the source and
the target in the graph is 2.
C
C
CCCC
100100100100
100
00
0
0
20
20
0
0
A
B
Figure 9: This graph illustrates the robustness of maxflow in the 
presence of colluders who report bogus high reputation values.
We apply the maxflow algorithm by constructing a graph whose
vertices are entities and the edges are the services that entities have
received from each other. This information can be stored using the
same methods as the shared history. A maxflow is the greatest level
of reputation the source can give to the sink without violating 
reputation capacity constraints. As a result, nodes who dishonestly
report high reputation values will not be able to subvert the 
reputation system.
Figure 9 illustrates a scenario in which all the colluders (labeled
with C) report high reputation values for each other. When node A
computes the subjective reputation of B using the maxflow 
algorithm, it will not be affected by the local false reputation values,
rather the maxflow in this case will be 0. This is because no service
has been received from any of the colluders.
107
In our algorithm, the benefit that entity i has received (indirectly)
from entity j is the maxflow from j to i. Conversely, the benefit that
entity i has provided indirectly to j is the maxflow from i to j. The
subjective reputation of entity j as perceived by i is:
min
maxflow(j to i)
maxflow(i to j)
, 1 (3)
0
1
2
3
4
5
6
0
100
200
300
400
500
600
700
800
900
1000
MeanOverallScore/Round
Population
Shared
Private
Subjective
Figure 10: Subjective shared history compared to objective shared 
history and private history in the presence of colluders.
Algorithm 1 CONSTANTTIMEMAXFLOW Bound the mean running time
of Maxflow to a constant.
method CTMaxflow(self, src, dst)
1: self.surplus ← self.surplus + self.increment
{Use the running mean as a prediction.}
2: if random() > (0.5∗self.surplus/self.mean iterations) then
3: return None {Not enough surplus to run.}
4: end if
{Get the flow and number of iterations used from the maxflow alg.}
5: flow, iterations ← Maxflow(self.G, src, dst)
6: self.surplus ← self.surplus − iterations
{Keep a running mean of the number of iterations used.}
7: self.mean iterations ← self.α ∗ self.mean iterations + (1 −
self.α) ∗ iterations
8: return flow
The cost of maxflow is its long running time. The standard 
preflowpush maxflow algorithm has a worst case running time of O(V 3
).
Instead, we use Algorithm 1 which has a constant mean running
time, but sometimes returns no flow even though one exists. The
essential idea is to bound the mean number of nodes examined 
during the maxflow computation. This bounds the overhead, but also
bounds the effectiveness. Despite this, the results below show that
a maxflow-based Reciprocative decision function scales to higher
populations than one using private history.
Figure 10 compares the effectiveness of subjective reputation to
objective reputation in the presence of colluders. In these 
scenarios, defectors collude by claiming that other colluders that they 
encounter gave them 100 cooperations for that encounter. Also, the
parameters for Algorithm 1 are set as follows: increment = 100,
α = 0.9.
As in previous sections, Reciprocative with private history results
in cooperation up to a point, beyond which it fails. The difference
here is that objective shared history fails for all population sizes.
This is because the Reciprocative players cooperate with the 
colluders because of their high reputations. However, subjective 
history can reach high levels of cooperation regardless of colluders.
This is because there are no high weight paths in the cooperation
graph from colluders to any non-colluders, so the maxflow from
a colluder to any non-colluder is 0. Therefore, a subjective 
Reciprocative player will conclude that that colluder has not provided
any service to her and will reject service to the colluder. Thus, the
maxflow algorithm enables Reciprocative to maintain the 
scalability of shared history without being vulnerable to collusion or 
requiring centralized trust (e.g., trusted peers). Since we bound the
running time of the maxflow algorithm, cooperation decreases as
the population size increases, but the key point is that the subjective
Reciprocative decision function scales to higher populations than
one using private history. This advantage only increases over time
as CPU power increases and more cycles can be devoted to running
the maxflow algorithm (by increasing the increment parameter).
Despite the robustness of the maxflow algorithm to the simple form
of collusion described previously, it still has vulnerabilities to more
sophisticated attacks. One is for an entity (the mole) to provide
service and then lie positively about other colluders. The other 
colluders can then exploit their reputation to receive service. However,
the effectiveness of this attack relies on the amount of service that
the mole provides. Since the mole is paying all of the cost of 
providing service and receiving none of the benefit, she has a strong
incentive to stop colluding and try another strategy. This forces the
colluders to use mechanisms to maintain cooperation within their
group, which may drive the cost of collusion to exceed the benefit.
4.2.2 False reports
Another attack is for a defector to lie about receiving or providing
service to another entity. There are four possibile actions that can be
lied about: providing service, not providing service, receiving 
service, and not receiving service. Falsely claiming to receive service
is the simple collusion attack described above. Falsely claiming not
to have provided service provides no benefit to the attacker.
Falsely claiming to have provided service or not to have received
it allows an attacker to boost her own reputation and/or lower the
reputation of another entity. An entity may want to lower another
entity"s reputation in order to discourage others from selecting it
and exclusively use its service. These false claims are clearly 
identifiable in the shared history as inconsistencies where one entity
claims a transaction occurred and another claims it did not. To limit
this attack, we modify the maxflow algorithm so that an entity 
always believes the entity that is closer to him in the flow graph. If
both entities are equally distant, then the disputed edge in the flow is
not critical to the evaluation and is ignored. This modification 
prevents those cases where the attacker is making false claims about an
entity that is closer than her to the evaluating entity, which prevents
her from boosting her own reputation. The remaining possibilities
are for the attacker to falsely claim to have provided service to or
not to have received it from a victim entity that is farther from the
evalulator than her. In these cases, an attacker can only lower the
reputation of the victim. The effectiveness of doing this is limited
by the number of services provided and received by the attacker,
which makes executing this attack expensive.
108
4.3 Zero-Cost Identities
History assumes that entities maintain persistent identities. 
However, in most P2P systems, identities are zero-cost. This is desirable
for network growth as it encourages newcomers to join the system.
However, this also allows misbehaving users to escape the 
consequences of their actions by switching to new identities (i.e., 
whitewashing). Whitewashers can cause the system to collapse if they
are not punished appropriately. Unfortunately, a player cannot tell
if a stranger is a whitewasher or a legitimate newcomer. Always 
cooperating with strangers encourages newcomers to join, but at the
same time encourages whitewashing behavior. Always defecting on
strangers prevents whitewashing, but discourages newcomers from
joining and may also initiate unfavorable cycles of defection.
This tension suggests that any stranger policy that has a fixed 
probability of cooperating with strangers will fail by either being too
stingy when most strangers are newcomers or too generous when
most strangers are whitewashers. Our solution is the Stranger
Adaptive stranger policy. The idea is to be generous to strangers
when they are being generous and stingy when they are stingy.
Let ps and cs be the number of services that strangers have 
provided and consumed, respectively. The probability that a player 
using Stranger Adaptive helps a stranger is ps/cs. However, we do
not wish to keep these counts permanently (for reasons described in
Section 4.4). Also, players may not know when strangers defect 
because defections are untraceable (as described in Section 2). 
Consequently, instead of keeping ps and cs, we assume that k = ps + cs,
where k is a constant and we keep the running ratio r = ps/cs.
When we need to increment ps or cs, we generate the current 
values of ps and cs from k and r:
cs = k/(1 + r)
ps = cs ∗ r
We then compute the new r as follows:
r = (ps + 1)/cs , if the stranger provided service
r = ps/(cs + 1) , if the stranger consumed service
This method allows us to keep a running ratio that reflects the 
recent generosity of strangers without knowing when strangers have
defected.
0
1
2
3
4
5
6
0.0001 0.001 0.01 0.1 1
MeanOverallScore/Round
Turnover
Stranger Cooperate
Stranger Defect
Stranger Adaptive
Figure 11: Different stranger policies for Reciprocative with shared
history. The x-axis is the turnover rate on a log scale. The y-axis is the
mean overall per round score.
Figures 11 and 12 compare the effectiveness of the 
Reciprocative strategy using different policies toward strangers. Figure 11
0
1
2
3
4
5
6
0.0001 0.001 0.01 0.1 1
MeanOverallScore/Round
Turnover
Stranger Cooperate
Stranger Defect
Stranger Adaptive
Figure 12: Different stranger policies for Reciprocative with private
history. The x-axis is the turnover rate on a log scale. The y-axis is the
mean overall per round score.
compares different stranger policies for Reciprocative with shared
history, while Figure 12 is with private history. In both figures,
the players using the 100% Defect strategy change their 
identity (whitewash) after every transaction and are indistinguishable
from legitimate newcomers. The Reciprocative players using the
Stranger Cooperate policy completely fail to achieve cooperation.
This stranger policy allows whitewashers to maximize their payoff
and consequently provides a high incentive for users to switch to
whitewashing.
In contrast, Figure 11 shows that the Stranger Defect policy is
effective with shared history. This is because whitewashers always
appear to be strangers and therefore the Reciprocative players will
always defect on them. This is consistent with previous work [13]
showing that punishing strangers deals with whitewashers. 
However, Figure 12 shows that Stranger Defect is not effective with
private history. This is because Reciprocative requires some initial
cooperation to bootstrap. In the shared history case, a Reciprocative
player can observe that another player has already cooperated with
others. With private history, the Reciprocative player only knows
about the other players" actions toward her. Therefore, the initial
defection dictated by the Stranger Defect policy will lead to later
defections, which will prevent Reciprocative players from ever 
cooperating with each other. In other simulations not shown here,
the Stranger Defect stranger policy fails even with shared history
when there are no initial 100% Cooperate players.
Figure 11 shows that with shared history, the Stranger 
Adaptive policy performs as well as Stranger Defect policy until the
turnover rate is very high (10% of the population turning over after
every transaction). In these scenarios, Stranger Adaptive is using
k = 10 and each player keeps a private r. More importantly, it is
significantly better than Stranger Defect policy with private 
history because it can bootstrap cooperation. Although the Stranger
Defect policy is marginally more effective than Stranger 
Adaptive at very high rates of turnover, P2P systems are unlikely to 
operate there because other services (e.g., routing) also cannot tolerate
very high turnover.
We conclude that of the stranger policies that we have explored,
Stranger Adaptive is the most effective. By using Stranger
Adaptive, P2P systems with zero-cost identities and a sufficiently
low turnover can sustain cooperation without a centralized 
allocation of identities.
109
4.4 Traitors
Traitors are players who acquire high reputation scores by 
cooperating for a while, and then traitorously turn into defectors before
leaving the system. They model both users who turn deliberately
to gain a higher score and cooperators whose identities have been
stolen and exploited by defectors. A strategy that maintains 
longterm history without discriminating between old and recent actions
becomes highly vulnerable to exploitation by these traitors.
The top two graphs in Figure 13 demonstrate the effect of traitors
on cooperation in a system where players keep long-term history
(never clear history). In these simulations, we run for 2000 rounds
and allow cooperative players to keep their identities when 
switching to the 100% Defector strategy. We use the default values for the
other parameters. Without traitors, the cooperative strategies thrive.
With traitors, the cooperative strategies thrive until a cooperator
turns traitor after 600 rounds. As this cooperator exploits her 
reputation to achieve a high score, other cooperative players notice this
and follow suit via learning. Cooperation eventually collapses. On
the other hand, if we maintain short-term history and/or 
discounting ancient history vis-a-vis recent history, traitors can be quickly
detected, and the overall cooperation level stays high, as shown in
the bottom two graphs in Figure 13.
0
20
40
60
80
100
1K 2K
Long-TermHistory
No Traitors
Population
0
20
40
60
80
100
1K 2K
Traitors
Defector
Cooperator
Recip. Shared
0
20
40
60
80
100
1K 2K
Short-TermHistory
Time
Population
0
20
40
60
80
100
1K 2K
Time
Figure 13: Keeping long-term vs. short-term history both with and
without traitors.
