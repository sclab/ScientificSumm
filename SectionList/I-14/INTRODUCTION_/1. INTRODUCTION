Over the last few years there have been increasing 
interests in studying how to control the search processes in
peer-to-peer(P2P) based information retrieval(IR) systems
[6, 13, 14, 15]. In this line of research, one of the core 
problems that concerns researchers is to efficiently route user
queries in the network to agents that are in possession of
appropriate documents. In the absence of global 
information, the dominant strategies in addressing this problem are
content-similarity based approaches [6, 13, 14, 15]. While
the content similarity between queries and local nodes 
appears to be a creditable indicator for the number of 
relevant documents residing on each node, these approaches
are limited by a number of factors. First of all, 
similaritybased metrics can be myopic since locally relevant nodes
may not be connected to other relevant nodes. Second, the
similarity-based approaches do not take into account the
run-time characteristics of the P2P IR systems, including
environmental parameters, bandwidth usage, and the 
historical information of the past search sessions, that provide
valuable information for the query routing algorithms.
In this paper, we develop a reinforcement learning based
IR approach for improving the performance of distributed
IR search algorithms. Agents can acquire better search
strategies by collecting and analyzing feedback information
from previous search sessions. Particularly, agents 
maintain estimates, namely expected utility, on the downstream
agents" capabilities of providing relevant documents for 
specific types of incoming queries. These estimates are 
updated gradually by learning from the feedback information
returned from previous search sessions. Based on the 
updated expected utility information, the agents derive 
corresponding routing policies. Thereafter, these agents route
the queries based on the learned policies and update the
estimates on the expected utility based on the new routing
policies. This process is conducted in an iterative manner.
The goal of the learning algorithm, even though it consumes
some network bandwidth, is to shorten the routing time so
that more queries are processed per time unit while at the
same time finding more relevant documents. This contrasts
with the content-similarity based approaches where similar
operations are repeated for every incoming query and the
processing time keeps largely constant over time.
Another way of viewing this paper is that our basic 
approach to distributed IR search is to construct a hierarchical
overlay network(agent organization) based on the 
contentsimilarity measure among agents" document collections in a
bottom-up fashion. In the past work, we have shown that
this organization improves search performance significantly.
However, this organizational structure does not take into
account the arrival patterns of queries, including their 
frequency, types, and where they enter the system, nor the
available communication bandwidth of the network and 
processing capabilities of individual agents. The intention of
the reinforcement learning is to adapt the agents" routing
decisions to the dynamic network situations and learn from
past search sessions. Specifically, the contributions of this
paper include: (1) a reinforcement learning based approach
for agents to acquire satisfactory routing policies based on
estimates of the potential contribution of their neighboring
agents; (2) two strategies to speed up the learning process.
To our best knowledge, this is one of the first reinforcement
learning applications in addressing distributed content 
sharing problems and it is indicative of some of the issues in
applying reinforcement in a complex application.
The remainder of this paper is organized as follows: 
Section 2 reviews the hierarchical content sharing systems and
the two-phase search algorithm based on such topology. 
Section 3 describes a reinforcement learning based approach to
direct the routing process; Section 4 details the experimental
settings and analyze the results. Section 5 discusses related
studies and Section 6 concludes the paper.
