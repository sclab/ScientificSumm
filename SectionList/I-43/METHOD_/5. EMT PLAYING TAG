The Game of Tag was first introduced in [11]. It is a single agent
problem of capturing a quarry, and belongs to the class of area
sweeping problems. An example domain is shown in Figure 2.
0 51 2 3 4 6
7 8 10 12 13
161514
17 18 19
2221
23
9 11Q A
20
Figure 2: Tag domain; an agent (A) attempts to seek and 
capture a quarry (Q)
The Game of Tag extremely limits the agent"s perception, so that
the agent is able to detect the quarry only if they are co-located in
the same cell of the grid world. In the classical version of the game,
co-location leads to a special observation, and the ‘Tag" action can
be performed. We slightly modify this setting: the moment that
both agents occupy the same cell, the game ends. As a result, both
the agent and its quarry have the same motion capability, which
allows them to move in four directions, North, South, East, and
West. These form a formal space of actions within a Markovian
environment.
The state space of the formal Markovian environment is described
by the cross-product of the agent and quarry"s positions. For 
Figure 2, it would be S = {s0, ..., s23} × {s0, ..., s23}.
The effects of an action taken by the agent are deterministic, but
the environment in general has a stochastic response due to the 
motion of the quarry. With probability q0
1
it stays put, and with 
probability 1 − q0 it moves to an adjacent cell further away from the
1
In our experiments this was taken to be q0 = 0.2.
agent. So for the instance shown in Figure 2 and q0 = 0.1:
P(Q = s9|Q = s9, A = s11) = 0.1
P(Q = s2|Q = s9, A = s11) = 0.3
P(Q = s8|Q = s9, A = s11) = 0.3
P(Q = s14|Q = s9, A = s11) = 0.3
Although the evasive behavior of the quarry is known to the
agent, the quarry"s position is not. The only sensory information
available to the agent is its own location.
We use EMT and directly specify the target dynamics. For the
Game of Tag, we can easily formulate three major trends: catching
the quarry, staying mobile, and stalking the quarry. This results in
the following three target dynamics:
Tcatch(At+1 = si|Qt = sj, At = sa) ∝
1 si = sj
0 otherwise
Tmobile(At+1 = si|Qt = so, At = sj) ∝
0 si = sj
1 otherwise
Tstalk(At+1 = si|Qt = so, At = sj) ∝ 1
dist(si,so)
Note that none of the above targets are directly achievable; for
instance, if Qt = s9 and At = s11, there is no action that can move
the agent to At+1 = s9 as required by the Tcatch target dynamics.
We ran several experiments to evaluate EMT performance in the
Tag Game. Three configurations of the domain shown in Figure 3
were used, each posing a different challenge to the agent due to 
partial observability. In each setting, a set of 1000 runs was performed
with a time limit of 100 steps. In every run, the initial position of
both the agent and its quarry was selected at random; this means
that as far as the agent was concerned, the quarry"s initial position
was uniformly distributed over the entire domain cell space.
We also used two variations of the environment observability
function. In the first version, observability function mapped all
joint positions of hunter and quarry into the position of the hunter as
an observation. In the second, only those joint positions in which
hunter and quarry occupied different locations were mapped into
the hunter"s location. The second version in fact utilized and 
expressed the fact that once hunter and quarry occupy the same cell
the game ends.
The results of these experiments are shown in Table 2. 
Balancing [15] the catch, move, and stalk target dynamics described in
the previous section by the weight vector [0.8, 0.1, 0.1], EMT 
produced stable performance in all three domains.
Although direct comparisons are difficult to make, the EMT 
performance displayed notable efficiency vis-`a-vis the POMDP 
approach. In spite of a simple and inefficient Matlab implementation
of the EMT algorithm, the decision time for any given step 
averaged significantly below 1 second in all experiments. For the 
irregular open arena domain, which proved to be the most difficult, 1000
experiment runs bounded by 100 steps each, a total of 42411 steps,
were completed in slightly under 6 hours. That is, over 4 × 104
online steps took an order of magnitude less time than the offline
computation of POMDP policy in [11]. The significance of this 
differential is made even more prominent by the fact that, should the
environment model parameters change, the online nature of EMT
would allow it to maintain its performance time, while the POMDP
policy would need to be recomputed, requiring yet again a large
overhead of computation.
We also tested the behavior cell frequency entropy, empirical
measures from trial data. As Figure 4 and Figure 5 show, 
empir794 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)
A
Q
Q A
0 1 2 3 4
5 6 7 8 9
10 11 12
13 14 15
16 17 18
A
Q
Figure 3: These configurations of the Tag Game space were used: a) multiple dead-end, b) irregular open arena, c) circular corridor
Table 2: Performance of the EMT-based solution in three Tag
Game domains and two observability models: I) omniposition
quarry, II) quarry is not at hunter"s position
Model Domain Capture% E(Steps) Time/Step
I
Dead-ends 100 14.8 72(mSec)
Arena 80.2 42.4 500(mSec)
Circle 91.4 34.6 187(mSec)
II
Dead-ends 100 13.2 91(mSec)
Arena 96.8 28.67 396(mSec)
Circle 94.4 31.63 204(mSec)
ical entropy grows with the length of interaction. For runs where
the quarry was not captured immediately, the entropy reaches 
between 0.85 and 0.952
for different runs and scenarios. As the agent
actively seeks the quarry, the entropy never reaches its maximum.
One characteristic of the entropy graph for the open arena 
scenario particularly caught our attention in the case of the 
omniposition quarry observation model. Near the maximum limit of trial
length (100 steps), entropy suddenly dropped. Further analysis of
the data showed that under certain circumstances, a fluctuating 
behavior occurs in which the agent faces equally viable versions of
quarry-following behavior. Since the EMT algorithm has greedy
action selection, and the state space does not encode any form of
commitment (not even speed or acceleration), the agent is locked
within a small portion of cells. It is essentially attempting to 
simultaneously follow several courses of action, all of which are 
consistent with the target dynamics. This behavior did not occur in our
second observation model, since it significantly reduced the set of
eligible courses of action-essentially contributing to tie-breaking
among them.
