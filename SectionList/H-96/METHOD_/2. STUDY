In this section we describe the user study conducted to address our
research questions.
2.1 Systems
Our study used two systems both of which suggested new query terms
to the user. One system suggested terms based on the user"s
interaction (IRF), the other used Explicit RF (ERF) asking the user to
explicitly indicate relevant material. Both systems used the same term
suggestion algorithm, [15], and used a common interface.
2.1.1 Interface Overview
In both systems, retrieved documents are represented at the interface
by their full-text and a variety of smaller, query-relevant
representations, created at retrieval time. We used the Web as the test
collection in this study and Google1
as the underlying search engine.
Document representations include the document title and a summary
of the document; a list of top-ranking sentences (TRS) extracted from
the top documents retrieved, scored in relation to the query, a sentence
in the document summary, and each summary sentence in the context
it occurs in the document (i.e., with the preceding and following
sentence). Each summary sentence and top-ranking sentence is
regarded as a representation of the document. The default display
contains the list of top-ranking sentences and the list of the first ten
document titles. Interacting with a representation guides searchers to a
different representation from the same document, e.g., moving the
mouse over a document title displays a summary of the document.
This presentation of progressively more information from documents
to aid relevance assessments has been shown to be effective in earlier
work [14, 16]. In Appendix A we show the complete interface to the
IRF system with the document representations marked and in
Appendix B we show a fragment from the ERF interface with the
checkboxes used by searchers to indicate relevant information. Both
systems provide an interactive query expansion feature by suggesting
new query terms to the user. The searcher has the responsibility for
choosing which, if any, of these terms to add to the query. The
searcher can also add or remove terms from the query at will.
2.1.2 Explicit RF system
This version of the system implements explicit RF. Next to each
document representation are checkboxes that allow searchers to mark
individual representations as relevant; marking a representation is an
indication that its contents are relevant. Only the representations
marked relevant by the user are used for suggesting new query terms.
This system was used as a baseline against which the IRF system
could be compared.
2.1.3 Implicit RF system
This system makes inferences about searcher interests based on the
information with which they interact. As described in Section 2.1.1
interacting with a representation highlights a new representation from
the same document. To the searcher this is a way they can find out
more information from a potentially interesting source. To the implicit
RF system each interaction with a representation is interpreted as an
implicit indication of interest in that representation; interacting with a
representation is assumed to be an indication that its contents are
relevant. The query modification terms are selected using the same
algorithm as in the Explicit RF system. Therefore the only difference
between the systems is how relevance is communicated to the system.
The results of the main experiment [13] indicated that these two
systems were comparable in terms of effectiveness.
2.2 Tasks
Search tasks were designed to encourage realistic search behaviour by
our subjects. The tasks were phrased in the form of simulated work
task situations [2], i.e., short search scenarios that were designed to
reflect real-life search situations and allow subjects to develop
personal assessments of relevance. We devised six search topics (i.e.,
applying to university, allergies in the workplace, art galleries in
Rome, Third Generation mobile phones, Internet music piracy and
petrol prices) based on pilot testing with a small representative group
of subjects. These subjects were not involved in the main experiment.
For each topic, three versions of each work task situation were
devised, each version differing in their predicted level of task
complexity. As described in [1] task complexity is a variable that
affects subject perceptions of a task and their interactive behaviour,
e.g., subjects perform more filtering activities with highly complex
search tasks. By developing tasks of different complexity we can
assess how the nature of the task affects the subjects" interactive
behaviour and hence the evidence supplied to IRF algorithms. Task
complexity was varied according to the methodology described in [1],
specifically by varying the number of potential information sources
and types of information required, to complete a task. In our pilot
tests (and in a posteriori analysis of the main experiment results) we
verified that subjects reporting of individual task complexity matched
our estimation of the complexity of the task.
Subjects attempted three search tasks: one high complexity, one
moderate complexity and one low complexity2
. They were asked to
read the task, place themselves in the situation it described and find
the information they felt was required to complete the task. Figure 1
shows the task statements for three levels of task complexity for one
of the six search topics.
HC Task: High Complexity
Whilst having dinner with an American colleague, they comment on the
high price of petrol in the UK compared to other countries, despite large
volumes coming from the same source. Unaware of any major differences,
you decide to find out how and why petrol prices vary worldwide.
MC Task: Moderate Complexity
Whilst out for dinner one night, one of your friends" guests is complaining
about the price of petrol and the factors that cause it. Throughout the night
they seem to be complaining about everything they can, reducing the
credibility of their earlier statements so you decide to research which
factors actually are important in determining the price of petrol in the UK.
LC Task: Low Complexity
While out for dinner one night, your friend complains about the rising
price of petrol. However, as you have not been driving for long, you are
unaware of any major changes in price. You decide to find out how the
price of petrol has changed in the UK in recent years.
Figure 1. Varying task complexity (Petrol Prices topic).
2.3 Subjects
156 volunteers expressed an interest in participating in our study. 48
subjects were selected from this set with the aim of populating two
groups, each with 24 subjects: inexperienced (infrequent/
inexperienced searchers) and experienced (frequent/ experienced
searchers). Subjects were not chosen and classified into their groups
until they had completed an entry questionnaire that asked them about
their search experience and computer use.
The average age of the subjects was 22.83 years (maximum 51,
minimum 18, Ïƒ = 5.23 years) and 75% had a university diploma or a
higher degree. 47.91% of subjects had, or were pursuing, a
qualification in a discipline related to Computer Science. The subjects
were a mixture of students, researchers, academic staff and others,
with different levels of computer and search experience. The subjects
were divided into the two groups depending on their search
experience, how often they searched and the types of searches they
performed. All were familiar with Web searching, and some with
searching in other domains.
2.4 Methodology
The experiment had a factorial design; with 2 levels of search
experience, 3 experimental systems (although we only report on the
findings from the ERF and IRF systems) and 3 levels of search task
complexity. Subjects attempted one task of each complexity,
2
The main experiment from which these results are drawn had a third
comparator system which had a different interface. Each subject
carried out three tasks, one on each system. We only report on the
results from the ERF and IRF systems as these are the only pertinent
ones for this paper.
switched systems after each task and used each system once. The
order in which systems were used and search tasks attempted was
randomised according to a Latin square experimental design.
Questionnaires used Likert scales, semantic differentials and 
openended questions to elicit subject opinions [4]. System logging was
also used to record subject interaction.
A tutorial carried out prior to the experiment allowed subjects to use a
non-feedback version of the system to attempt a practice task before
using the first experimental system. Experiments lasted between 
oneand-a-half and two hours, dependent on variables such as the time
spent completing questionnaires. Subjects were offered a 5 minute
break after the first hour. In each experiment:
i. the subject was welcomed and asked to read an introduction to
the experiments and sign consent forms. This set of instructions
was written to ensure that each subject received precisely the
same information.
ii. the subject was asked to complete an introductory questionnaire.
This contained questions about the subject"s education, general
search experience, computer experience and Web search
experience.
iii. the subject was given a tutorial on the interface, followed by a
training topic on a version of the interface with no RF.
iv. the subject was given three task sheets and asked to choose one
task from the six topics on each sheet. No guidelines were given
to subjects when choosing a task other than they could not
choose a task from any topic more than once. Task complexity
was rotated by the experimenter so each subject attempted one
high complexity task, one moderate complexity task and one low
complexity task.
v. the subject was asked to perform the search and was given 15
minutes to search. The subject could terminate a search early if
they were unable to find any more information they felt helped
them complete the task.
vi. after completion of the search, the subject was asked to complete
a post-search questionnaire.
vii. the remaining tasks were attempted by the subject, following
steps v. and vi.
viii. the subject completed a post-experiment questionnaire and
participated in a post-experiment interview.
Subjects were told that their interaction may be used by the IRF
system to help them as they searched. They were not told which
behaviours would be used or how it would be used.
We now describe the findings of our analysis.
