It is our hope that this study will have a positive impact in at least
three different ways. The first is practical: we believe our analysis
has implications for the design of future monitoring protocols and
for public policy.
For protocol designers, we first provide fresh motivation to create
monitoring systems. We have argued that the poor accountability of
the Internet is a fundamental obstacle to alleviating the pathologies
of commoditization and lack of innovation. Unless accountability
improves, these pathologies are guaranteed to remain.
Secondly, we suggest directions for future advances in monitoring.
We have shown that adding verifiability to monitors allows for
some improvements in the characteristics of competition. At the
same time, this does not present a fully satisfying solution. This
paper has suggested a novel standard for monitors to aspire to - one
of supporting optimal routes in innovative competition games under
fixed-route coalition-proof protect-the-innocent equilibrium. We
have shown that under bilateral contracts, this specifically requires
contractible rest of path monitors.
This is not to say that other types of monitors are unimportant. We
included an example in which individual hop quality monitors and a
path monitor can also meet our standard for sustaining competition.
However, in order for this to happen, a mechanism must be included
192
to combine proofs from these monitors to form a proof of rest of
path quality. In other words, the monitors must ultimately be
combined to form contractible rest of path monitors. To support
service differentiation and innovation, it may be easier to design rest
of path monitors directly, thereby avoiding the task of designing
mechanisms for combining component monitors.
As far as policy implications, our analysis points to the need for
legal institutions to enforce contracts based on quality. These
institutions must be equipped to verify proofs of quality, and police
illegal contracting behavior. As quality-based contracts become
numerous and complicated, and possibly negotiated by machine,
this may become a challenging task, and new standards and
regulations may have to emerge in response. This remains an
interesting and unexplored area for research.
The second area we hope our study will benefit is that of clean-slate
architectural design. Traditionally, clean-slate design tends to focus
on creating effective and elegant networks for a static set of
requirements. Thus, the approach is often one of engineering,
which tends to neglect competitive effects. We agree with
Ratnasamy, Shenker, and McCanne, that designing for evolution
should be a top priority [11]. We have demonstrated that the
network"s monitoring ability is critical to supporting innovation, as
are the institutions that support contracting. These elements should
feature prominently in new designs. Our analysis specifically
suggests that architectures based on bilateral contracts should
include contractible rest of path monitoring. From a clean-slate
perspective, these monitors can be transparently and fully integrated
with the routing and contracting systems.
Finally, the last contribution our study makes is methodological.
We believe that the mathematical formalization we present is
applicable to a variety of future research questions. While a
significant literature addresses innovation in the presence of
network effects, to the best of our knowledge, ours is the first model
of innovation in a network industry that successfully incorporates
the actual topological structure as input. This allows the discovery
of new properties, such as the weakening of market forces with the
number of ISPs on a data path that we observe with 
lowaccountability.
Our method also stands in contrast to the typical approach of
distributed algorithmic mechanism design. Because this field is
based on a principle-agent framework, contracts are usually
proposed by the source, who is allowed to make a take it or leave it
offer to network nodes. Our technique allows contracts to emerge
from a competitive framework, so the source is limited to selecting
the most desirable contract. We believe this is a closer reflection of
the industry.
Based on the insights in this study, the possible directions for future
research are numerous and exciting. To some degree, contracting
based on quality opens a Pandora"s Box of pressing questions: Do
quality-based contracts stand counter to the principle of network
neutrality? Should ISPs be allowed to offer a choice of contracts at
different quality levels? What anti-competitive behaviors are
enabled by quality-based contracts? Can a contracting system
support optimal multicast trees?
In this study, we have focused on bilateral contracts. This system
has seemed natural, especially since it is the prevalent system on the
current network. Perhaps its most important benefit is that each
contract is local in nature, so both parties share a common, familiar
legal jurisdiction. There is no need to worry about who will enforce
a punishment against another ISP on the opposite side of the planet,
nor is there a dispute over whose legal rules to apply in interpreting
a contract.
Although this benefit is compelling, it is worth considering other
systems. The clearest alternative is to form a contract between the
source and every node on the path. We may call these source
contracts. Source contracting may present surprising advantages.
For instance, since ISPs do not exchange money with each other, an
ISP cannot save money by selecting a cheaper next hop.
Additionally, if the source only has contracts with nodes on the
intended path, other nodes won"t even be willing to accept packets
from this source since they won"t receive compensation for carrying
them. This combination seems to eliminate all temptation for a
single cheater to cheat in route. Because of this and other
encouraging features, we believe source contracts are a fertile topic
for further study.
Another important research task is to relax our assumption that
quality can be measured fully and precisely. One possibility is to
assume that monitoring is only probabilistic or suffers from noise.
Even more relevant is the possibility that quality monitors are
fundamentally incomplete. A quality monitor can never anticipate
every dimension of quality that future applications will care about,
nor can it anticipate a new and valuable protocol that an ISP
introduces. We may define a monitor space as a subspace of the
quality space that a monitor can measure, QM ⊂ , and a
corresponding monitoring function that simply projects the full
range of qualities onto the monitor space, MQm →: .
Clearly, innovations that leave quality invariant under m are not
easy to support - they are invisible to the monitoring system. In this
environment, we expect that path monitoring becomes more
important, since it is the only way to ensure data reaches certain
innovator ISPs. Further research is needed to understand this
process.
