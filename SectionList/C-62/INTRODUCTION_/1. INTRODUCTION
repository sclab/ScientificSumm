Many studies before us have noted the Internet"s resistance to new
services and evolution. In recent decades, numerous ideas have
been developed in universities, implemented in code, and even
written into the routers and end systems of the network, only to
languish as network operators fail to turn them on on a large scale.
The list includes Multicast, IPv6, IntServ, and DiffServ. Lacking
the incentives just to activate services, there seems to be little hope
of ISPs devoting adequate resources to developing new ideas. In the
long term, this pathology stands out as a critical obstacle to the
network"s continued success (Ratnasamy, Shenker, and McCanne
provide extensive discussion in [11]).
On a smaller time scale, ISPs shun new services in favor of cost
cutting measures. Thus, the network has characteristics of a
commodity market. Although in theory, ISPs have a plethora of
routing policies at their disposal, the prevailing strategy is to route in
the cheapest way possible [2]. On one hand, this leads directly to
suboptimal routing. More importantly, commoditization in the short
term is surely related to the lack of innovation in the long term.
When the routing decisions of others ignore quality characteristics,
ISPs are motivated only to lower costs. There is simply no reward
for introducing new services or investing in quality improvements.
In response to these pathologies and others, researchers have put
forth various proposals for improving the situation. These can be
divided according to three high-level strategies: The first attempts
to improve the status quo by empowering end-users. Clark, et al.,
suggest that giving end-users control over routing would lead to
greater service diversity, recognizing that some payment mechanism
must also be provided [5]. Ratnasamy, Shenker, and McCanne
postulate a link between network evolution and user-directed
routing [11]. They propose a system of Anycast to give end-users
the ability to tunnel their packets to an ISP that introduces a
desirable protocol. The extra traffic to the ISP, the authors suggest,
will motivate the initial investment.
The second strategy suggests a revision of the contracting system.
This is exemplified by MacKie-Mason and Varian, who propose a
smart market to control access to network resources [10]. Prices
are set to the market-clearing level based on bids that users associate
to their traffic. In another direction, Afergan and Wroclawski
suggest that prices should be explicitly encoded in the routing
protocols [2]. They argue that such a move would improve stability
and align incentives.
The third high-level strategy calls for greater network
accountability. In this vein, Argyraki, et al., propose a system of
packet obituaries to provide feedback as to which ISPs drop packets
[3]. They argue that such feedback would help reveal which ISPs
were adequately meeting their contractual obligations. Unlike the
first two strategies, we are not aware of any previous studies that
have connected accountability with the pathologies of
commoditization or lack of innovation.
It is clear that these three strategies are closely linked to each other
(for example, [2], [5], and [9] each argue that giving end-users
routing control within the current contracting system is
problematic). Until today, however, the relationship between them
has been poorly understood. There is currently little theoretical
foundation to compare the relative merits of each proposal, and a
particular lack of evidence linking accountability with innovation
and service differentiation. This paper will address both issues.
We will begin by introducing an economic network model that
relates accountability, contracts, competition, and innovation. Our
model is highly stylized and may be considered preliminary: it is
based on a single source sending data to a single destination.
Nevertheless, the structure is rich enough to expose previously
unseen features of network behavior. We will use our model for
two main purposes:
First, we will use our model to argue that the lack of accountability
in today"s network is a fundamental obstacle to overcoming the
pathologies of commoditization and lack of innovation. In other
words, unless new monitoring capabilities are introduced, and
integrated with the system of contracts, the network cannot achieve
optimal routing and innovation characteristics. This result provides
motivation for the remainder of the paper, in which we explore how
accountability can be leveraged to overcome these pathologies and
create a sustainable industry. We will approach this problem from a
clean-slate perspective, deriving the level of accountability needed
to sustain an ideal competitive structure.
When we say that today"s Internet has poor accountability, we mean
that it reveals little information about the behavior - or misbehavior
- of ISPs. This well-known trait is largely rooted in the network"s
history. In describing the design philosophy behind the Internet
protocols, Clark lists accountability as the least important among
seven second level goals. [4] Accordingly, accountability
received little attention during the network"s formative years. Clark
relates this to the network"s military context, and finds that had the
network been designed for commercial development, accountability
would have been a top priority.
Argyraki, et al., conjecture that applying the principles of layering
and transparency may have led to the network"s lack of
accountability [3]. According to these principles, end hosts should
be informed of network problems only to the extent that they are
required to adapt. They notice when packet drops occur so that they
can perform congestion control and retransmit packets. Details of
where and why drops occur are deliberately concealed.
The network"s lack of accountability is highly relevant to a
discussion of innovation because it constrains the system of
contracts. This is because contracts depend upon external
institutions to function - the judge in the language of incomplete
contract theory, or simply the legal system. Ultimately, if a judge
cannot verify that some condition holds, she cannot enforce a
contract based on that condition. Of course, the vast majority of
contracts never end up in court. Especially when a judge"s ruling is
easily predicted, the parties will typically comply with the contract
terms on their own volition. This would not be possible, however,
without the judge acting as a last resort.
An institution to support contracts is typically complex, but we
abstract it as follows: We imagine that a contract is an algorithm
that outputs a payment transfer among a set of ISPs (the parties) at
every time. This payment is a function of the past and present
behaviors of the participants, but only those that are verifiable.
Hence, we imagine that a contract only accepts proofs as inputs.
We will call any process that generates these proofs a contractible
monitor. Such a monitor includes metering or sensing devices on
the physical network, but it is a more general concept. Constructing
a proof of a particular behavior may require readings from various
devices distributed among many ISPs. The contractible monitor
includes whatever distributed algorithmic mechanism is used to
motivate ISPs to share this private information.
Figure 1 demonstrates how our model of contracts fits together. We
make the assumption that all payments are mediated by contracts.
This means that without contractible monitors that attest to, say,
latency, payments cannot be conditioned on latency.
Figure 1: Relationship between monitors and contracts
With this model, we may conclude that the level of accountability in
today"s Internet only permits best effort contracts. Nodes cannot
condition payments on either quality or path characteristics.
Is there anything wrong with best-effort contracts? The reader
might wonder why the Internet needs contracts at all. After all, in
non-network industries, traditional firms invest in research and
differentiate their products, all in the hopes of keeping their
customers and securing new ones. One might believe that such
market forces apply to ISPs as well. We may adopt this as our null
hypothesis:
Null hypothesis: Market forces are sufficient to maintain service
diversity and innovation on a network, at least to the same extent
as they do in traditional markets.
There is a popular intuitive argument that supports this hypothesis,
and it may be summarized as follows:
Intuitive argument supporting null hypothesis:
1. Access providers try to increase their quality to get more
consumers.
2. Access providers are themselves customers for second hop
ISPs, and the second hops will therefore try to provide 
highquality service in order to secure traffic from access
providers. Access providers try to select high quality transit
because that increases their quality.
3. The process continues through the network, giving every
ISP a competitive reason to increase quality.
We are careful to model our network in continuous time, in order to
capture the essence of this argument. We can, for example, specify
equilibria in which nodes switch to a new next hop in the event of a
quality drop.
Moreover, our model allows us to explore any theoretically possible
punishments against cheaters, including those that are costly for
end-users to administer. By contrast, customers in the real world
rarely respond collectively, and often simply seek the best deal
currently offered. These constraints limit their ability to punish
cheaters.
Even with these liberal assumptions, however, we find that we must
reject our null hypothesis. Our model will demonstrate that
identifying a cheating ISP is difficult under low accountability,
limiting the threat of market driven punishment. We will define an
index of commoditization and show that it increases without bound
as data paths grow long. Furthermore, we will demonstrate a
framework in which an ISP"s maximum research investment
decreases hyperbolically with its distance from the end-user.
Network
Behavior
Monitor Contract
Proof
Payments
184
To summarize, we argue that the Internet"s lack of accountability
must be addressed before the pathologies of commoditization and
lack of innovation can be resolved. This leads us to our next topic:
How can we leverage accountability to overcome these pathologies?
We approach this question from a clean-slate perspective. Instead
of focusing on incremental improvements, we try to imagine how an
ideal industry would behave, then derive the level of accountability
needed to meet that objective. According to this approach, we first
craft a new equilibrium concept appropriate for network
competition. Our concept includes the following requirements:
First, we require that punishing ISPs that cheat is done without
rerouting the path. Rerouting is likely to prompt end-users to switch
providers, punishing access providers who administer punishments
correctly. Next, we require that the equilibrium cannot be
threatened by a coalition of ISPs that exchanges illicit side
payments. Finally, we require that the punishment mechanism that
enforces contracts does not punish innocent nodes that are not in the
coalition.
The last requirement is somewhat unconventional from an economic
perspective, but we maintain that it is crucial for any reasonable
solution. Although ISPs provide complementary services when they
form a data path together, they are likely to be horizontal
competitors as well. If innocent nodes may be punished, an ISP
may decide to deliberately cheat and draw punishment onto itself
and its neighbors. By cheating, the ISP may save resources, thereby
ensuring that the punishment is more damaging to the other ISPs,
which probably compete with the cheater directly for some
customers. In the extreme case, the cheater may force the other
ISPs out of business, thereby gaining a monopoly on some routes.
Applying this equilibrium concept, we derive the monitors needed
to maintain innovation and optimize routes. The solution is
surprisingly simple: contractible monitors must report the quality of
the rest of the path, from each ISP to the destination. It turns out
that this is the correct minimum accountability requirement, as
opposed to either end-to-end monitors or hop-by-hop monitors, as
one might initially suspect.
Rest of path monitors can be implemented in various ways. They
may be purely local algorithms that listen for packet echoes.
Alternately, they can be distributed in nature. We describe a way to
construct a rest of path monitor out of monitors for individual ISP
quality and for the data path. This requires a mechanism to
motivate ISPs to share their monitor outputs with each other. The
rest of path monitor then includes the component monitors and the
distributed algorithmic mechanism that ensures that information is
shared as required. This example shows that other types of monitors
may be useful as building blocks, but must be combined to form rest
of path monitors in order to achieve ideal innovation characteristics.
Our study has several practical implications for future protocol
design. We show that new monitors must be implemented and
integrated with the contracting system before the pathologies of
commoditization and lack of innovation can be overcome.
Moreover, we derive exactly what monitors are needed to optimize
routes and support innovation. In addition, our results provide
useful input for clean-slate architectural design, and we use several
novel techniques that we expect will be applicable to a variety of
future research.
The rest of this paper is organized as follows: In section 2, we lay
out our basic network model. In section 3, we present a 
lowaccountability network, modeled after today"s Internet. We
demonstrate how poor monitoring causes commoditization and a
lack of innovation. In section 4, we present verifiable monitors, and
show that proofs, even without contracts, can improve the status
quo. In section 5, we turn our attention to contractible monitors.
We show that rest of path monitors can support competition games
with optimal routing and innovation. We further show that rest of
path monitors are required to support such competition games. We
continue by discussing how such monitors may be constructed using
other monitors as building blocks. In section 6, we conclude and
present several directions for future research.
