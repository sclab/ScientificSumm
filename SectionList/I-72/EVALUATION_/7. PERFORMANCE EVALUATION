We evaluate the performance of the proposed systems in respect
to learning technique they used, DCEA and ID3, by comparing
them with the CEA, RO (for random offering), and SCR (offering
based on current request only).
We apply a variety of scenarios on this dataset in order to see the
performance differences. Each test scenario contains a list of 
preferences for the user and number of matches from the product list.
Table 3 shows these preferences and availability of those products
in the inventory for first five scenarios. Note that these preferences
are internal to the consumer and the producer tries to learn these
during negotiation.
Table 3: Availability of wines in different test scenarios
ID Preference of consumer Availability (out of 19)
1 Dry wine 15
2 Red and dry wine 8
3 Red, dry and moderate wine 4
4 Red and strong wine 2
5 Red or rose, and strong 3
7.1 Comparison of Learning Algorithms
In comparison of learning algorithms, we use the five scenarios
in Table 3. Here, first we use Tversky"s similarity measure. With
these test cases, we are interested in finding the number of 
iterations that are required for the producer to generate an acceptable
offer for the consumer. Since the performance also depends on the
initial request, we repeat our experiments with different initial 
requests. Consequently, for each case, we run the algorithms five
times with several variations of the initial requests. In each 
experiment, we count the number of iterations that were needed to reach
an agreement. We take the average of these numbers in order to
evaluate these systems fairly. As is customary, we test each 
algorithm with the same initial requests.
Table 4 compares the approaches using different learning 
algorithm. When the large parts of inventory is compatible with the
customer"s preferences as in the first test case, the performance of
all techniques are nearly same (e.g., Scenario 1). As the number of
compatible services drops, RO performs poorly as expected. The
second worst method is SCR since it only considers the customer"s
most recent request and does not learn from previous requests.
CEA gives the best results when it can generate an answer but 
cannot handle the cases containing disjunctive preferences, such as the
one in Scenario 5. ID3 and DCEA achieve the best results. Their
performance is comparable and they can handle all cases including
Scenario 5.
Table 4: Comparison of learning algorithms in terms of average
number of interactions
Run DCEA SCR RO CEA ID3
Scenario 1: 1.2 1.4 1.2 1.2 1.2
Scenario 2: 1.4 1.4 2.6 1.4 1.4
Scenario 3: 1.4 1.8 4.4 1.4 1.4
Scenario 4: 2.2 2.8 9.6 1.8 2
Scenario 5: 2 2.6 7.6 1.75+ No offer 1.8
Avg. of all cases: 1.64 2 5.08 1.51+No offer 1.56
7.2 Comparison of Similarity Metrics
To compare the similarity metrics that were explained in 
Section 5, we fix the learning algorithm to DCEA. In addition to the
scenarios shown in Table 3, we add following five new scenarios
considering the hierarchical information.
• The customer wants to buy wine whose winery is located in
California and whose grape is a type of white grape. 
Moreover, the winery of the wine should not be expensive. There
are only four products meeting these conditions.
• The customer wants to buy wine whose color is red or rose
and grape type is red grape. In addition, the location of wine
should be in Europe. The sweetness degree is wished to be
dry or off dry. The flavor should be delicate or moderate
where the body should be medium or light. Furthermore, the
winery of the wine should be an expensive winery. There are
two products meeting all these requirements.
• The customer wants to buy moderate rose wine, which is 
located around French Region. The category of winery should
be Moderate Winery. There is only one product meeting
these requirements.
• The customer wants to buy expensive red wine, which is 
located around California Region or cheap white wine, which
is located in around Texas Region. There are five available
products.
• The customer wants to buy delicate white wine whose 
producer in the category of Expensive Winery. There are two
available products.
The first seven scenarios are tested with the first dataset that 
contains a total of 19 services and the last three scenarios are tested
with the second dataset that contains 50 services.
Table 5 gives the performance evaluation in terms of the number
of interactions needed to reach a consensus. Tversky"s metric gives
the worst results since it does not consider the semantic similarity.
Lin"s performance are better than Tversky but worse than others.
Wu Palmer"s metric and RP similarity measure nearly give the same
performance and better than others. When the results are examined,
considering semantic closeness increases the performance.
