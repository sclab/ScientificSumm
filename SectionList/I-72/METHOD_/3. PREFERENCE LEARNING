The requests of the consumer and the counter offers of the 
producer are represented as vectors, where each element in the vector
corresponds to the value of a feature. The requests of the consumers
represent individual wine products whereas their preferences are
constraints over service features. For example, a consumer may
have preference for red wine. This means that the consumer is
willing to accept any wine offered by the producers as long as the
color is red. Accordingly, the consumer generates a request where
the color feature is set to red and other features are set to arbitrary
values, e.g. (Medium, Strong, Red).
At the beginning of negotiation, the producer agent does not
know the consumer"s preferences but will need to learn them 
using information obtained from the dialogues between the producer
and the consumer. The preferences denote the relative importance
of the features of the services demanded by the consumer agents.
For instance, the color of the wine may be important so the 
consumer insists on buying the wine whose color is red and rejects all
1302 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)
Table 1: How DCEA works
Type Sample The most The most
general set specific set
+ (Full,Strong,White) {(?, ?, ?)} {(Full,Strong,White)}
{{(?-Full), ?, ? },
- (Full,Delicate,Rose) {?, (?-Delicate), ?}, {(Full,Strong,White)}
{?, ?, (?-Rose)}}
{{(?-Full), ?, ?}, {{(Full,Strong,White)},
+ (Medium,Moderate,Red) {?,(?-Delicate), ?}, {(Medium,Moderate,Red)}}
{?, ?, (?-Rose)}}
the offers involving the wine whose color is white or rose. On the
contrary, the winery may not be as important as the color for this
customer, so the consumer may have a tendency to accept wines
from any winery as long as the color is red.
To tackle this problem, we propose to use incremental learning
algorithms [6]. This is necessary since no training data is 
available before the interactions start. We particularly investigate two
approaches. The first one is inductive learning. This technique is
applied to learn the preferences as concepts. We elaborate on 
Candidate Elimination Algorithm (CEA) for Version Space [10]. CEA
is known to perform poorly if the information to be learned is 
disjunctive. Interestingly, most of the time consumer preferences are
disjunctive. Say, we are considering an agent that is buying wine.
The consumer may prefer red wine or rose wine but not white wine.
To use CEA with such preferences, a solid modification is 
necessary. The second approach is decision trees. Decision trees can
learn from examples easily and classify new instances as positive
or negative. A well-known incremental decision tree is ID5R [18].
However, ID5R is known to suffer from high computational 
complexity. For this reason, we instead use the ID3 algorithm [13] and
iteratively build decision trees to simulate incremental learning.
3.1 CEA
CEA [10] is one of the inductive learning algorithms that learns
concepts from observed examples. The algorithm maintains two
sets to model the concept to be learned. The first set is the most
general set G. G contains hypotheses about all the possible values
that the concept may obtain. As the name suggests, it is a 
generalization and contains all possible values unless the values have
been identified not to represent the concept. The second set is the
most specific set S. S contains only hypotheses that are known to
identify the concept that is being learned. At the beginning of the
algorithm, G is initialized to cover all possible concepts while S is
initialized to be empty.
During the interactions, each request of the consumer can be 
considered as a positive example and each counter offer generated by
the producer and rejected by the consumer agent can be thought of
as a negative example. At each interaction between the producer
and the consumer, both G and S are modified. The negative 
samples enforce the specialization of some hypotheses so that G does
not cover any hypothesis accepting the negative samples as 
positive. When a positive sample comes, the most specific set S should
be generalized in order to cover the new training instance. As a 
result, the most general hypotheses and the most special hypotheses
cover all positive training samples but do not cover any negative
ones. Incrementally, G specializes and S generalizes until G and
S are equal to each other. When these sets are equal, the algorithm
converges by means of reaching the target concept.
3.2 Disjunctive CEA
Unfortunately, CEA is primarily targeted for conjunctive 
concepts. On the other hand, we need to learn disjunctive concepts in
the negotiation of a service since consumer may have several 
alternative wishes. There are several studies on learning disjunctive
concepts via Version Space. Some of these approaches use multiple
version space. For instance, Hong et al. maintain several version
spaces by split and merge operation [7]. To be able to learn 
disjunctive concepts, they create new version spaces by examining the
consistency between G and S.
We deal with the problem of not supporting disjunctive concepts
of CEA by extending our hypothesis language to include 
disjunctive hypothesis in addition to the conjunctives and negation. Each
attribute of the hypothesis has two parts: inclusive list, which holds
the list of valid values for that attribute and exclusive list, which is
the list of values which cannot be taken for that feature.
EXAMPLE 1. Assume that the most specific set is {(Light, 
Delicate, Red)} and a positive example, (Light, Delicate, White) comes.
The original CEA will generalize this as (Light, Delicate, ?), 
meaning the color can take any value. However, in fact, we only know
that the color can be red or white. In the DCEA, we generalize it as
{(Light, Delicate, [White, Red] )}. Only when all the values exist
in the list, they will be replaced by ?. In other words, we let the
algorithm generalize more slowly than before.
We modify the CEA algorithm to deal with this change. The
modified algorithm, DCEA, is given as Algorithm 1. Note that
compared to the previous studies of disjunctive versions, our 
approach uses only a single version space rather than multiple version
space. The initialization phase is the same as the original algorithm
(lines 1, 2). If any positive sample comes, we add the sample to the
special set as before (line 4). However, we do not eliminate the 
hypotheses in G that do not cover this sample since G now contains a
disjunction of many hypotheses, some of which will be conflicting
with each other. Removing a specific hypothesis from G will result
in loss of information, since other hypotheses are not guaranteed
to cover it. After some time, some hypotheses in S can be merged
and can construct one hypothesis (lines 6, 7).
When a negative sample comes, we do not change S as before.
We only modify the most general hypotheses not to cover this 
negative sample (lines 11-15). Different from the original CEA, we
try to specialize the G minimally. The algorithm removes the 
hypothesis covering the negative sample (line 13). Then, we generate
new hypotheses as the number of all possible attributes by using
the removed hypothesis.
For each attribute in the negative sample, we add one of them
at each time to the exclusive list of the removed hypothesis. Thus,
all possible hypotheses that do not cover the negative sample are
generated (line 14). Note that, exclusive list contains the values that
the attribute cannot take. For example, consider the color attribute.
If a hypothesis includes red in its exclusive list and ? in its inclusive
list, this means that color may take any value except red.
The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1303
Algorithm 1 Disjunctive Candidate Elimination Algorithm
1: G ←the set of maximally general hypotheses in H
2: S ←the set of maximally specific hypotheses in H
3: For each training example, d
4: if d is a positive example then
5: Add d to S
6: if s in S can be combined with d to make one element then
7: Combine s and d into sd {sd is the rule covers s and d}
8: end if
9: end if
10: if d is a negative example then
11: For each hypothesis g in G does cover d
12: * Assume : g = (x1, x2, ..., xn) and d = (d1, d2, ..., dn)
13: - Remove g from G
14: - Add hypotheses g1, g2, gn where g1= (x1-d1, x2,..., xn),
g2= (x1, x2-d2,..., xn),..., and gn= (x1, x2,..., xn-dn)
15: - Remove from G any hypothesis that is less general than
another hypothesis in G
16: end if
EXAMPLE 2. Table 1 illustrates the first three interactions and
the workings of DCEA. The most general set and the most specific
set show the contents of G and S after the sample comes in. After
the first positive sample, S is generalized to also cover the instance.
The second sample is negative. Thus, we replace (?, ?, ?) by three
disjunctive hypotheses; each hypothesis being minimally 
specialized. In this process, at each time one attribute value of negative
sample is applied to the hypothesis in the general set. The third
sample is positive and generalizes S even more.
Note that in Table 1, we do not eliminate {(?-Full), ?, ?} from
the general set while having a positive sample such as (Full, Strong,
White). This stems from the possibility of using this rule in the 
generation of other hypotheses. For instance, if the example continues
with a negative sample (Full, Strong, Red), we can specialize the
previous rule such as {(?-Full), ?, (?-Red)}. By Algorithm 1, we
do not miss any information.
3.3 ID3
ID3 [13] is an algorithm that constructs decision trees in a 
topdown fashion from the observed examples represented in a vector
with attribute-value pairs. Applying this algorithm to our system
with the intention of learning the consumer"s preferences is 
appropriate since this algorithm also supports learning disjunctive 
concepts in addition to conjunctive concepts.
The ID3 algorithm is used in the learning process with the 
purpose of classification of offers. There are two classes: positive and
negative. Positive means that the service description will possibly
be accepted by the consumer agent whereas the negative implies
that it will potentially be rejected by the consumer. Consumer"s 
requests are considered as positive training examples and all rejected
counter-offers are thought as negative ones.
The decision tree has two types of nodes: leaf node in which the
class labels of the instances are held and non-leaf nodes in which
test attributes are held. The test attribute in a non-leaf node is one of
the attributes making up the service description. For instance, body,
flavor, color and so on are potential test attributes for wine service.
When we want to find whether the given service description is 
acceptable, we start searching from the root node by examining the
value of test attributes until reaching a leaf node.
The problem with this algorithm is that it is not an 
incremental algorithm, which means all the training examples should exist
before learning. To overcome this problem, the system keeps 
consumer"s requests throughout the negotiation interaction as positive
examples and all counter-offers rejected by the consumer as 
negative examples. After each coming request, the decision tree is
rebuilt. Without doubt, there is a drawback of reconstruction such
as additional process load. However, in practice we have evaluated
ID3 to be fast and the reconstruction cost to be negligible.
