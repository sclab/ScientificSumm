Above we gave an intuitive definition of reusability: a
collection is reusable if we can trust our estimates of 
confidence in an evaluation. By that we mean that if we have
made some relevance judgments and have, for example, 75%
confidence that system A is better than system B, we would
like there to be no more than 25% chance that our 
assessment of the relative quality of the systems will change as
we continue to judge documents. Our evaluation should be
robust to missing judgments.
In our previous work, we defined confidence as the 
probability that the difference in an evaluation measure calculated
for two systems is less than zero [8]. This notion of 
confidence is defined in the context of a particular evaluation
task that we call comparative evaluation: determining the
sign of the difference in an evaluation measure. Other 
evaluation tasks could be defined; estimating the magnitude of
the difference or the values of the measures themselves are
examples that entail different notions of confidence.
We therefore see confidence as a probability estimate. One
of the questions we must ask about a probability estimate is
what it means. What does it mean to have 75% confidence
that system A is better than system B? As described above,
we want it to mean that if we continue to judge documents,
there will only be a 25% chance that our assessment will
change. If this is what it means, we can trust the confidence
estimates. But do we know it has that meaning?
Our calculation of confidence rested on an assumption
about the probability of relevance of unjudged documents,
specifically that each unjudged document was equally likely
to be relevant or nonrelevant. This assumption is almost
certainly not realistic in most IR applications. As it turns
out, it is this assumption that determines whether the 
confidence estimates can eb trusted. Before elaborating on this,
we formally define confidence.
2.1 Estimating Confidence
Average precision (AP) is a standard evaluation metric
that captures both the ability of a system to rank relevant
documents highly (precision) as well as its ability to retrieve
relevant documents (recall). It is typically written as the
mean precision at the ranks of relevant documents:
AP =
1
|R| i∈R
prec@r(i)
where R is the set of relevant documents and r(i) is the rank
of document i. Let Xi be a random variable indicating the
relevance of document i. If documents are ordered by rank,
we can express precision as prec@i = 1/i i
j=1 Xj .
Average precision then becomes the quadratic equation
AP =
1
Xi
n
i=1
Xi/i
i
j=1
Xj
=
1
Xi
n
i=1 j≥i
aijXiXj
where aij = 1/ max{r(i), r(j)}. Using aij instead of 1/i 
allows us to number the documents arbitrarily. To see why
this is true, consider a toy example: a list of 3 documents
with relevant documents B, C at ranks 1 and 3 and 
nonrelevant document A at rank 2. Average precision will be
1
2
(1
1
x2
B+ 1
2
xBxA+ 1
3
xBxC + 1
2
x2
A+ 1
3
xAxC + 1
3
x2
C) = 1
2
1 + 2
3
because xA = 0, xB = 1, xC = 1. Though the ordering
B, A, C is different from the labeling A, B, C, it does not
affect the computation.
We can now see average precision itself is a random 
variable with a distribution over all possible assignments of 
relevance to all documents. This random variable has an 
expectation, a variance, confidence intervals, and a certain
probability of being less than or equal to a given value.
All of these are dependent on the probability that 
document i is relevant: pi = p(Xi = 1). Suppose in our 
previous example we do not know the relevance judgments,
but we believe pA = 0.4, pB = 0.8, pC = 0.7. We can
then compute e.g. P(AP = 0) = 0.2 · 0.6 · 0.3 = 0.036,
or P(AP = 1
2
) = 0.2 · 0.4 · 0.7 = 0.056.
Summing over all possibilities, we can compute 
expectation and variance:
E[AP] ≈
1
pi
aiipi +
j>i
aij pipj
V ar[AP] ≈
1
( pi)2
n
i
a2
iipiqi +
j>i
a2
ijpipj(1 − pipj)
+
i=j
2aiiaijpipj(1 − pi) +
k>j=i
2aijaikpipjpk(1 − pi)
AP asymptotically converges to a normal distribution with
expectation and variance as defined above.1
For our comparative evaluation task we are interested in
the sign of the difference in two average precisions: ΔAP =
AP1 − AP2. As we showed in our previous work, ΔAP has
a closed form when documents are ordered arbitrarily:
ΔAP =
1
Xi
n
i=1 j≥i
cij XiXj
cij = aij − bij
where bij is defined analogously to aij for the second 
ranking. Since AP is normal, ΔAP is normal as well, meaning
we can use the normal cumulative density function to 
determine the confidence that a difference in AP is less than
zero.
Since topics are independent, we can easily extend this
to mean average precision (MAP). MAP is also normally
distributed; its expectation and variance are:
EMAP =
1
T t∈T
E[APt] (1)
VMAP =
1
T2
t∈T
V ar[APt]
ΔMAP = MAP1 − MAP2
Confidence can then be estimated by calculating the 
expectation and variance and using the normal density function
to find P(ΔMAP < 0).
2.2 Confidence and Robustness
Having defined confidence, we turn back to the issue of
trust in confidence estimates, and show how it ties into the
robustness of the collection to missing judgments.
1
These are actually approximations to the true expectation
and variance, but the error is a negligible O(n2−n
).
Let Z be the set of all pairs of ranked results for a 
common set of topics. Suppose we have a set of m relevance
judgments xm
= {x1, x2, ..., xm} (using small x rather than
capital X to distinguish between judged and unjudged 
documents); these are the judgments against which we compute
confidence. Let Zα be the subset of pairs in Z for which
we predict that ΔMAP = −1 with confidence α given the
judgments xm
. For the confidence estimates to be 
accurate, we need at least α · |Zα| of these pairs to actually have
ΔMAP = −1 after we have judged every document. If they
do, we can trust the confidence estimates; our evaluation
will be robust to missing judgments.
If our confidence estimates are based on unrealistic 
assumptions, we cannot expect them to be accurate. The
assumptions they are based on are the probabilities of 
relevance pi. We need these to be realistic.
We argue that the best possible distribution of relevance
p(Xi) is the one that explains all of the data (all of the
observations made about the retrieval systems) while at the
same time making no unwarranted assumptions. This is
known as the principle of maximum entropy [13].
The entropy of a random variable X with distribution
p(X) is defined as H(p) = − i p(X = i) log p(X = i).
This has found a wide array of uses in computer science and
information retrieval. The maximum entropy distribution
is the one that maximizes H. This distribution is unique
and has an exponential form. The following theorem shows
the utility of a maximum entropy distribution for relevance
when estimating confidence.
Theorem 1. If p(Xn
|I, xm
) = argmaxpH(p), confidence
estimates will be accurate.
where xm
is the set of relevance judgments defined above,
Xn
is the full set of documents that we wish to estimate the
relevance of, and I is some information about the documents
(unspecified as of now). We forgo the proof for the time
being, but it is quite simple.
This says that the better the estimates of relevance, the
more accurate the evaluation. The task of creating a reusable
test collection thus becomes the task of estimating the 
relevance of unjudged documents.
The theorem and its proof say nothing whatsoever about
the evaluation metric. The probability estimates are entirely
indepedent of the measure we are interested in. This means
the same probability estimates can tell us about average
precision as well as precision, recall, bpref, etc.
Furthermore, we could assume that the relevance of 
documents i and j is independent and achieve the same result,
which we state as a corollary:
Corollary 1. If p(Xi|I, xm
) = argmaxpH(p), confidence
estimates will be accurate.
The task therefore becomes the imputation of the missing
values of relevance. The theorem implies that the closer we
get to the maximum entropy distribution of relevance, the
closer we get to robustness.
