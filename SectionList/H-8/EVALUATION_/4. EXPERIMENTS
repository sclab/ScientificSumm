Three hypotheses are under consideration. The first, and
most important, is that using our expert aggregation model
to predict relevance produces test collections that are robust
enough to be reusable; that is, we can trust the estimates of
confidence when we evaluate systems that did not contribute
any judgments to the pool.
The other two hypotheses relate to the improvement we
see by using better estimates of relevance than we did in our
previous work [8]. These are that (a) it takes fewer relevance
track no. topics no. runs no. judged no. rel
ad-hoc 94 50 40 97,319 9,805
ad-hoc 95 49 33 87,069 6,503
ad-hoc 96 50 61 133,681 5,524
ad-hoc 97 50 74 72,270 4,611
ad-hoc 98 50 103 80,345 4,674
ad-hoc 99 50 129 86,830 4,728
web 04 225 74 88,566 1,763
robust 05 50 74 37,798 6,561
terabyte 05 50 58 45,291 10,407
Table 1: Number of topics, number of runs, number
of documents judged, and number found relevant for
each of our data sets.
judgments to reach 95% confidence and (b) the accuracy of
the predictions is higher than if we were to simply assume
pi = 0.5 for all unjudged documents.
4.1 Data
We obtained full ad-hoc runs submitted to TRECs 3
through 8. Each run ranks at most 1000 documents for 50
topics (49 topics for TREC 4). Additionally, we obtained
all runs from the Web track of TREC 13, the Robust2
track
of TREC 14, and the Terabyte (ad-hoc) track of TREC 14.
These are the tracks that have replaced the ad-hoc track
since its end in 1999. Statistics are shown in Table 1.
We set aside the TREC 4 (ad-hoc 95) set for training,
TRECs 3 and 5-8 (ad-hoc 94 and 96-99) for primary testing,
and the remaining sets for additional testing.
We use the qrels files assembled by NIST as truth. The
number of relevance judgments made and relevant 
documents found for each track are listed in Table 1.
For computational reasons, we truncate ranked lists at
100 documents. There is no reason that we could not go
deeper, but calculating variance is O(n3
) and thus very 
timeconsuming. Because of the reciprocal rank nature of AP, we
do not lose much information by truncating at rank 100.
4.2 Algorithms
We will compare three algorithms for acquiring relevance
judgments. The baseline is a variation of TREC pooling that
we will call incremental pooling (IP). This algorithm takes
a number k as input and presents the first k documents in
rank order (without regard to topic) to be judged. It does
not estimate the relevance of unjudged documents; it simply
assumes any unjudged document is nonrelevant.
The second algorithm is that presented in Carterette et
al. [8] (Algorithm 1). Documents are selected based on how
interesting they are in determining whether a difference
in mean average precision exists. For this approach pi = 0.5
for all i; there is no estimation of probabilities. We will call
this MTC for minimal test collection.
The third algorithm augments MTC with updated 
estimates of probabilities of relevance. We will call this RTC
for robust test collection. It is identical to Algorithm 1, 
except that every 10th iteration we estimate pi for all unjudged
documents i using the expert aggregation model of Section 3.
RTC has smoothing (prior distribution) parameters that
must be set. We trained using the ad-hoc 95 set. We limited
2
Robust here means robust retrieval; this is different from
our goal of robust evaluation.
Algorithm 1 (MTC) Given two ranked lists and confidence
level α, predict the sign of ΔMAP.
1: pi ← 0.5 for all documents i
2: while P(ΔMAP < 0) < α do
3: calculate weight wi for all unjudged documents i
(see Carterette et al. [8] for details)
4: j ← argmaxiwi
5: xj ← 1 if document j is relevant, 0 otherwise
6: pj ← xj
7: end while
the search to uniform priors with relatively high variance.
For expert aggregation, the prior parameters are α = β = 1.
4.3 Experimental Design
First, we want to know whether we can augment a set
of relevance judgments with a set of relevance probabilities
in order to reuse the judgments to evaluate a new set of
systems. For each experimental trial:
1. Pick a random subset of k runs.
2. From those k, pick an initial c < k to evaluate.
3. Run RTC to 95% confidence on the initial c.
4. Using the model from Section 3, estimate the 
probabilities of relevance for all documents retrieved by all
k runs.
5. Calculate EMAP for all k runs, and P(ΔMAP < 0)
for all pairs of runs.
We do the same for MTC, but omit step 4. Note that 
after evaluating the first c systems, we make no additional
relevance judgments.
To put our method to the test, we selected c = 2: we will
build a set of judgments from evaluating only two initial
systems. We will then generalize to a set of k = 10 (of
which those two are a subset).
As we run more trials, we obtain the data we need to test
all three of our hypotheses.
4.4 Experimental Evaluation
Recall that a set of judgments is robust if the accuracy of
the predictions it makes is at least its estimated confidence.
One way to evaluate robustness is to bin pairs by their 
confidence, then calculate the accuracy over all the pairs in each
bin. We would like the accuracy to be no less than the lowest
confidence score in the bin, but preferably higher.
Since summary statistics are useful, we devised the 
following metric. Suppose we are a bookmaker taking bets
on whether ΔMAP < 0. We use RTC or MTC to set the
odds O = P (ΔMAP <0)
1−P (ΔMAP <0)
. Suppose a bettor wagers $1 on
ΔMAP ≥ 0. If it turns out that ΔMAP < 0, we win the
dollar. Otherwise, we pay out O. If our confidence 
estimates are perfectly accurate, we break even. If confidence
is greater than accuracy, we lose money; we win if accuracy
is greater than confidence.
Counterintuitively, the most desirable outcome is 
breaking even: if we lose money, we cannot trust the confidence
estimates, but if we win money, we have either 
underestimated confidence or judged more documents than necessary.
However, the cost of not being able to trust the confidence
estimates is higher than the cost of extra relevance 
judgments, so we will treat positive outcomes as good.
The amount we win on each pairwise comparison i is:
Wi = yi − (1 − yi)
Pi
1 − Pi
=
yi − Pi
1 − Pi
yi = 1 if ΔMAP < 0 and 0 otherwise, and Pi = P(ΔMAP <
0). The summary statistic is W, the mean of Wi.
Note that as Pi increases, we lose more for being wrong.
This is as it should be: the penalty should be great for
missing the high probability predictions. However, since our
losses grow without bound as predictions approach certainty,
we cap −Wi at 100.
For our hypothesis that RTC requires fewer judgments
than MTC, we are interested in the number of judgments
needed to reach 95% confidence on the first pair of systems.
The median is more interesting than the mean: most pairs
require a few hundred judgments, but a few pairs require 
several thousand. The distribution is therefore highly skewed,
and the mean strongly affected by those outliers.
Finally, for our hypothesis that RTC is more accurate
than MTC, we will look at Kendall"s τ correlation between
a ranking of k systems by a small set of judgments and the
true ranking using the full set of judgments. Kendall"s τ,
a nonparametric statistic based on pairwise swaps between
two lists, is a standard evaluation for this type of study.
It ranges from −1 (perfectly anti-correlated) to 1 (rankings
identical), with 0 meaning that half of the pairs are swapped.
As we touched on in the introduction, though, an accuracy
measure like rank correlation is not a good evaluation of
reusability. We include it for completeness.
4.4.1 Hypothesis Testing
Running multiple trials allows the use of statistical 
hypothesis testing to compare algorithms. Using the same sets
of systems allows the use of paired tests.
As we stated above, we are more interested in the median
number of judgments than the mean. A test for difference
in median is the Wilcoxon sign rank test. We can also use
a paired t-test to test for a difference in mean.
For rank correlation, we can use a paired t-test to test for
a difference in τ.
