In this section, we study the computational power of 
information markets for a very simple class of aggregation 
functions: Boolean functions of n variables. We characterize the
set of Boolean functions that can be computed in our market
model for all prior distributions and then prove upper and
lower bounds on the worst-case convergence time for these
markets.
The information structure we assume is as follows: There
are n agents, and each agent i has a single bit of private 
information xi. We use x to denote the vector (x1, . . . , xn) of
inputs. All the agents also have a common prior probability
distribution P : {0, 1}n
→ [0, 1] over the values of x. We
define a Boolean aggregate function f(x) : {0, 1}n
→ {0, 1}
that we would like the market to compute. Note that x, and
hence f(x), is completely determined by the combination of
all the agents" information, but it is not known to any one
agent. The agents trade in a Boolean security F, which
pays off $1 if f(x) = 1 and $0 if f(x) = 0. So an omniscient
6
Risk neutrality implies that each agent"s utility for the 
security is linearly related to his or her subjective estimation of
the expected payoff of the security. Myopic behavior means
that agents treat each round as if it were the final round:
They do not reason about how their bids may affect the bids
of other agents in future rounds.
158
agent with access to all the agents" bits would know the true
value of security F-either exactly $1 or exactly $0. In 
reality, risk-neutral agents with limited information will value
F according to their expectation of its payoff, or Ei[f(x)],
where Ei is the expectation operator applied according to
agent i"s probability distribution.
For any function f, trading in F may happen to converge
to the true value of f(x) by coincidence if the prior 
probability distribution is sufficiently degenerate. More 
interestingly, we would like to know for which functions f does the
price of the security F always converge to f(x) for all prior
probability distributions P.7
In Section 4.2, we prove a 
necessary and sufficient condition that guarantees convergence.
In Section 4.3, we address the natural follow-up question,
by deriving upper and lower bounds on the worst-case 
number of rounds of trading required for the value of f(x) to be
revealed.
4.1 Equilibrium price characterization
Our analysis builds on a characterization of the 
equilibrium price of F that follows from a powerful result on 
common knowledge of aggregates due to McKelvey and Page [19],
later extended by Nielsen et al. [21].
Information markets aim to aggregate the knowledge of
all the agents. Procedurally, this occurs because the agents
learn from the markets: The price of the security conveys
information to each agent about the knowledge of other
agents. We can model the flow of information through prices
as follows.
Let Ω = {0, 1}n
be the set of possible values of x; we say
that Ω denotes the set of possible states of the world. The
prior P defines everyone"s initial belief about the likelihood
of each state. As trading proceeds, some possible states can
be logically ruled out, but the relative likelihoods among the
remaining states are fully determined by the prior P. So the
common knowledge after any stage is completely described
by the set of states that an external observer-with no 
information beyond the sequence of prices observed-considers
possible (along with the prior). Similarly, the knowledge of
agent i at any point is also completely described by the set
of states she considers possible. We use the notation Sr
to
denote the common-knowledge possibility set after round r,
and Sr
i to denote the set of states that agent i considers
possible after round r.
Initially, the only common knowledge is that the input
vector x is in Ω; in other words, the set of states considered
possible by an external observer before trading has occurred
is the set S0
= Ω. However, each agent i also knows the
value of her bit xi; thus, her knowledge set S0
i is the set
{y ∈ Ω|yi = xi}. Agent i"s first-round bid is her conditional
expectation of the event f(x) = 1 given that x ∈ S0
i . All
the agents" bids are processed, and the clearing price p1
is
announced. An external observer could predict agent i"s bid
if he knew the value of xi. Thus, if he knew the value of
x, he could predict the value of p1
. In other words, the
external observer knows the function price1
(x) that relates
the first round price to the true state x. Of course, he does
not know the value of x; however, he can rule out any vector
x that would have resulted in a different clearing price from
the observed price p1
.
7
We assume that the common prior is consistent with x in
the sense that it assigns a non-zero probability to the actual
value of x.
Thus, the common knowledge after round 1 is the set
S1
= {y ∈ S0
| price1
(y) = p1
}. Agent i knows the 
common knowledge and, in addition, knows the value of bit xi.
Hence, after every round r, the knowledge of agent i is given
by Sr
i = {y ∈ Sr
|yi = xi}. Note that, because knowledge
can only improve over time, we must always have Sr
i ⊆ Sr−1
i
and Sr
⊆ Sr−1
. Thus, only a finite number of changes
in each agent"s knowledge are possible, and so eventually
we must converge to an equilibrium after which no player
learns any further information. We use S∞
to denote the
common knowledge at this point, and S∞
i to denote agent
i"s knowledge at this point. Let p∞
denote the clearing price
at equilibrium.
Informally, McKelvey and Page [19] show that, if n 
people with common priors but different information about the
likelihood of some event A agree about a suitable 
aggregate of their individual conditional probabilities, then their
individual conditional probabilities of event A"s occurring
must be identical. (The precise definition of suitable is
described below.) There is a strong connection to rational
expectation equilibria in markets, which was noted in the
original McKelvey-Page paper: The market price of a 
security is common knowledge at the point of equilibrium. Thus,
if the price is a suitable aggregate of the conditional 
expectations of all the agents, then in equilibrium they must
have identical conditional expectations of the event that the
security will pay off. (Note that their information may still
be different.)
Definition 1. A function g : n
→ is called 
stochastically monotone if it can be written in the form g(x) =
i gi(xi), where each function gi : → is strictly 
increasing.
Bergin and Brandenburger [2] proved that this simple 
definition of stochastically monotone functions is equivalent to
the original definition in McKelvey-Page [19].
Definition 2. A function g : n
→ is called 
stochastically regular if it can be written in the form g = h ◦ g ,
where g is stochastically monotone and h is invertible on
the range of g .
We can now state the McKelvey-Page result, as generalized
by Nielsen et al. [21]. In our context, the following simple
theorem statement suffices; more general versions of this
theorem can be found in [19, 21].
Theorem 1. (Nielsen et al. [21]) Suppose that, at 
equilibrium, the n agents have a common prior, but possibly 
different information, about the value of a random variable F,
as described above. For all i, let p∞
i = E(F|x ∈ S∞
i ). If g
is a stochastically regular function and g(p∞
1 , p∞
2 , . . . , p∞
n ) is
common knowledge, then it must be the case that
p∞
1 = p∞
2 = · · · = p∞
n = E(F|x ∈ S∞
) = p∞
In one round of our simplified Shapley-Shubik trading
model, the announced price is the mean of the conditional
expectations of the n agents. The mean is a stochastically
regular function; hence, Theorem 1 shows that, at 
equilibrium, all agents have identical conditional expectations of
the payoff of the security. It follows that the equilibrium
159
price p∞
must be exactly the conditional expectations of all
agents at equilibrium.
Theorem 1 does not in itself say how the equilibrium is
reached. McKelvey and Page, extending an argument due
to Geanakoplos and Polemarchakis [10], show that repeated
announcement of the aggregate will eventually result in 
common knowledge of the aggregate. In our context, this is
achieved by announcing the current price at the end of each
round; this will ultimately converge to a state in which all
agents bid the same price p∞
.
However, reaching an equilibrium price is not sufficient for
the purposes of information aggregation. We also want the
price to reveal the actual value of f(x). It is possible that
the equilibrium price p∞
of the security F will not be either
0 or 1, and so we cannot infer the value of f(x) from it.
Example 1: Consider two agents 1 and 2 with private input
bits x1 and x2 respectively. Suppose the prior probability
distribution is uniform, i.e., x = (x1, x2) takes the values
(0, 0), (0, 1), (1, 0), and (1, 1) each with probability 1
4
. Now,
suppose the aggregate function we want to compute is the
XOR function, f(x) = x1 ⊕ x2. To this end, we design a
market to trade in a Boolean security F, which will 
eventually payoff $1 iff x1 ⊕ x2 = 1.
If agent 1 observes x1 = 1, she estimates the expected
value of F to be the probability that x2 = 0 (given x1 = 1),
which is 1
2
. If she observes x1 = 0, her expectation of the
value of F is the conditional probability that x2 = 1, which
is also 1
2
. Thus, in either case, agent 1 will bid 0.5 for F
in the first round. Similarly, agent 2 will also always bid
0.5 in the first round. Hence, the first round of trading
ends with a clearing price of 0.5. From this, agent 2 can
infer that agent 1 bid 0.5, but this gives her no information
about the value of x1-it is still equally likely to be 0 or
1. Agent 1 also gains no information from the first round
of trading, and hence neither agent changes her bid in the
following rounds. Thus, the market reaches equilibrium at
this point. As predicted by Theorem 1, both agents have the
same conditional expectation (0.5) at equilibrium. However,
the equilibrium price of the security F does not reveal the
value of f(x1, x2), even though the combination of agents"
information is enough to determine it precisely.
4.2 Characterizing computable aggregates
We now give a necessary and sufficient characterization of
the class of functions f such that, for any prior distribution
on x, the equilibrium price of F will reveal the true value
of f. We show that this is exactly the class of weighted
threshold functions:
Definition 3. A function f : {0, 1}n
→ {0, 1} is a 
weighted threshold function iff there are real constants w1, w2,
. . . , wn such that
f(x) = 1 iff
n
i=1
wixi ≥ 1
Theorem 2. If f is a weighted threshold function, then,
for any prior probability distribution P, the equilibrium price
of F is equal to f(x).
Proof:
Let S∞
i denote the possibility set of agent i at equilibrium.
As before, we use p∞
to denote the final trading price at
this point. Note that, by Theorem 1, p∞
is exactly agent i"s
conditional expectation of the value of f(x), given her final
possibility set S∞
i .
First, observe that if p∞
is 0 or 1, then we must have
f(x) = p∞
, regardless of the form of f. For instance, if
p∞
= 1, this means that E(f(y)|y ∈ S∞
) = 1. As f(·)
can only take the values 0 or 1, it follows that P(f(y) =
1|y ∈ S∞
) = 1. The actual value x is always in the final
possibility set S∞
, and, furthermore, it must have non-zero
prior probability, because it actually occurred. Hence, it
follows that f(x) = 1 in this case. An identical argument
shows that if p∞
= 0, f(x) = 0.
Hence, it is enough to show that, if f is a weighted 
threshold function, then p∞
is either 0 or 1. We prove this by 
contradiction. Let f(·) be a weighted threshold function 
corresponding to weights {wi}, and assume that 0 < p∞
< 1. By
Theorem 1, we must have:
P(f(y) = 1|y ∈ S∞
) = p∞
(1)
∀i P(f(y) = 1|y ∈ S∞
i ) = p∞
(2)
Recall that S∞
i = {y ∈ S∞
|yi = xi}. Thus, Equation (2)
can be written as
∀i P(f(y) = 1|y ∈ S∞
, yi = xi) = p∞
(3)
Now define
J+
i = P(yi = 1|y ∈ S∞
, f(y) = 1)
J−
i = P(yi = 1|y ∈ S∞
, f(y) = 0)
J+
=
n
i=1
wiJ+
i
J−
=
n
i=1
wiJ−
i
Because by assumption p∞
= 0, 1, both J+
i and J−
i are
well-defined (for all i): Neither is conditioned on a 
zeroprobability event.
Claim: Eqs. 1 and 3 imply that J+
i = J−
i , for all i.
Proof of claim: We consider the two cases xi = 1 and
xi = 0 separately.
Case (i): xi = 1. We can assume that J−
i and J+
i are not
both 0 (or else, the claim is trivially true). In this case, we
have
P(f(y) = 1|y ∈ S∞
) · J+
i
P(f(y) = 1|y ∈ S∞) · J+
i + P(f(y) = 0|y ∈ S∞) · J−
i
= P(f(y) = 1|yi = 1, y ∈ S∞
) (Bayes" law)
p∞
J+
i
p∞J+
i + (1 − p∞)J−
i
= p∞
(by Eqs. 1 and 3)
J+
i = p∞
J+
i + (1 − p∞
)J−
i
=⇒ J+
i = J−
i (as p∞
= 1)
Case (ii): xi = 0. When xi = 0, observe that the argument
of Case (i) can be used to prove that (1 − J+
i ) = (1 − J−
i ).
It immediately follows that J+
i = J−
i as well. 2
Hence, we must also have J+
= J−
. But using linearity
of expectation, we can also write J+
as
J+
= E
n
i=1
wiyi y ∈ S∞
, f(y) = 1 ,
160
and, because f(y) = 1 only when i wiyi ≥ 1, this gives us
J+
≥ 1. Similarly,
J−
= E
n
i=1
wiyi y ∈ S∞
, f(y) = 0 ,
and thus J−
< 1. This implies J−
= J+
, which leads to a
contradiction. 2
Perhaps surprisingly, the converse of Theorem 2 also holds:
Theorem 3. Suppose f : {0, 1}n
→ {0, 1} cannot be 
expressed as a weighted threshold function. Then there exists
a prior distribution P for which the price of the security F
does not converge to the value of f(x).
Proof: We start from a geometric characterization of 
weighted threshold functions. Consider the Boolean hypercube
{0, 1}n
as a set of points in n
. It is well known that f
is expressible as a weighted threshold function iff there is a
hyperplane in n
that separates all the points at which f
has value 0 from all the points at which f has value 1.
Now, consider the sets
H+
= Conv(f−1
(1))
and
H−
= Conv(f−1
(0)),
where Conv(S) denotes the convex hull of S in n
. H+
and
H−
are convex sets in n
, and so, if they do not intersect, we
can find a separating hyperlane between them. This means
that, if f is not expressible as a weighted threshold function,
H+
and H−
must intersect. In this case, we show how to
construct a prior P for which f(x) is not computed by the
market.
Let x∗
∈ n
be a point in H+
∩ H−
. Because x∗
is in
H+
, there exists some points z1
, z2
, . . . , zm
and constants
λ1, λ2, . . . , λm, such that the following constraints are 
satisfied:
∀k zk
∈ {0, 1}n
, and f(zk
) = 1
∀k 0 < λk ≤ 1
m
k=1
λk = 1
m
k=1
λkzk
= x∗
Similarly, because x∗
∈ H−
, there are points y1
, y2
, . . . , yl
and constants µ1, µ2, . . . , µl, such that
∀j yj
∈ {0, 1}n
, and f(yj
) = 0
∀j 0 < µj ≤ 1
l
j=1
µj = 1
l
j=1
µj yj
= x∗
We now define our prior distribution P as follows:
P(zk
) =
λk
2
for k = 1, 2, . . . , m
P(yj
) =
µj
2
for j = 1, 2, . . . , l,
and all other points are assigned probability 0. It is easy to
see that this is a valid probability distribution. Under this
distribution P, first observe that P(f(x) = 1) = 1
2
. Further,
for any i such that 0 < x∗
i < 1, we have
P(f(x) = 1|xi = 1) =
P(f(x) = 1 ∧ xi = 1)
P(xi = 1)
=
x∗
i
2
x∗
i
=
1
2
and
P(f(x) = 1|xi = 0) =
P(f(x) = 1 ∧ xi = 0)
P(xi = 0)
=
(1−x∗
i )
2
(1 − x∗
i )
=
1
2
For indices i such that x∗
i is 0 or 1 exactly, i"s private 
information reveals no additional information under prior P,
and so here too we have P(f(x) = 1|xi = 0) = P(f(x) =
1|xi = 1) = 1
2
.
Hence, regardless of her private bit xi, each agent i will
bid 0.5 for security F in the first round. The clearing price
of 0.5 also reveals no additional information, and so this is
an equilibrium with price p∞
= 0.5 that does not reveal the
value of f(x). 2
The XOR function is one example of a function that 
cannot be expressed as weighted threshold function; Example 1
illustrates Theorem 3 for this function.
4.3 Convergence time bounds
We have shown that the class of Boolean functions 
computable in our model is the class of weighted threshold 
functions. The next natural question to ask is: How many
rounds of trading are necessary before the equilibrium is
reached? We analyze this problem using the same 
simplified Shapley-Shubik model of market clearing in each round.
We first prove that, in the worst case, at most n rounds are
required.
The idea of the proof is to consider the sequence of 
common knowledge sets Ω = S0
, S1
, . . ., and show that, until
the market reaches equilibrium, each set has a strictly lower
dimension than the previous set.
Definition 4. For a set S ⊆ {0, 1}n
, the dimension of
set S is the dimension of the smallest linear subspace of n
that contains all the points in S; we use the notation dim(S)
to denote it.
Lemma 1. If Sr
= Sr−1
, then dim(Sr
) < dim(Sr−1
).
Proof: Let k = dim(Sr−1
). Consider the bids in round r.
In our model, agent i will bid her current expectation for
the value of F,
br
i = E(f(y) = 1|y ∈ Sr−1
, yi = xi).
Thus, depending on the value of xi, br
i will take on one of
two values h
(0)
i or h
(1)
i . Note that h
(0)
i and h
(1)
i depend only
on the set Sr−1
, which is common knowledge before round
161
r. Setting di = h
(1)
i − h
(0)
i , we can write br
i = h
(0)
i + dixi. It
follows that the clearing price in round r is given by
pr
=
1
n
n
i=1
(h
(0)
i + dixi) (4)
All the agents already know all the h
(0)
i and di values, and
they observe the price pr
at the end of the rth round. Thus,
they effectively have a linear equation in x1, x2, . . . , xn that
they use to improve their knowledge by ruling out any 
possibility that would not have resulted in price pr
. In other
words, after r rounds, the common knowledge set Sr
is the
intersection of Sr−1
with the hyperplane defined by 
Equation (4).
It follows that Sr
is contained in the intersection of this
hyperplane with the k-dimension linear space containing
Sr−1
. If Sr
is not equal to Sr−1
, this intersection defines a
linear subspace of dimension (k − 1) that contains Sr
, and
hence Sr
has dimension at most (k − 1). 2
Theorem 4. Let f be a weighted threshold function, and
let P be an arbitrary prior probability distribution. Then,
after at most n rounds of trading, the price reaches its 
equilibrium value p∞
= f(x).
Proof: Consider the sequence of common knowledge sets
S0
, S1
, . . ., and let r be the minimum index such that Sr
=
Sr−1
. Then, the rth round of trading does not improve any
agent"s knowledge, and thus we must have S∞
= Sr−1
and
p∞
= pr−1
. Observing that dim(S0
) = n, and applying
Lemma 1 to the first r − 1 rounds, we must have (r − 1) ≤
n. Thus, the price reaches its equilibrium value within n
rounds. 2
Theorem 4 provides an upper bound of O(n) on the 
number of rounds required for convergence. We now show that
this bound is tight to within a factor of 2 by constructing a
threshold function with 2n inputs and a prior distribution
for which it takes n rounds to determine the value of f(x)
in the worst case.
The functions we use are the carry-bit functions. The
function Cn takes 2n inputs; for convenience, we write the
inputs as x1, x2 . . . , xn, y1, y2, . . . , yn or as a pair (x, y). The
function value is the value of the high-order carry bit when
the binary numbers xnxn−1 · · · x1 and ynyn−1 · · · y1 are 
added together. In weighted threshold form, this can be written
as
Cn(x, y) = 1 iff
n
i=1
xi + yi
2n+1−i
≥ 1.
For this proof, let us call the agents A1, A2, . . . , An, B1, B2,
. . . , Bn, where Ai holds input bit xi, and Bi holds input bit
yi.
We first illustrate our technique by proving that 
computing C2 requires 2 rounds in the worst case. To do this, we
construct a common prior P2 as follows:
• The pair (x1, y1) takes on the values (0, 0), (0, 1), (1, 0),
(1, 1) uniformly (i.e., with probability 1
4
each).
• We extend this to a distribution on (x1, x2, y1, y2) by
specifying the conditional distribution of (x2, y2) given
(x1, y1): If (x1, y1) = (1, 1), then (x2, y2) takes the
values (0, 0), (0, 1), (1, 0), (1, 1) with probabilities 1
2
, 1
6
,
1
6
, 1
6
respectively. Otherwise, (x2, y2) takes the values
(0, 0), (0, 1), (1, 0), (1, 1) with probabilities 1
6
, 1
6
, 1
6
, 1
2
respectively.
Now, suppose x1 turns out to be 1, and consider agent
A1"s bid in the first round. It is given by
b1
A1
= P(C2(x1, x2, y1, y2) = 1|x1 = 1))
= P(y1 = 1|x1 = 1)
· P((x2, y2) = (0, 0)|x1 = 1, y1 = 1)
+P(y1 = 0|x1 = 1)
· P((x2, y2) = (1, 1)|x1 = 1, y1 = 0)
=
1
2
·
1
2
+
1
2
·
1
2
=
1
2
On the other hand, if x1 turns out to be 0, agent A1"s bid
would be given by
b1
A1
= P(C2(x1, x2, y1, y2) = 1|x1 = 0))
= P((x2, y2) = (1, 1)|x1 = 0)
=
1
2
Thus, irrespective of her bit, A1 will bid 0.5 in the first
round. Note that the function and distribution are 
symmetric between x and y, and so the same argument shows that
B1 will also bid 0.5 in the first round. Thus, the price p1
announced at the end of the first round reveals no 
information about x1 or y1. The reason this occurs is that, under
this distribution, the second carry bit C2 is statistically 
independent of the first carry bit (x1 ∧ y1); we will use this
trick again in the general construction.
Now, suppose that (x2, y2) is either (0, 1) or (1, 0). Then,
even if x2 and y2 are completely revealed by the first-round
price, the value of C2(x1, x2, y1, y2) is not revealed: It will
be 1 if x1 = y1 = 1 and 0 otherwise. Thus, we have shown
that at least 2 rounds of trading will be required to reveal
the function value in this case.
We now extend this construction to show by induction
that the function Cn takes n rounds to reach an equilibrium
in the worst case.
Theorem 5. There is a function Cn with 2n inputs and a
prior distribution Pn such that, in the worst case, the market
takes n rounds to reveal the value of Cn(·).
Proof: We prove the theorem by induction on n. The base
case for n = 2 has already been shown to be true. 
Starting from the distribution P2 described above, we construct
the distributions P3, P4, . . . , Pn by inductively applying the
following rule:
• Let x−n
denote the vector (x1, x2, . . . , xn−1), and 
define y−n
similarly. We extend the distribution Pn−1
on (x−n
, y−n
) to a distribution Pn on (x, y) by 
specifying the conditional distribution of (xn, yn) given
(x−n
, y−n
): If Cn−1(x−n
, y−n
) = 1, then (xn, yn) takes
the values (0, 0), (0, 1), (1, 0), (1, 1) with 
probabilities 1
2
, 1
6
, 1
6
, 1
6
respectively. Otherwise, (xn, yn) takes
the values (0, 0), (0, 1), (1, 0), (1, 1) with probabilities
1
6
, 1
6
, 1
6
, 1
2
respectively.
Claim: Under distribution Pn, for all i < n,
P(Cn(x, y) = 1|xi = 1) = P(Cn(x, y) = 1|xi = 0).
162
Proof of claim: A similar calculation to that used for C2
above shows that the value of Cn(x, y) under this 
distribution is statistically independent of Cn−1(x−n
, y−n
). For
i < n, xi can affect the value of Cn only through Cn−1. Also,
by contruction of Pn, given the value of Cn−1, the 
distribution of Cn is independent of xi. It follows that Cn(x, y) is
statistically independent of xi as well. Of course, a similar
result holds for yi by symmetry.
Thus, in the first round, for all i = 1, 2, . . . , n − 1, the
bids of agents Ai and Bi do not reveal anything about their
private information. Thus, the first-round price does not
reveal any information about the value of (x−n
, y−n
).
On the other hand, agents An and Bn do have different
expectations of Cn(x) depending on whether their input bit
is a 0 or a 1; thus, the first-round price does reveal whether
neither, one, or both of xn and yn are 1. Now, consider
a situation in which (xn, yn) takes on the value (1, 0) or
(0, 1). We show that, in this case, after one round we are
left with the residual problem of computing the value of
Cn−1(x−n
, y−n
) under the prior Pn−1.
Clearly, when xn + yn = 1, Cn(x, y) = Cn−1(x−n
, y−n
).
Further, according to the construction of Pn, the event (xn+
yn = 1) has the same probability (1/3) for all values of
(x−n
, y−n
). Thus, conditioning on this fact does not alter
the probability distribution over (x−n
, y−n
); it must still be
Pn−1.
Finally, the inductive assumption tells us that solving this
residual problem will take at least n − 1 more rounds in the
worst case and hence that finding the value of Cn(x, y) takes
at least n rounds in the worst case. 2
