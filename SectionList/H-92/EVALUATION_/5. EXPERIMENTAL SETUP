The ultimate goal of incorporating implicit feedback into ranking
is to improve the relevance of the returned web search results.
Hence, we compare the ranking methods over a large set of judged
queries with explicit relevance labels provided by human judges.
In order for the evaluation to be realistic we obtained a random
sample of queries from web search logs of a major search engine,
with associated results and traces for user actions. We describe
this dataset in detail next. Our metrics are described in Section 5.2
that we use to evaluate the ranking alternatives, listed in Section
5.3 in the experiments of Section 6.
5.1 Datasets
We compared our ranking methods over a random sample of 3,000
queries from the search engine query logs. The queries were
drawn from the logs uniformly at random by token without
replacement, resulting in a query sample representative of the
overall query distribution. On average, 30 results were explicitly
labeled by human judges using a six point scale ranging from
Perfect down to Bad. Overall, there were over 83,000 results
with explicit relevance judgments. In order to compute various
statistics, documents with label Good or better will be
considered relevant, and with lower labels to be non-relevant.
Note that the experiments were performed over the results already
highly ranked by a web search engine, which corresponds to a
typical user experience which is limited to the small number of the
highly ranked results for a typical web search query.
The user interactions were collected over a period of 8 weeks
using voluntary opt-in information. In total, over 1.2 million
unique queries were instrumented, resulting in over 12 million
individual interactions with the search engine. The data consisted
of user interactions with the web search engine (e.g., clicking on a
result link, going back to search results, etc.) performed after a
query was submitted. These actions were aggregated across users
and search sessions and converted to features in Table 4.1.
To create the training, validation, and test query sets, we created
three different random splits of 1,500 training, 500 validation, and
1000 test queries. The splits were done randomly by query, so that
there was no overlap in training, validation, and test queries.
5.2 Evaluation Metrics
We evaluate the ranking algorithms over a range of accepted
information retrieval metrics, namely Precision at K (P(K)),
Normalized Discounted Cumulative Gain (NDCG), and Mean
Average Precision (MAP). Each metric focuses on a deferent
aspect of system performance, as we describe below.
• Precision at K: As the most intuitive metric, P(K) reports the
fraction of documents ranked in the top K results that are
labeled as relevant. In our setting, we require a relevant
document to be labeled Good or higher. The position of
relevant documents within the top K is irrelevant, and hence
this metric measure overall user satisfaction with the top K
results.
• NDCG at K: NDCG is a retrieval measure devised specifically
for web search evaluation [10]. For a given query q, the ranked
results are examined from the top ranked down, and the NDCG
computed as:
 
=
+−=
K
j
jr
qq jMN
1
)(
)1log(/)12(
Where Mq is a normalization constant calculated so that a
perfect ordering would obtain NDCG of 1; and each r(j) is an
integer relevance label (0=Bad and 5=Perfect) of result
returned at position j. Note that unlabeled and Bad documents
do not contribute to the sum, but will reduce NDCG for the
query pushing down the relevant labeled documents, reducing
their contributions. NDCG is well suited to web search
evaluation, as it rewards relevant documents in the top ranked
results more heavily than those ranked lower.
• MAP: Average precision for each query is defined as the mean
of the precision at K values computed after each relevant
document was retrieved. The final MAP value is defined as the
mean of average precisions of all queries in the test set. This
metric is the most commonly used single-value summary of a
run over a set of queries.
5.3 Ranking Methods Compared
Recall that our goal is to quantify the effectiveness of implicit
behavior for real web search. One dimension is to compare the
utility of implicit feedback with other information available to a
web search engine. Specifically, we compare effectiveness of
implicit user behaviors with content-based matching, static page
quality features, and combinations of all features.
• BM25F: As a strong web search baseline we used the BM25F
scoring, which was used in one of the best performing systems
in the TREC 2004 Web track [23,27]. BM25F and its variants
have been extensively described and evaluated in IR literature,
and hence serve as a strong, reproducible baseline. The BM25F
variant we used for our experiments computes separate match
scores for each field for a result document (e.g., body text,
title, and anchor text), and incorporates query-independent 
linkbased information (e.g., PageRank, ClickDistance, and URL
depth). The scoring function and field-specific tuning is
described in detail in [23]. Note that BM25F does not directly
consider explicit or implicit feedback for tuning.
• RN: The ranking produced by a neural net ranker (RankNet,
described in Section 3.3) that learns to rank web search results
by incorporating BM25F and a large number of additional static
and dynamic features describing each search result. This system
automatically learns weights for all features (including the
BM25F score for a document) based on explicit human labels
for a large set of queries. A system incorporating an
implementation of RankNet is currently in use by a major
search engine and can be considered representative of the state
of the art in web search.
• BM25F-RerankCT: The ranking produced by incorporating
clickthrough statistics to reorder web search results ranked by
BM25F above. Clickthrough is a particularly important special
case of implicit feedback, and has been shown to correlate with
result relevance. This is a special case of the ranking method in
Section 3.1, with the weight wI set to 1000 and the ranking Id
is simply the number of clicks on the result corresponding to d.
In effect, this ranking brings to the top all returned web search
results with at least one click (and orders them in decreasing
order by number of clicks). The relative ranking of the
remainder of results is unchanged and they are inserted below
all clicked results. This method serves as our baseline implicit
feedback reranking method.
BM25F-RerankAll The ranking produced by reordering the
BM25F results using all user behavior features (Section 4).
This method learns a model of user preferences by correlating
feature values with explicit relevance labels using the RankNet
neural net algorithm (Section 4.2). At runtime, for a given
query the implicit score Ir is computed for each result r with
available user interaction features, and the implicit ranking is
produced. The merged ranking is computed as described in
Section 3.1. Based on the experiments over the development set
we fix the value of wI to 3 (the effect of the wI parameter for
this ranker turned out to be negligible).
• BM25F+All: Ranking derived by training the RankNet
(Section 3.3) learner over the features set of the BM25F score
as well as all implicit feedback features (Section 3.2). We used
the 2-layer implementation of RankNet [5] trained on the
queries and labels in the training and validation sets.
• RN+All: Ranking derived by training the 2-layer RankNet
ranking algorithm (Section 3.3) over the union of all content,
dynamic, and implicit feedback features (i.e., all of the features
described above as well as all of the new implicit feedback
features we introduced).
The ranking methods above span the range of the information used
for ranking, from not using the implicit or explicit feedback at all
(i.e., BM25F) to a modern web search engine using hundreds of
features and tuned on explicit judgments (RN). As we will show
next, incorporating user behavior into these ranking systems
dramatically improves the relevance of the returned documents.
