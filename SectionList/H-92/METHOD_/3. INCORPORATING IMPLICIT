FEEDBACK
We consider two complementary approaches to ranking with
implicit feedback: (1) treating implicit feedback as independent
evidence for ranking results, and (2) integrating implicit feedback
features directly into the ranking algorithm. We describe the two
general ranking approaches next. The specific implicit feedback
features are described in Section 4, and the algorithms for
interpreting and incorporating implicit feedback are described in
Section 5.
3.1 Implicit Feedback as Independent
Evidence
The general approach is to re-rank the results obtained by a web
search engine according to observed clickthrough and other user
interactions for the query in previous search sessions. Each result
is assigned a score according to expected relevance/user
satisfaction based on previous interactions, resulting in some
preference ordering based on user interactions alone.
While there has been significant work on merging multiple
rankings, we adapt a simple and robust approach of ignoring the
original rankers" scores, and instead simply merge the rank orders.
The main reason for ignoring the original scores is that since the
feature spaces and learning algorithms are different, the scores are
not directly comparable, and re-normalization tends to remove the
benefit of incorporating classifier scores.
We experimented with a variety of merging functions on the
development set of queries (and using a set of interactions from a
different time period from final evaluation sets). We found that a
simple rank merging heuristic combination works well, and is
robust to variations in score values from original rankers. For a
given query q, the implicit score ISd is computed for each result d
from available user interaction features, resulting in the implicit
rank Id for each result. We compute a merged score SM(d) for d by
combining the ranks obtained from implicit feedback, Id with the
original rank of d, Od:
 
 
¡
 
 
¢
£
+
+
+
+
=
otherwise
O
dforexistsfeedbackimplicitif
OI
w
wOIdS
d
dd
I
IddM
1
1
1
1
1
1
),,,(
where the weight wI is a heuristically tuned scaling factor
representing the relative importance of the implicit feedback.
The query results are ordered in by decreasing values of SM to
produce the final ranking. One special case of this model arises
when setting wI to a very large value, effectively forcing clicked
results to be ranked higher than un-clicked results - an intuitive
and effective heuristic that we will use as a baseline. Applying
more sophisticated classifier and ranker combination algorithms
may result in additional improvements, and is a promising
direction for future work.
The approach above assumes that there are no interactions
between the underlying features producing the original web search
ranking and the implicit feedback features. We now relax this
assumption by integrating implicit feedback features directly into
the ranking process.
3.2 Ranking with Implicit Feedback Features
Modern web search engines rank results based on a large number
of features, including content-based features (i.e., how closely a
query matches the text or title or anchor text of the document), and
query-independent page quality features (e.g., PageRank of the
document or the domain). In most cases, automatic (or 
semiautomatic) methods are developed for tuning the specific ranking
function that combines these feature values.
Hence, a natural approach is to incorporate implicit feedback
features directly as features for the ranking algorithm. During
training or tuning, the ranker can be tuned as before but with
additional features. At runtime, the search engine would fetch the
implicit feedback features associated with each query-result URL
pair. This model requires a ranking algorithm to be robust to
missing values: more than 50% of queries to web search engines
are unique, with no previous implicit feedback available. We now
describe such a ranker that we used to learn over the combined
feature sets including implicit feedback.
3.3 Learning to Rank Web Search Results
A key aspect of our approach is exploiting recent advances in
machine learning, namely trainable ranking algorithms for web
search and information retrieval (e.g., [5, 11] and classical results
reviewed in [3]). In our setting, explicit human relevance
judgments (labels) are available for a set of web search queries
and results. Hence, an attractive choice to use is a supervised
machine learning technique to learn a ranking function that best
predicts relevance judgments.
RankNet is one such algorithm. It is a neural net tuning algorithm
that optimizes feature weights to best match explicitly provided
pairwise user preferences. While the specific training algorithms
used by RankNet are beyond the scope of this paper, it is
described in detail in [5] and includes extensive evaluation and
comparison with other ranking methods. An attractive feature of
RankNet is both train- and run-time efficiency - runtime ranking
can be quickly computed and can scale to the web, and training
can be done over thousands of queries and associated judged
results.
We use a 2-layer implementation of RankNet in order to model
non-linear relationships between features. Furthermore, RankNet
can learn with many (differentiable) cost functions, and hence can
automatically learn a ranking function from human-provided
labels, an attractive alternative to heuristic feature combination
techniques. Hence, we will also use RankNet as a generic ranker
to explore the contribution of implicit feedback for different
ranking alternatives.
