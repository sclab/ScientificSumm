The preliminary tests presented in this section aim to analyse
the performance and scalability of the solution and evaluate the
impact on application execution in terms of metrics collection
overhead. All tests were executed using two Pentium 4 3.0 GHz
PCs with 1,024 MB of RAM, running Java 1.4.2_08. The two
machines were connected to a router with a third computer acting
as a file server and hosting the external adaptation engine
implemented within the MobJeX system controller, thereby
simulating a global adaptation scenario.
Since only a limited number of tests could be executed, this
evaluation chose to measure the worst case scenario in which all
metrics collection was initiated in mobjects, wherein the
propagation cost is higher than for any other metrics collected in
the system. In addition, since exhaustive testing of criteria is
beyond the scope of this paper, two different types of criteria were
used in the tests. The measure metrics criterion was chosen, since
this represents the starting point of the measurement process and
can control under what circumstances and how frequently metrics
are measured. In addition, the push criterion was also
implemented on the service, in order to provide an evaluation of
controlling the frequency of metrics delivery to the adaptation
engine. All other (update and push) criteria were set to always
meaning that they always evaluated to true and thus a notification
was posted.
Figure 2 shows the metric collection overhead in the mobject
(MMCO), for different numbers of mobjects and methods when
all criteria are set to always to provide the maximum measurement
and propagation of metrics and thus an absolute worst case
performance scenario. It can be seen that the independent factors
of increasing the number of mobjects and methods independently
are linear. Although combining these together provides an
exponential growth that is approximately n-squared, the initial
results are not discouraging since delivering all of the metrics
associated with 20 mobjects, each having 20 methods (which
constitutes quite a large application given that mobjects typically
represent coarse grained object clusters) is approximately 400ms,
which could reasonably be expected to be offset with adaptation
gains. Note that in contrast, the proxy metrics collection overhead
(PMCO) was relatively small and constant at < 5ms, since in the
absence of a proxy push criterion (this was only implemented on
the service) the response time (RT) data for a single method is
pushed during every invocation.
50
150
250
350
450
550
1 5 10 15 20 25
Number of Mobjects/Methods
MobjectMetricsCollectionOverheadMMCO(ms)
Methods
Mobjects
Both
Figure 2. Worst case performance characteristics
The next step was to determine the percentage metrics
collection overhead compared with execution time in order to
provide information about the execution characteristics of objects
that would be suitable for adaptation using this metric collection
approach. Clearly, it is not practical to measure metrics and
perform adaptation on objects with short execution times that
cannot benefit from remote execution on hosts with greater
processing power, thereby offsetting IT overhead of remote
compared with local execution as well as the cost of object
migration and the metrics collection process itself.
In addition, to demonstrate the effect of using simple
frequency based criteria, the MMCO results as a percentage of
method execution time were plotted as a 3-dimensional graph in
Figure 3 with the z-axis representing the frequency used in both
the measure metrics criterion and the service to adaptation engine
push criterion. This means that for a frequency value of 5 (n=5),
metrics are only measured on every fifth method call, which then
results in a notification through the model entity hierarchy to the
service, on this same fifth invocation. Furthermore, the value of
n=5 was also applied to the service push criterion so that metrics
were only pushed to the adaptation engine after five such
notifications, that is for example five different mobjects had
updated their metrics.
These results are encouraging since even for the worst case
scenario of n=1 the metric collection overhead is an acceptable
20% for a method of 1500ms duration (which is relatively short
for a component or service level object in a distributed enterprise
class application) with previous work on adaptation showing that
such an overhead could easily be recovered by the efficiency gains
made by adaptation [5]. Furthermore, the measurement time
includes delivering the results synchronously via a remote call to
the adaptation engine on a different host, which would normally
be done asynchronously, thus further reducing the impact on
method execution performance. The graph also demonstrates that
even using modest criteria to reduce the metrics measurement to
more realistic levels, has a rapid improvement on collection
overhead at 20% for 500ms of ET.
0
1000
2000
3000
4000
5000
1
2
3
4
5
6
0
20
40
60
80
100
120
MMCO (%)
ET (milliseconds) N (interval)
MMCO (%)
Figure 3. Performance characteristics with simple criteria
