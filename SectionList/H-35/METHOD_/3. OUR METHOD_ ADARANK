3.1 General Framework
We first describe the general framework of learning to rank for
document retrieval. In retrieval (testing), given a query the system
returns a ranking list of documents in descending order of the 
relevance scores. The relevance scores are calculated with a ranking
function (model). In learning (training), a number of queries and
their corresponding retrieved documents are given. Furthermore,
the relevance levels of the documents with respect to the queries are
also provided. The relevance levels are represented as ranks (i.e.,
categories in a total order). The objective of learning is to construct
a ranking function which achieves the best results in ranking of the
training data in the sense of minimization of a loss function. Ideally
the loss function is defined on the basis of the performance measure
used in testing.
Suppose that Y = {r1, r2, · · · , r } is a set of ranks, where denotes
the number of ranks. There exists a total order between the ranks
r r −1 · · · r1, where ‘ " denotes a preference relationship.
In training, a set of queries Q = {q1, q2, · · · , qm} is given. Each
query qi is associated with a list of retrieved documents di = {di1, di2,
· · · , di,n(qi)} and a list of labels yi = {yi1, yi2, · · · , yi,n(qi)}, where n(qi)
denotes the sizes of lists di and yi, dij denotes the jth
document in
di, and yij ∈ Y denotes the rank of document di j. A feature 
vector xij = Ψ(qi, di j) ∈ X is created from each query-document pair
(qi, di j), i = 1, 2, · · · , m; j = 1, 2, · · · , n(qi). Thus, the training set
can be represented as S = {(qi, di, yi)}m
i=1.
The objective of learning is to create a ranking function f : X →
, such that for each query the elements in its corresponding 
document list can be assigned relevance scores using the function and
then be ranked according to the scores. Specifically, we create a
permutation of integers π(qi, di, f) for query qi, the 
corresponding list of documents di, and the ranking function f. Let di =
{di1, di2, · · · , di,n(qi)} be identified by the list of integers {1, 2, · · · , n(qi)},
then permutation π(qi, di, f) is defined as a bijection from {1, 2, · · · ,
n(qi)} to itself. We use π( j) to denote the position of item j (i.e.,
di j). The learning process turns out to be that of minimizing the
loss function which represents the disagreement between the 
permutation π(qi, di, f) and the list of ranks yi, for all of the queries.
Table 1: Notations and explanations.
Notations Explanations
qi ∈ Q ith
query
di = {di1, di2, · · · , di,n(qi)} List of documents for qi
yi j ∈ {r1, r2, · · · , r } Rank of di j w.r.t. qi
yi = {yi1, yi2, · · · , yi,n(qi)} List of ranks for qi
S = {(qi, di, yi)}m
i=1 Training set
xij = Ψ(qi, dij) ∈ X Feature vector for (qi, di j)
f(xij) ∈ Ranking model
π(qi, di, f) Permutation for qi, di, and f
ht(xi j) ∈ tth
weak ranker
E(π(qi, di, f), yi) ∈ [−1, +1] Performance measure function
In the paper, we define the rank model as a linear combination of
weak rankers: f(x) = T
t=1 αtht(x), where ht(x) is a weak ranker, αt
is its weight, and T is the number of weak rankers.
In information retrieval, query-based performance measures are
used to evaluate the ‘goodness" of a ranking function. By query
based measure, we mean a measure defined over a ranking list
of documents with respect to a query. These measures include
MAP, NDCG, MRR (Mean Reciprocal Rank), WTA (Winners Take
ALL), and Precision@n [1, 15]. We utilize a general function
E(π(qi, di, f), yi) ∈ [−1, +1] to represent the performance 
measures. The first argument of E is the permutation π created using
the ranking function f on di. The second argument is the list of
ranks yi given by humans. E measures the agreement between π
and yi. Table 1 gives a summary of notations described above.
Next, as examples of performance measures, we present the 
definitions of MAP and NDCG. Given a query qi, the corresponding
list of ranks yi, and a permutation πi on di, average precision for qi
is defined as:
AvgPi =
n(qi)
j=1 Pi( j) · yij
n(qi)
j=1 yij
, (1)
where yij takes on 1 and 0 as values, representing being relevant or
irrelevant and Pi( j) is defined as precision at the position of dij:
Pi( j) =
k:πi(k)≤πi(j) yik
πi(j)
, (2)
where πi( j) denotes the position of di j.
Given a query qi, the list of ranks yi, and a permutation πi on di,
NDCG at position m for qi is defined as:
Ni = ni ·
j:πi(j)≤m
2yi j − 1
log(1 + πi( j))
, (3)
where yij takes on ranks as values and ni is a normalization 
constant. ni is chosen so that a perfect ranking π∗
i "s NDCG score at
position m is 1.
3.2 Algorithm
Inspired by the AdaBoost algorithm for classification, we have
devised a novel algorithm which can optimize a loss function based
on the IR performance measures. The algorithm is referred to as
‘AdaRank" and is shown in Figure 1.
AdaRank takes a training set S = {(qi, di, yi)}m
i=1 as input and
takes the performance measure function E and the number of 
iterations T as parameters. AdaRank runs T rounds and at each round it
creates a weak ranker ht(t = 1, · · · , T). Finally, it outputs a ranking
model f by linearly combining the weak rankers.
At each round, AdaRank maintains a distribution of weights over
the queries in the training data. We denote the distribution of weights
Input: S = {(qi, di, yi)}m
i=1, and parameters E and T
Initialize P1(i) = 1/m.
For t = 1, · · · , T
• Create weak ranker ht with weighted distribution Pt on 
training data S .
• Choose αt
αt =
1
2
· ln
m
i=1 Pt(i){1 + E(π(qi, di, ht), yi)}
m
i=1 Pt(i){1 − E(π(qi, di, ht), yi)}
.
• Create ft
ft(x) =
t
k=1
αkhk(x).
• Update Pt+1
Pt+1(i) =
exp{−E(π(qi, di, ft), yi)}
m
j=1 exp{−E(π(qj, dj, ft), yj)}
.
End For
Output ranking model: f(x) = fT (x).
Figure 1: The AdaRank algorithm.
at round t as Pt and the weight on the ith
training query qi at round
t as Pt(i). Initially, AdaRank sets equal weights to the queries. At
each round, it increases the weights of those queries that are not
ranked well by ft, the model created so far. As a result, the learning
at the next round will be focused on the creation of a weak ranker
that can work on the ranking of those ‘hard" queries.
At each round, a weak ranker ht is constructed based on training
data with weight distribution Pt. The goodness of a weak ranker is
measured by the performance measure E weighted by Pt:
m
i=1
Pt(i)E(π(qi, di, ht), yi).
Several methods for weak ranker construction can be considered.
For example, a weak ranker can be created by using a subset of
queries (together with their document list and label list) sampled
according to the distribution Pt. In this paper, we use single features
as weak rankers, as will be explained in Section 3.6.
Once a weak ranker ht is built, AdaRank chooses a weight αt > 0
for the weak ranker. Intuitively, αt measures the importance of ht.
A ranking model ft is created at each round by linearly 
combining the weak rankers constructed so far h1, · · · , ht with weights
α1, · · · , αt. ft is then used for updating the distribution Pt+1.
3.3 Theoretical Analysis
The existing learning algorithms for ranking attempt to minimize
a loss function based on instance pairs (document pairs). In 
contrast, AdaRank tries to optimize a loss function based on queries.
Furthermore, the loss function in AdaRank is defined on the basis
of general IR performance measures. The measures can be MAP,
NDCG, WTA, MRR, or any other measures whose range is within
[−1, +1]. We next explain why this is the case.
Ideally we want to maximize the ranking accuracy in terms of a
performance measure on the training data:
max
f∈F
m
i=1
E(π(qi, di, f), yi), (4)
where F is the set of possible ranking functions. This is equivalent
to minimizing the loss on the training data
min
f∈F
m
i=1
(1 − E(π(qi, di, f), yi)). (5)
It is difficult to directly optimize the loss, because E is a 
noncontinuous function and thus may be difficult to handle. We instead
attempt to minimize an upper bound of the loss in (5)
min
f∈F
m
i=1
exp{−E(π(qi, di, f), yi)}, (6)
because e−x
≥ 1 − x holds for any x ∈ . We consider the use of a
linear combination of weak rankers as our ranking model:
f(x) =
T
t=1
αtht(x). (7)
The minimization in (6) then turns out to be
min
ht∈H,αt∈ +
L(ht, αt) =
m
i=1
exp{−E(π(qi, di, ft−1 + αtht), yi)}, (8)
where H is the set of possible weak rankers, αt is a positive weight,
and ( ft−1 + αtht)(x) = ft−1(x) + αtht(x). Several ways of computing
coefficients αt and weak rankers ht may be considered. Following
the idea of AdaBoost, in AdaRank we take the approach of ‘forward
stage-wise additive modeling" [12] and get the algorithm in Figure
1. It can be proved that there exists a lower bound on the ranking
accuracy for AdaRank on training data, as presented in Theorem 1.
T 1. The following bound holds on the ranking 
accuracy of the AdaRank algorithm on training data:
1
m
m
i=1
E(π(qi, di, fT ), yi) ≥ 1 −
T
t=1
e−δt
min 1 − ϕ(t)2,
where ϕ(t) = m
i=1 Pt(i)E(π(qi, di, ht), yi), δt
min = mini=1,··· ,m δt
i, and
δt
i = E(π(qi, di, ft−1 + αtht), yi) − E(π(qi, di, ft−1), yi)
−αtE(π(qi, di, ht), yi),
for all i = 1, 2, · · · , m and t = 1, 2, · · · , T.
A proof of the theorem can be found in appendix. The theorem
implies that the ranking accuracy in terms of the performance 
measure can be continuously improved, as long as e−δt
min 1 − ϕ(t)2 < 1
holds.
3.4 Advantages
AdaRank is a simple yet powerful method. More importantly, it
is a method that can be justified from the theoretical viewpoint, as
discussed above. In addition AdaRank has several other advantages
when compared with the existing learning to rank methods such as
Ranking SVM, RankBoost, and RankNet.
First, AdaRank can incorporate any performance measure, 
provided that the measure is query based and in the range of [−1, +1].
Notice that the major IR measures meet this requirement. In 
contrast the existing methods only minimize loss functions that are
loosely related to the IR measures [16].
Second, the learning process of AdaRank is more efficient than
those of the existing learning algorithms. The time complexity of
AdaRank is of order O((k+T)·m·n log n), where k denotes the 
number of features, T the number of rounds, m the number of queries
in training data, and n is the maximum number of documents for
queries in training data. The time complexity of RankBoost, for
example, is of order O(T · m · n2
) [8].
Third, AdaRank employs a more reasonable framework for 
performing the ranking task than the existing methods. Specifically in
AdaRank the instances correspond to queries, while in the existing
methods the instances correspond to document pairs. As a result,
AdaRank does not have the following shortcomings that plague the
existing methods. (a) The existing methods have to make a strong
assumption that the document pairs from the same query are 
independently distributed. In reality, this is clearly not the case and this
problem does not exist for AdaRank. (b) Ranking the most relevant
documents on the tops of document lists is crucial for document 
retrieval. The existing methods cannot focus on the training on the
tops, as indicated in [4]. Several methods for rectifying the problem
have been proposed (e.g., [4]), however, they do not seem to 
fundamentally solve the problem. In contrast, AdaRank can naturally
focus on training on the tops of document lists, because the 
performance measures used favor rankings for which relevant documents
are on the tops. (c) In the existing methods, the numbers of 
document pairs vary from query to query, resulting in creating models
biased toward queries with more document pairs, as pointed out in
[4]. AdaRank does not have this drawback, because it treats queries
rather than document pairs as basic units in learning.
3.5 Differences from AdaBoost
AdaRank is a boosting algorithm. In that sense, it is similar to
AdaBoost, but it also has several striking differences from AdaBoost.
First, the types of instances are different. AdaRank makes use of
queries and their corresponding document lists as instances. The 
labels in training data are lists of ranks (relevance levels). AdaBoost
makes use of feature vectors as instances. The labels in training
data are simply +1 and −1.
Second, the performance measures are different. In AdaRank,
the performance measure is a generic measure, defined on the 
document list and the rank list of a query. In AdaBoost the 
corresponding performance measure is a specific measure for binary 
classification, also referred to as ‘margin" [25].
Third, the ways of updating weights are also different. In 
AdaBoost, the distribution of weights on training instances is 
calculated according to the current distribution and the performance of
the current weak learner. In AdaRank, in contrast, it is calculated
according to the performance of the ranking model created so far,
as shown in Figure 1. Note that AdaBoost can also adopt the weight
updating method used in AdaRank. For AdaBoost they are 
equivalent (cf., [12] page 305). However, this is not true for AdaRank.
3.6 Construction of Weak Ranker
We consider an efficient implementation for weak ranker 
construction, which is also used in our experiments. In the 
implementation, as weak ranker we choose the feature that has the optimal
weighted performance among all of the features:
max
k
m
i=1
Pt(i)E(π(qi, di, xk), yi).
Creating weak rankers in this way, the learning process turns out
to be that of repeatedly selecting features and linearly combining
the selected features. Note that features which are not selected in
the training phase will have a weight of zero.
