Traditional crawlers normally follow links on the Web to 
discover and download pages. Therefore they cannot get to the Hidden
Web pages which are only accessible through query interfaces. In
this paper, we studied how we can build a Hidden Web crawler that
can automatically query a Hidden Web site and download pages
from it. We proposed three different query generation policies for
the Hidden Web: a policy that picks queries at random from a list
of keywords, a policy that picks queries based on their frequency
in a generic text collection, and a policy which adaptively picks a
good query based on the content of the pages downloaded from the
Hidden Web site. Experimental evaluation on 4 real Hidden Web
sites shows that our policies have a great potential. In particular, in
certain cases the adaptive policy can download more than 90% of
a Hidden Web site after issuing approximately 100 queries. Given
these results, we believe that our work provides a potential 
mechanism to improve the search-engine coverage of the Web and the
user experience of Web search.
6.1 Future Work
We briefly discuss some future-research avenues.
Multi-attribute Databases We are currently investigating how
to extend our ideas to structured multi-attribute databases. While
generating queries for multi-attribute databases is clearly a more
difficult problem, we may exploit the following observation to 
address this problem: When a site supports multi-attribute queries,
the site often returns pages that contain values for each of the query
attributes. For example, when an online bookstore supports queries
on title, author and isbn, the pages returned from a query
typically contain the title, author and ISBN of corresponding books.
Thus, if we can analyze the returned pages and extract the values
for each field (e.g, title = ‘Harry Potter", author =
‘J.K. Rowling", etc), we can apply the same idea that we
used for the textual database: estimate the frequency of each 
attribute value and pick the most promising one. The main challenge
is to automatically segment the returned pages so that we can 
identify the sections of the pages that present the values corresponding
to each attribute. Since many Web sites follow limited formatting
styles in presenting multiple attributes - for example, most book
titles are preceded by the label Title: - we believe we may learn
page-segmentation rules automatically from a small set of training
examples.
Other Practical Issues In addition to the automatic query 
generation problem, there are many practical issues to be addressed
to build a fully automatic Hidden-Web crawler. For example, in
this paper we assumed that the crawler already knows all query 
interfaces for Hidden-Web sites. But how can the crawler discover
the query interfaces? The method proposed in [15] may be a good
starting point. In addition, some Hidden-Web sites return their 
results in batches of, say, 20 pages, so the user has to click on a
next button in order to see more results. In this case, a fully 
automatic Hidden-Web crawler should know that the first result index
page contains only a partial result and press the next button 
automatically. Finally, some Hidden Web sites may contain an infinite
number of Hidden Web pages which do not contribute much 
significant content (e.g. a calendar with links for every day). In this
case the Hidden-Web crawler should be able to detect that the site
does not have much more new content and stop downloading pages
from the site. Page similarity detection algorithms may be useful
for this purpose [9, 13].
