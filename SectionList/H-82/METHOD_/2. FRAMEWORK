In this section, we present a formal framework for the study of
the Hidden-Web crawling problem. In Section 2.1, we describe our
assumptions on Hidden-Web sites and explain how users interact
with the sites. Based on this interaction model, we present a 
highlevel algorithm for a Hidden-Web crawler in Section 2.2. Finally in
Section 2.3, we formalize the Hidden-Web crawling problem.
2.1 Hidden-Web database model
There exists a variety of Hidden Web sources that provide 
information on a multitude of topics. Depending on the type of 
information, we may categorize a Hidden-Web site either as a textual
database or a structured database. A textual database is a site that
Figure 2: A multi-attribute search interface
mainly contains plain-text documents, such as PubMed and 
LexisNexis (an online database of legal documents [1]). Since 
plaintext documents do not usually have well-defined structure, most
textual databases provide a simple search interface where users
type a list of keywords in a single search box (Figure 1). In 
contrast, a structured database often contains multi-attribute relational
data (e.g., a book on the Amazon Web site may have the fields
title=‘Harry Potter", author=‘J.K. Rowling" and
isbn=‘0590353403") and supports multi-attribute search 
interfaces (Figure 2). In this paper, we will mainly focus on 
textual databases that support single-attribute keyword queries. We
discuss how we can extend our ideas for the textual databases to
multi-attribute structured databases in Section 6.1.
Typically, the users need to take the following steps in order to
access pages in a Hidden-Web database:
1. Step 1. First, the user issues a query, say liver, through the
search interface provided by the Web site (such as the one shown
in Figure 1).
2. Step 2. Shortly after the user issues the query, she is presented
with a result index page. That is, the Web site returns a list of
links to potentially relevant Web pages, as shown in Figure 3(a).
3. Step 3. From the list in the result index page, the user identifies
the pages that look interesting and follows the links. Clicking
on a link leads the user to the actual Web page, such as the one
shown in Figure 3(b), that the user wants to look at.
2.2 A generic Hidden Web crawling algorithm
Given that the only entry to the pages in a Hidden-Web site
is its search from, a Hidden-Web crawler should follow the three
steps described in the previous section. That is, the crawler has
to generate a query, issue it to the Web site, download the result
index page, and follow the links to download the actual pages. In
most cases, a crawler has limited time and network resources, so
the crawler repeats these steps until it uses up its resources.
In Figure 4 we show the generic algorithm for a Hidden-Web
crawler. For simplicity, we assume that the Hidden-Web crawler
issues single-term queries only.3
The crawler first decides which
query term it is going to use (Step (2)), issues the query, and 
retrieves the result index page (Step (3)). Finally, based on the links
found on the result index page, it downloads the Hidden Web pages
from the site (Step (4)). This same process is repeated until all the
available resources are used up (Step (1)).
Given this algorithm, we can see that the most critical decision
that a crawler has to make is what query to issue next. If the
crawler can issue successful queries that will return many matching
pages, the crawler can finish its crawling early on using minimum
resources. In contrast, if the crawler issues completely irrelevant
queries that do not return any matching pages, it may waste all
of its resources simply issuing queries without ever retrieving 
actual pages. Therefore, how the crawler selects the next query can
greatly affect its effectiveness. In the next section, we formalize
this query selection problem.
3
For most Web sites that assume AND for multi-keyword
queries, single-term queries return the maximum number of results.
Extending our work to multi-keyword queries is straightforward.
101
(a) List of matching pages for query liver. (b) The first matching page for liver.
Figure 3: Pages from the PubMed Web site.
ALGORITHM 2.1. Crawling a Hidden Web site
Procedure
(1) while ( there are available resources ) do
// select a term to send to the site
(2) qi = SelectTerm()
// send query and acquire result index page
(3) R(qi) = QueryWebSite( qi )
// download the pages of interest
(4) Download( R(qi) )
(5) done
Figure 4: Algorithm for crawling a Hidden Web site.
S
q1
q
qq
2
34
Figure 5: A set-formalization of the optimal query selection
problem.
2.3 Problem formalization
Theoretically, the problem of query selection can be formalized
as follows: We assume that the crawler downloads pages from a
Web site that has a set of pages S (the rectangle in Figure 5). We
represent each Web page in S as a point (dots in Figure 5). Every
potential query qi that we may issue can be viewed as a subset of S,
containing all the points (pages) that are returned when we issue qi
to the site. Each subset is associated with a weight that represents
the cost of issuing the query. Under this formalization, our goal is to
find which subsets (queries) cover the maximum number of points
(Web pages) with the minimum total weight (cost). This problem
is equivalent to the set-covering problem in graph theory [16].
There are two main difficulties that we need to address in this
formalization. First, in a practical situation, the crawler does not
know which Web pages will be returned by which queries, so the
subsets of S are not known in advance. Without knowing these
subsets the crawler cannot decide which queries to pick to 
maximize the coverage. Second, the set-covering problem is known to
be NP-Hard [16], so an efficient algorithm to solve this problem
optimally in polynomial time has yet to be found.
In this paper, we will present an approximation algorithm that
can find a near-optimal solution at a reasonable computational cost.
Our algorithm leverages the observation that although we do not
know which pages will be returned by each query qi that we issue,
we can predict how many pages will be returned. Based on this 
information our query selection algorithm can then select the best
queries that cover the content of the Web site. We present our 
prediction method and our query selection algorithm in Section 3.
2.3.1 Performance Metric
Before we present our ideas for the query selection problem, we
briefly discuss some of our notation and the cost/performance 
metrics.
Given a query qi, we use P(qi) to denote the fraction of pages
that we will get back if we issue query qi to the site. For example, if
a Web site has 10,000 pages in total, and if 3,000 pages are returned
for the query qi = medicine, then P(qi) = 0.3. We use P(q1 ∧
q2) to represent the fraction of pages that are returned from both
q1 and q2 (i.e., the intersection of P(q1) and P(q2)). Similarly, we
use P(q1 ∨ q2) to represent the fraction of pages that are returned
from either q1 or q2 (i.e., the union of P(q1) and P(q2)).
We also use Cost(qi) to represent the cost of issuing the query
qi. Depending on the scenario, the cost can be measured either in
time, network bandwidth, the number of interactions with the site,
or it can be a function of all of these. As we will see later, our
proposed algorithms are independent of the exact cost function.
In the most common case, the query cost consists of a number
of factors, including the cost for submitting the query to the site,
retrieving the result index page (Figure 3(a)) and downloading the
actual pages (Figure 3(b)). We assume that submitting a query 
incurs a fixed cost of cq. The cost for downloading the result index
page is proportional to the number of matching documents to the
query, while the cost cd for downloading a matching document is
also fixed. Then the overall cost of query qi is
Cost(qi) = cq + crP(qi) + cdP(qi). (1)
In certain cases, some of the documents from qi may have already
been downloaded from previous queries. In this case, the crawler
may skip downloading these documents and the cost of qi can be
Cost(qi) = cq + crP(qi) + cdPnew(qi). (2)
Here, we use Pnew(qi) to represent the fraction of the new 
documents from qi that have not been retrieved from previous queries.
Later in Section 3.1 we will study how we can estimate P(qi) and
Pnew(qi) to estimate the cost of qi.
Since our algorithms are independent of the exact cost function,
we will assume a generic cost function Cost(qi) in this paper. When
we need a concrete cost function, however, we will use Equation 2.
Given the notation, we can formalize the goal of a Hidden-Web
crawler as follows:
102
PROBLEM 1. Find the set of queries q1, . . . , qn that maximizes
P(q1 ∨ · · · ∨ qn)
under the constraint
n
i=1
Cost(qi) ≤ t.
Here, t is the maximum download resource that the crawler has.
