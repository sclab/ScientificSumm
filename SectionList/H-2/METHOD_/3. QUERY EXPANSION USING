DESKTOP DATA
Desktop data represents a very rich repository of profiling 
information. However, this information comes in a very 
unstructured way, covering documents which are highly diverse in 
format, content, and even language characteristics. In this section we
first tackle this problem by proposing several lexical analysis 
algorithms which exploit user"s PIR to extract keyword expansion terms
at various granularities, ranging from term frequency within 
Desktop documents up to utilizing global co-occurrence statistics over
the personal information repository. Then, in the second part of the
section we empirically analyze the performance of each approach.
3.1 Algorithms
This section presents the five generic approaches for 
analyzing user"s Desktop data in order to provide expansion terms for
Web search. In the proposed algorithms we gradually increase the
amount of personal information utilized. Thus, in the first part we
investigate three local analysis techniques focused only on those
Desktop documents matching user"s query best. We append to the
Web query the most relevant terms, compounds, and sentence 
summaries from these documents. In the second part of the section we
move towards a global Desktop analysis, proposing to investigate
term co-occurrences, as well as thesauri, in the expansion process.
3.1.1 Expanding with Local Desktop Analysis
Local Desktop Analysis is related to enhancing Pseudo 
Relevance Feedback to generate query expansion keywords from the
PIR best hits for user"s Web query, rather than from the top ranked
Web search results. We distinguish three granularity levels for this
process and we investigate each of them separately.
Term and Document Frequency. As the simplest possible 
measures, TF and DF have the advantage of being very fast to compute.
Previous experiments with small data sets have showed them to
yield very good results [11]. We thus independently associate a
score with each term, based on each of the two statistics. The TF
based one is obtained by multiplying the actual frequency of a term
with a position score descending as the term first appears closer
to the end of the document. This is necessary especially for longer
documents, because more informative terms tend to appear towards
their beginning [10]. The complete TF based keyword extraction
formula is as follows:
TermScore =
1
2
+
1
2
·
nrWords − pos
nrWords
!
· log(1 + TF) (1)
where nrWords is the total number of terms in the document and
pos is the position of the first appearance of the term; TF 
represents the frequency of each term in the Desktop document matching
user"s Web query.
The identification of suitable expansion terms is even simpler
when using DF: Given the set of Top-K relevant Desktop 
documents, generate their snippets as focused on the original search
request. This query orientation is necessary, since the DF scores
are computed at the level of the entire PIR and would produce too
noisy suggestions otherwise. Once the set of candidate terms has
been identified, the selection proceeds by ordering them according
to the DF scores they are associated with. Ties are resolved using
the corresponding TF scores.
Note that a hybrid TFxIDF approach is not necessarily efficient,
since one Desktop term might have a high DF on the Desktop,
while being quite rare in the Web. For example, the term 
PageRank would be quite frequent on the Desktop of an IR scientist,
thus achieving a low score with TFxIDF. However, as it is rather
rare in the Web, it would make a good resolution of the query 
towards the correct topic.
Lexical Compounds. Anick and Tipirneni [2] defined the 
lexical dispersion hypothesis, according to which an expression"s 
lexical dispersion (i.e., the number of different compounds it appears in
within a document or group of documents) can be used to 
automatically identify key concepts over the input document set. Although
several possible compound expressions are available, it has been
shown that simple approaches based on noun analysis are almost
as good as highly complex part-of-speech pattern identification 
algorithms [1]. We thus inspect the matching Desktop documents for
all their lexical compounds of the following form:
{ adjective? noun+ }
All such compounds could be easily generated off-line, at indexing
time, for all the documents in the local repository. Moreover, once
identified, they can be further sorted depending on their dispersion
within each document in order to facilitate fast retrieval of the most
frequent compounds at run-time.
Sentence Selection. This technique builds upon sentence 
oriented document summarization: First, the set of relevant Desktop
documents is identified; then, a summary containing their most
important sentences is generated as output. Sentence selection is
the most comprehensive local analysis approach, as it produces the
most detailed expansions (i.e., sentences). Its downside is that, 
unlike with the first two algorithms, its output cannot be stored 
efficiently, and consequently it cannot be computed off-line. We 
generate sentence based summaries by ranking the document sentences
according to their salience score, as follows [21]:
SentenceScore =
SW2
TW
+ PS +
TQ2
NQ
The first term is the ratio between the square amount of significant
words within the sentence and the total number of words therein. A
word is significant in a document if its frequency is above a 
threshold as follows:
TF > ms =
V
`
X
7 − 0.1 ∗ (25 − NS) , if NS < 25
7 , if NS ∈ [25, 40]
7 + 0.1 ∗ (NS − 40) , if NS > 40
with NS being the total number of sentences in the document
(see [21] for details). The second term is a position score set to
(Avg(NS) − SentenceIndex)/Avg2
(NS) for the first ten 
sentences, and to 0 otherwise, Avg(NS) being the average number of
sentences over all Desktop items. This way, short documents such
as emails are not affected, which is correct, since they usually do
not contain a summary in the very beginning. However, as longer
documents usually do include overall descriptive sentences in the
beginning [10], these sentences are more likely to be relevant. The
final term biases the summary towards the query. It is the ratio 
between the square number of query terms present in the sentence and
the total number of terms from the query. It is based on the belief
that the more query terms contained in a sentence, the more likely
will that sentence convey information highly related to the query.
3.1.2 Expanding with Global Desktop Analysis
In contrast to the previously presented approach, global analysis
relies on information from across the entire personal Desktop to
infer the new relevant query terms. In this section we propose two
such techniques, namely term co-occurrence statistics, and filtering
the output of an external thesaurus.
Term Co-occurrence Statistics. For each term, we can easily
compute off-line those terms co-occurring with it most frequently
in a given collection (i.e., PIR in our case), and then exploit this
information at run-time in order to infer keywords highly 
correlated with the user query. Our generic co-occurrence based query
expansion algorithm is as follows:
Algorithm 3.1.2.1. Co-occurrence based keyword similarity search.
Off-line computation:
1: Filter potential keywords k with DF ∈ [10, . . . , 20% · N]
2: For each keyword ki
3: For each keyword kj
4: Compute SCki,kj
, the similarity coefficient of (ki, kj)
On-line computation:
1: Let S be the set of keywords,
potentially similar to an input expression E.
2: For each keyword k of E:
3: S ← S ∪ TSC(k), where TSC(k) contains the
Top-K terms most similar to k
4: For each term t of S:
5a: Let Score(t) ←
Q
k∈E(0.01 + SCt,k)
5b: Let Score(t) ← #DesktopHits(E|t)
6: Select Top-K terms of S with the highest scores.
The off-line computation needs an initial trimming phase (step
1) for optimization purposes. In addition, we also restricted the 
algorithm to computing co-occurrence levels across nouns only, as
they contain by far the largest amount of conceptual information,
and as this approach reduces the size of the co-occurrence matrix
considerably. During the run-time phase, having the terms most
correlated with each particular query keyword already identified,
one more operation is necessary, namely calculating the correlation
of every output term with the entire query. Two approaches are
possible: (1) using a product of the correlation between the term
and all keywords in the original expression (step 5a), or (2) 
simply counting the number of documents in which the proposed term
co-occurs with the entire user query (step 5b). We considered the
following formulas for Similarity Coefficients [17]:
• Cosine Similarity, defined as:
CS =
DFx,y
pDFx · DFy
(2)
• Mutual Information, defined as:
MI = log
N · DFx,y
DFx · DFy
(3)
• Likelihood Ratio, defined in the paragraphs below.
DFx is the Document Frequency of term x, and DFx,y is the 
number of documents containing both x and y. To further increase the
quality of the generated scores we limited the latter indicator to 
cooccurrences within a window of W terms. We set W to be the same
as the maximum amount of expansion keywords desired.
Dunning"s Likelihood Ratio λ [9] is a co-occurrence based 
metric similar to χ2
. It starts by attempting to reject the null 
hypothesis, according to which two terms A and B would appear in
text independently from each other. This means that P(A B) =
P(A¬B) = P(A), where P(A¬B) is the probability that term A
is not followed by term B. Consequently, the test for independence
of A and B can be performed by looking if the distribution of A
given that B is present is the same as the distribution of A given
that B is not present. Of course, in reality we know these terms are
not independent in text, and we only use the statistical metrics to
highlight terms which are frequently appearing together. We 
compare the two binomial processes by using likelihood ratios of their
associated hypotheses. First, let us define the likelihood ratio for
one hypothesis:
λ =
maxω∈Ω0
H(ω; k)
maxω∈Ω H(ω; k)
(4)
where ω is a point in the parameter space Ω, Ω0 is the particular
hypothesis being tested, and k is a point in the space of observations
K. If we assume that two binomial distributions have the same
underlying parameter, i.e., {(p1, p2) | p1 = p2}, we can write:
λ =
maxp H(p, p; k1, k2, n1, n2)
maxp1,p2
H(p1, p2; k1, k2, n1, n2)
(5)
where H(p1, p2; k1, k2, n1, n2) = pk1
1 · (1 − p1)(n1−k1)
·
 n1
k1
¡
·
pk2
2 · (1 − p2)(n2−k2)
·
 n2
k2
¡
. Since the maxima are obtained with
p1 = k1
n1
, p2 = k2
n2
, and p = k1+k2
n1+n2
, we have:
λ =
maxp L(p, k1, n1)L(p, k2, n2)
maxp1,p2 L(p1, k1, n1)L(p2, k2, n2)
(6)
where L(p, k, n) = pk
· (1 − p)n−k
. Taking the logarithm of the
likelihood, we obtain:
−2 · log λ = 2 · [log L(p1, k1, n1) + log L(p2, k2, n2) −
log L(p, k1, n1) − log L(p, k2, n2)]
where log L(p, k, n) = k · log p + (n − k) · log(1 − p). Finally,
if we write O11 = P(A B), O12 = P(¬A B), O21 = P(A ¬B),
and O22 = P(¬A¬B), then the co-occurrence likelihood of terms
A and B becomes:
−2 · log λ = 2 · [O11 · log p1 + O12 · log (1 − p1) +
O21 · log p2 + O22 · log (1 − p2) −
(O11 + O21) · log p − (O12 + O22) · log (1 − p)]
where p1 =
k1
n1
=
O11
O11+O12
, p2 =
k2
n2
=
O21
O21+O22
, and p =
k1+k2
n1+n2
Thesaurus Based Expansion. Large scale thesauri encapsulate
global knowledge about term relationships. Thus, we first identify
the set of terms closely related to each query keyword, and then
we calculate the Desktop co-occurrence level of each of these 
possible expansion terms with the entire initial search request. In the
end, those suggestions with the highest frequencies are kept. The
algorithm is as follows:
Algorithm 3.1.2.2. Filtered thesaurus based query expansion.
1: For each keyword k of an input query Q:
2: Select the following sets of related terms using WordNet:
2a: Syn: All Synonyms
2b: Sub: All sub-concepts residing one level below k
2c: Super: All super-concepts residing one level above k
3: For each set Si of the above mentioned sets:
4: For each term t of Si:
5: Search the PIR with (Q|t), i.e.,
the original query, as expanded with t
6: Let H be the number of hits of the above search
(i.e., the co-occurence level of t with Q)
7: Return Top-K terms as ordered by their H values.
We observe three types of term relationships (steps 2a-2c): (1)
synonyms, (2) sub-concepts, namely hyponyms (i.e., sub-classes)
and meronyms (i.e., sub-parts), and (3) super-concepts, namely
hypernyms (i.e., super-classes) and holonyms (i.e., super-parts).
As they represent quite different types of association, we 
investigated them separately. We limited the output expansion set
(step 7) to contain only terms appearing at least T times on
the Desktop, in order to avoid noisy suggestions, with T =
min( N
DocsPerTopic
, MinDocs). We set DocsPerTopic = 2, 500, and
MinDocs = 5, the latter one coping with the case of small PIRs.
3.2 Experiments
3.2.1 Experimental Setup
We evaluated our algorithms with 18 subjects (Ph.D. and 
PostDoc. students in different areas of computer science and 
education). First, they installed our Lucene based search engine3
and
3
Clearly, if one had already installed a Desktop search application,
then this overhead would not be present.
indexed all their locally stored content: Files within user selected
paths, Emails, and Web Cache. Without loss of generality, we 
focused the experiments on single-user machines. Then, they chose
4 queries related to their everyday activities, as follows:
• One very frequent AltaVista query, as extracted from the top
2% queries most issued to the search engine within a 7.2 
million entries log from October 2001. In order to connect such
a query to each user"s interests, we added an off-line 
preprocessing phase: We generated the most frequent search 
requests and then randomly selected a query with at least 10
hits on each subject"s Desktop. To further ensure a real life
scenario, users were allowed to reject the proposed query and
ask for a new one, if they considered it totally outside their
interest areas.
• One randomly selected log query, filtered using the same 
procedure as above.
• One self-selected specific query, which they thought to have
only one meaning.
• One self-selected ambiguous query, which they thought to
have at least three meanings.
The average query lengths were 2.0 and 2.3 terms for the log
queries, as well as 2.9 and 1.8 for the self-selected ones. Even
though our algorithms are mainly intended to enhance search when
using ambiguous query keywords, we chose to investigate their 
performance on a wide span of query types, in order to see how they
perform in all situations. The log queries evaluate real life requests,
in contrast to the self-selected ones, which target rather the 
identification of top and bottom performances. Note that the former ones
were somewhat farther away from each subject"s interest, thus 
being also more difficult to personalize on. To gain an insight into the
relationship between each query type and user interests, we asked
each person to rate the query itself with a score of 1 to 5, having the
following interpretations: (1) never heard of it, (2) do not know it,
but heard of it, (3) know it partially, (4) know it well, (5) major 
interest. The obtained grades were 3.11 for the top log queries, 3.72
for the randomly selected ones, 4.45 for the self-selected specific
ones, and 4.39 for the self-selected ambiguous ones.
For each query, we collected the Top-5 URLs generated by 20
versions of the algorithms4
presented in Section 3.1. These results
were then shuffled into one set containing usually between 70 and
90 URLs. Thus, each subject had to assess about 325 documents
for all four queries, being neither aware of the algorithm, nor of
the ranking of each assessed URL. Overall, 72 queries were issued
and over 6,000 URLs were evaluated during the experiment. For
each of these URLs, the testers had to give a rating ranging from
0 to 2, dividing the relevant results in two categories, (1) relevant
and (2) highly relevant. Finally, the quality of each ranking was
assessed using the normalized version of Discounted Cumulative
Gain (DCG) [15]. DCG is a rich measure, as it gives more weight
to highly ranked documents, while also incorporating different 
relevance levels by giving them different gain values:
DCG(i) =
&
G(1) , if i = 1
DCG(i − 1) + G(i)/log(i) , otherwise.
We used G(i) = 1 for relevant results, and G(i) = 2 for highly 
relevant ones. As queries having more relevant output documents will
have a higher DCG, we also normalized its value to a score between
0 (the worst possible DCG given the ratings) and 1 (the best 
possible DCG given the ratings) to facilitate averaging over queries. All
results were tested for statistical significance using T-tests.
4
Note that all Desktop level parts of our algorithms were performed with
Lucene using its predefined searching and ranking functions.
Algorithmic specific aspects. The main parameter of our 
algorithms is the number of generated expansion keywords. For this
experiment we set it to 4 terms for all techniques, leaving an 
analysis at this level for a subsequent investigation. In order to optimize
the run-time computation speed, we chose to limit the number of
output keywords per Desktop document to the number of 
expansion keywords desired (i.e., four). For all algorithms we also 
investigated bigger limitations. This allowed us to observe that the
Lexical Compounds method would perform better if only at most
one compound per document were selected. We therefore chose
to experiment with this new approach as well. For all other 
techniques, considering less than four terms per document did not seem
to consistently yield any additional qualitative gain. We labeled the
algorithms we evaluated as follows:
0. Google: The actual Google query output, as returned by the
Google API;
1. TF, DF: Term and Document Frequency;
2. LC, LC[O]: Regular and Optimized (by considering only
one top compound per document) Lexical Compounds;
3. SS: Sentence Selection;
4. TC[CS], TC[MI], TC[LR]: Term Co-occurrence Statistics
using respectively Cosine Similarity, Mutual Information,
and Likelihood Ratio as similarity coefficients;
5. WN[SYN], WN[SUB], WN[SUP]: WordNet based 
expansion with synonyms, sub-concepts, and super-concepts, 
respectively.
Except for the thesaurus based expansion, in all cases we also 
investigated the performance of our algorithms when exploiting only the
Web browser cache to represent user"s personal information. This
is motivated by the fact that other personal documents such as for
example emails are known to have a somewhat different language
than that residing on the world wide Web [34]. However, as this
approach performed visibly poorer than using the entire Desktop
data, we omitted it from the subsequent analysis.
3.2.2 Results
Log Queries. We evaluated all variants of our algorithms using
NDCG. For log queries, the best performance was achieved with
TF, LC[O], and TC[LR]. The improvements they brought were up
to 5.2% for top queries (p = 0.14) and 13.8% for randomly 
selected queries (p = 0.01, statistically significant), both obtained
with LC[O]. A summary of all results is depicted in Table 1.
Both TF and LC[O] yielded very good results, indicating that
simple keyword and expression oriented approaches might be 
sufficient for the Desktop based query expansion task. LC[O] was much
better than LC, ameliorating its quality with up to 25.8% in the case
of randomly selected log queries, improvement which was also 
significant with p = 0.04. Thus, a selection of compounds spanning
over several Desktop documents is more informative about user"s
interests than the general approach, in which there is no restriction
on the number of compounds produced from every personal item.
The more complex Desktop oriented approaches, namely 
sentence selection and all term co-occurrence based algorithms,
showed a rather average performance, with no visible 
improvements, except for TC[LR]. Also, the thesaurus based expansion
usually produced very few suggestions, possibly because of the
many technical queries employed by our subjects. We observed
however that expanding with sub-concepts is very good for 
everyday life terms (e.g., car), whereas the use of super-concepts is
valuable for compounds having at least one term with low 
technicality (e.g., document clustering). As expected, the synonym
based expansion performed generally well, though in some very
Algorithm NDCG Signific. NDCG Signific.
Top vs. Google Random vs. Google
Google 0.42 - 
0.40TF 0.43 p = 0.32 0.43 p = 0.04
DF 0.17 - 
0.23LC 0.39 - 
0.36LC[O] 0.44 p = 0.14 0.45 p = 0.01
SS 0.33 - 
0.36TC[CS] 0.37 - 
0.35TC[MI] 0.40 - 
0.36TC[LR] 0.41 - 0.42 p = 0.06
WN[SYN] 0.42 - 
0.38WN[SUB] 0.28 - 
0.33WN[SUP] 0.26 - 
0.26Table 1: Normalized Discounted Cumulative Gain at the first
5 results when searching for top (left) and random (right) log
queries.
Algorithm NDCG Signific. NDCG Signific.
Clear vs. Google Ambiguous vs. Google
Google 0.71 - 
0.39TF 0.66 - 0.52 p 0.01
DF 0.37 - 
0.31LC 0.65 - 0.54 p 0.01
LC[O] 0.69 - 0.59 p 0.01
SS 0.56 - 0.52 p 0.01
TC[CS] 0.60 - 0.50 p = 0.01
TC[MI] 0.60 - 0.47 p = 0.02
TC[LR] 0.56 - 0.47 p = 0.03
WN[SYN] 0.70 - 
0.36WN[SUB] 0.46 - 
0.32WN[SUP] 0.51 - 
0.29Table 2: Normalized Discounted Cumulative Gain at the first
5 results when searching for user selected clear (left) and 
ambiguous (right) queries.
technical cases it yielded rather general suggestions. Finally, we
noticed Google to be very optimized for some top frequent queries.
However, even within this harder scenario, some of our 
personalization algorithms produced statistically significant improvements
over regular search (i.e., TF and LC[O]).
Self-selected Queries. The NDCG values obtained with 
selfselected queries are depicted in Table 2. While our algorithms did
not enhance Google for the clear search tasks, they did produce
strong improvements of up to 52.9% (which were of course also
highly significant with p 0.01) when utilized with ambiguous
queries. In fact, almost all our algorithms resulted in statistically
significant improvements over Google for this query type.
In general, the relative differences between our algorithms were
similar to those observed for the log based queries. As in the 
previous analysis, the simple Desktop based Term Frequency and 
Lexical Compounds metrics performed best. Nevertheless, a very good
outcome was also obtained for Desktop based sentence selection
and all term co-occurrence metrics. There were no visible 
differences between the behavior of the three different approaches to 
cooccurrence calculation. Finally, for the case of clear queries, we
noticed that fewer expansion terms than 4 might be less noisy and
thus helpful in bringing further improvements. We thus pursued
this idea with the adaptive algorithms presented in the next section.
