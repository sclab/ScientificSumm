The TIGRE Grid middleware consists of minimal set of
components derived from a subset of the Virtual Data
Toolkit (VDT) [9] which supports a variety of operating
systems. The purpose of choosing a minimal software stack
is to support applications at hand, and to simplify
installation and distribution of client/server stacks across
TIGRE sites. Additional components will be added as they
become necessary. The PacMan [10] packaging and
distribution mechanism is employed for TIGRE
client/server installation and management. The PacMan
distribution mechanism involves retrieval, installation, and
often configuration of the packaged software. This
approach allows the clients to keep current, consistent
versions of TIGRE software. It also helps TIGRE sites to
install the needed components on resources distributed
throughout the participating sites. The TIGRE client/server
stack consists of an authentication and authorization layer,
Globus GRAM4-based job submission via web services
(pre-web services installations are available up on request).
The tools for handling Grid proxy generation, Grid-enabled
file transfer and Grid-enabled remote login are supported.
The pertinent details of TIGRE services and tools for job
scheduling and management are provided below.
2.1. Certificate Authority
The TIGRE security infrastructure includes a certificate
authority (CA) accredited by the International Grid Trust
Federation (IGTF) for issuing X. 509 user and resource
Grid certificates [11]. The Texas Advanced Computing
Center (TACC), University of Texas at Austin is the
TIGRE"s shared CA. The TIGRE Institutions serve as
Registration Authorities (RA) for their respective local user
base. For up-to-date information on securing user and
resource certificates and their installation instructions see
ref [2]. The users and hosts on TIGRE are identified by
their distinguished name (DN) in their X.509 certificate
provided by the CA. A native Grid-mapfile that contains a
list of authorized DNs is used to authenticate and authorize
user job scheduling and management on TIGRE site
resources. At Texas Tech University, the users are
dynamically allocated one of the many generic pool
accounts. This is accomplished through the Grid User
Management System (GUMS) [12].
2.2. Job Scheduling and Management
The TIGRE environment supports GRAM4-based job
submission via web services. The job submission scripts are
generated using XML. The web services GRAM translates
the XML scripts into target cluster specific batch schedulers
such as LSF, PBS, or SGE. The high bandwidth file transfer
protocols such as GridFTP are utilized for staging files in
and out of the target machine. The login to remote hosts for
compilation and debugging is only through GSISSH service
which requires resource authentication through X.509
certificates. The authentication and authorization of Grid
jobs are managed by issuing Grid certificates to both users
and hosts. The certificate revocation lists (CRL) are
updated on a daily basis to maintain high security standards
of the TIGRE Grid services. The TIGRE portal [2]
documentation area provides a quick start tutorial on
running jobs on TIGRE.
2.3. Metascheduler
The metascheduler interoperates with the cluster level
batch schedulers (such as LSF, PBS) in the overall Grid
workflow management. In the present work, we have
employed GridWay [8] metascheduler - a Globus incubator
project - to schedule and manage jobs across TIGRE.
The GridWay is a light-weight metascheduler that fully
utilizes Globus functionalities. It is designed to provide
efficient use of dynamic Grid resources by multiple users
for Grid infrastructures built on top of Globus services. The
TIGRE site administrator can control the resource sharing
through a powerful built-in scheduler provided by GridWay
or by extending GridWay"s external scheduling module to
provide their own scheduling policies. Application users
can write job descriptions using GridWay"s simple and
direct job template format (see Section 4 for details) or
standard Job Submission Description Language (JSDL).
See section 4 for implementation details.
2.4. Customer Service Management System
A TIGRE portal [2] was designed and deployed to interface
users and resource providers. It was designed using
GridPort [13] and is maintained by TACC. The TIGRE
environment is supported by open source tools such as the
Open Ticket Request System (OTRS) [14] for servicing
trouble tickets, and MoinMoin [15] Wiki for TIGRE
content and knowledge management for education, outreach
and training. The links for OTRS and Wiki are consumed
by the TIGRE portal [2] - the gateway for users and
resource providers. The TIGRE resource status and loads
are monitored by the Grid Port Information Repository
(GPIR) service of the GridPort toolkit [13] which interfaces
with local cluster load monitoring service such as Ganglia.
The GPIR utilizes cron jobs on each resource to gather
site specific resource characteristics such as jobs that are
running, queued and waiting for resource allocation.
