COMMENTS
The free textual comments associated to online reviews
are a valuable source of information for understanding the
reasons behind the numerical ratings left by the reviewers.
The text may, for example, reveal concrete examples of 
aspects that the user liked or disliked, thus justifying some of
the high, respectively low ratings for certain features. The
text may also offer guidelines for understanding the 
preferences of the reviewer, and the weights of different features
when computing an overall rating.
The problem, however, is that free textual comments are
difficult to read. Users are required to scroll through many
reviews and read mostly repetitive information. Significant
improvements would be obtained if the reviews were 
automatically interpreted and aggregated. Unfortunately, this
seems a difficult task for computers since human users often
use witty language, abbreviations, cultural specific phrases,
and the figurative style.
Nevertheless, several important results use the textual
comments of online reviews in an automated way. Using well
established natural language techniques, reviews or parts of
reviews can be classified as having a positive or negative 
semantic orientation. Pang et al. [2] classify movie reviews
into positive/negative by training three different classifiers
(Naive Bayes, Maximum Entropy and SVM) using 
classification features based on unigrams, bigrams or part-of-speech
tags.
Dave et al. [4] analyze reviews from CNet and 
Amazon, and surprisingly show that classification features based
on unigrams or bigrams perform better than higher-order
n-grams. This result is challenged by Cui et al. [3] who
look at large collections of reviews crawled from the web.
They show that the size of the data set is important, and
that bigger training sets allow classifiers to successfully use
more complex classification features based on n-grams.
Hu and Liu [11] also crawl the web for product reviews and
automatically identify product attributes that have been 
discussed by reviewers. They use Wordnet to compute the 
semantic orientation of product evaluations and summarize
user reviews by extracting positive and negative evaluations
of different product features. Popescu and Etzioni [20] 
analyze a similar setting, but use search engine hit-counts to
identify product attributes; the semantic orientation is 
assigned through the relaxation labeling technique.
Ghose et al. [7, 8] analyze seller reviews from the Amazon
secondary market to identify the different dimensions (e.g.,
delivery, packaging, customer support, etc.) of reputation.
They parse the text, and tag the part-of-speech for each
word. Frequent nouns, noun phrases and verbal phrases
are identified as dimensions of reputation, while the 
corresponding modifiers (i.e., adjectives and adverbs) are used to
derive numerical scores for each dimension. The enhanced
reputation measure correlates better with the pricing 
information observed in the market. Pavlou and Dimoka [19]
analyze eBay reviews and find that textual comments have
an important impact on reputation premiums.
Our approach is similar to the previously mentioned
works, in the sense that we identify the aspects (i.e., 
hotel features) discussed by the users in the textual reviews.
However, we do not compute the semantic orientation of the
text, nor attempt to infer missing ratings.
We define the weight, wi
f , of feature f ∈ F in the text
Ti
associated with the review (ri
, Ti
), as the fraction of Ti
dedicated to discussing aspects (both positive and negative)
related to feature f. We propose an elementary method to
approximate the values of these weights. For each feature
we manually construct the word list Lf containing 
approximately 50 words that are most commonly associated to the
feature f. The initial words were selected from reading some
of the reviews, and seeing what words coincide with 
discussion of which features. The list was then extended by adding
all thesaurus entries that were related to the initial words.
Finally, we brainstormed for missing words that would 
normally be associated with each of the features.
Let Lf ∩Ti
be the list of terms common to both Lf and Ti.
Each term of Lf is counted the number of times it appears
in Ti
, with two exceptions:
• in cases where the user submits a title to the review,
we account for the title text by appending it three
times to the review text Ti
. The intuitive assumption
is that the user"s opinion is more strongly reflected in
the title, rather than in the body of the review. For
example, many reviews are accurately summarized by
titles such as Excellent service, terrible location or
Bad value for money;
• certain words that occur only once in the text are
counted multiple times if their relevance to that 
feature is particularly strong. These were "root" words for
each feature (e.g., "staff" is a root word for the feature
Service), and were weighted either 2 or 3. Each 
feature was assigned up to 3 such root words, so almost
all words are counted only once.
The list of words for the feature Rooms is given for reference
in Appendix A.
The weight wi
f is computed as:
wi
f =
|Lf ∩ Ti|
f∈F |Lf ∩ Ti|
(1)
where |Lf ∩Ti
| is the number of terms common to Lf and Ti
.
The weight for the feature Overall was set to min{ |T i
|
5000
, 1}
where |Ti
| is the number of character in Ti
.
The following is a TripAdvisor review for a Boston hotel
(the name of the hotel is omitted): I"ll start by saying that
I"m more of a Holiday Inn person than a *** type. So I get
frustrated when I pay double the room rate and get half the
amenities that I"d get at a Hampton Inn or Holiday Inn. The
location was definitely the main asset of this place. It was
only a few blocks from the Hynes Center subway stop and it
was easy to walk to some good restaurants in the Back Bay
area. Boylston isn"t far off at all. So I had no trouble with
foregoing a rental car and taking the subway from the 
airport to the hotel and using the subway for any other travel.
Otherwise, they make you pay for anything and everything.
136
And when you"ve already dropped $215/night on the room,
that gets frustrating.The room itself was decent, about what I
would expect. Staff was also average, not bad and not 
excellent. Again, I think you"re paying for location and the ability
to walk to a lot of good stuff. But I think next time I"ll stay
in Brookline, get more amenities, and use the subway a bit
more.
This numerical ratings associated to this review are rO =
3, rR = 3, rS = 3, rC = 4, rV = 2 for features Overall(O),
Rooms(R), Service(S), Cleanliness(C) and Value(V) 
respectively. The ratings for the features Food(F), Location(L) and
Noise(N) are absent (i.e., rF = rL = rN = 0).
The weights wf are computed from the following lists of
common terms:
LR ∩ T ={room}; wR = 0.066
LS ∩ T ={3 * Staff, amenities}; wS = 0.267
LC ∩ T = ∅; wC = 0
LV ∩ T ={$, rate}; wV = 0.133
LF ∩ T ={restaurant}; wF = 0.067
LL ∩ T ={2 * center, 2 * walk, 2 * location, area}; wL = 0.467
LN ∩ T = ∅; wN = 0
The root words "Staff" and "Center" were tripled and 
doubled respectively. The overall weight of the textual review
is wO = 0.197. These values account reasonably well for the
weights of different features in the discussion of the reviewer.
One point to note is that some terms in the lists Lf possess
an inherent semantic orientation. For example the word
"grime" (belonging to the list LC ) would be used most often
to assert the presence, and not the absence of grime. This is
unavoidable, but care was taken to ensure words from both
sides of the spectrum were used. For this reason, some lists
such as LR contain only nouns of objects that one would
typically describe in a room (see Appendix A).
The goal of this section is to analyse the influence of the
weights wi
f on the numerical ratings ri
f . Intuitively, users
who spent a lot of their time discussing a feature f (i.e., wi
f
is high) had something to say about their experience with
regard to this feature. Obviously, feature f is important for
user i. Since people tend to be more knowledgeable in the
aspects they consider important, our hypothesis is that the
ratings ri
f (corresponding to high weights wi
f ) constitute a
subset of expert ratings for feature f.
Figure 2 plots the distribution of the rates r
i(h)
C with 
respect to the weights w
i(h)
C for the cleanliness of a Las Vegas
hotel, h. Here, the high ratings are restricted to the reviews
that discuss little the cleanliness. Whenever cleanliness 
appears in the discussion, the ratings are low. Many hotels
exhibit similar rating patterns for various features. Ratings
corresponding to low weights span the whole spectrum from
1 to 5, while the ratings corresponding to high weights are
more grouped together (either around good or bad ratings).
We therefore make the following hypothesis:
Hypothesis 1. The ratings ri
f corresponding to the 
reviews where wi
f is high, are more similar to each other than
to the overall collection of ratings.
To test the hypothesis, we take the entire set of reviews,
and feature by feature, we compute the standard deviation
of the ratings with high weights, and the standard deviation
of the entire set of ratings. High weights were defined as
those belonging to the upper 20% of the weight range for
the corresponding feature. If Hypothesis 1 were true, the
standard deviation of all ratings should be higher than the
standard deviation of the ratings with high weights.
0
1
2
3
4
5
6
0 0.1 0.2 0.3 0.4 0.5 0.6
Rating
Weight
Figure 2: The distribution of ratings against the
weight of the cleanliness feature.
We use a standard T-test to measure the significance of
the results. City by city and feature by feature, Table 2
presents the average standard deviation of all ratings, and
the average standard deviation of ratings with high weights.
Indeed, the ratings with high weights have lower standard
deviation, and the results are significant at the standard 0.05
significance threshold (although for certain cities taken 
independently there doesn"t seem to be a significant difference,
the results are significant for the entire data set). Please
note that only the features O,R,S,C and V were considered,
since for the others (F, L, and N) we didn"t have enough
ratings.
Table 2: Average standard deviation for all 
ratings, and average standard deviation for ratings with
high weights. In square brackets, the corresponding
p-values for a positive difference between the two.
City O R S C V
all 1.189 0.998 1.144 0.935 1.123
Boston high 0.948 0.778 0.954 0.767 0.891
p-val [0.000] [0.004] [0.045] [0.080] [0.009]
all 1.040 0.832 1.101 0.847 0.963
Sydney high 0.801 0.618 0.691 0.690 0.798
p-val [0.012] [0.023] [0.000] [0.377] [0.037]
all 1.272 1.142 1.184 1.119 1.242
Vegas high 1.072 0.752 1.169 0.907 1.003
p-val [0.0185] [0.001] [0.918] [0.120] [0.126]
Hypothesis 1 not only provides some basic understanding
regarding the rating behavior of online users, it also suggests
some ways of computing better quality estimates. We can,
for example, construct a feature-by-feature quality estimate
with much lower variance: for each feature we take the 
subset of reviews that amply discuss that feature, and output
as a quality estimate the average rating for this subset. 
Initial experiments suggest that the average feature-by-feature
ratings computed in this way are different from the average
ratings computed on the whole data set. Given that, 
indeed, high weights are indicators of expert opinions, the
estimates obtained in this way are more accurate than the
current ones. Nevertheless, the validation of this underlying
assumption requires further controlled experiments.
137
