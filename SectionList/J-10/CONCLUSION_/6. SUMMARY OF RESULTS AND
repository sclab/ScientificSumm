CONCLUSION
The goal of this paper is to explore the factors that drive
a user to submit a particular rating, rather than the 
incentives that encouraged him to submit a report in the first
place. For that we use two additional sources of information
besides the vector of numerical ratings: first we look at the
textual comments that accompany the reviews, and second
we consider the reports that have been previously submitted
by other users.
Using simple natural language processing algorithms, we
were able to establish a correlation between the weight of a
certain feature in the textual comment accompanying the 
review, and the noise present in the numerical rating. 
Specifically, it seems that users who discuss amply a certain feature
are likely to agree on a common rating. This observation
allows the construction of feature-by-feature estimators of
quality that have a lower variance, and are hopefully less
noisy. Nevertheless, further evidence is required to support
the intuition that ratings corresponding to high weights are
expert opinions that deserve to be given higher priority when
computing estimates of quality.
Second, we emphasize the dependence of ratings on 
previous reports. Previous reports create an expectation of 
quality which affects the subjective perception of the user. We
validate two facts about the hotel reviews we collected from
TripAdvisor: First, the ratings following low expectations
(where the expectation is computed as the average of the
previous reports) are likely to be higher than the ratings
Table 10: Precision(p), Recall(r), s= 2pr
p+r
while 
spotting poor ratings for Sydney
O R S C V
p 0.650 0.463 0.544 0.550 0.580
σ r 0.234 0.378 0.571 0.169 0.592
s 0.343 0.452 0.557 0.259 0.586
p 0.562 0.615 0.600 0.500 0.600
τ r 0.054 0.098 0.101 0.015 0.175
s 0.098 0.168 0.172 0.030 0.271
following high expectations. Intuitively, the perception of
quality (and consequently the rating) depends on how well
the actual experience of the user meets her expectation. 
Second, we include evidence from the textual comments, and
find that when users devote a large fraction of the text to
discussing a certain feature, they are likely to motivate a
divergent rating (i.e., a rating that does not conform to the
prior expectation). Intuitively, this supports the hypothesis
that review forums act as discussion groups where users are
keen on presenting and motivating their own opinion.
We have captured the empirical evidence in a behavior
model that predicts the ratings submitted by the users. The
final rating depends, as expected, on the true observation,
and on the gap between the observation and the expectation.
The gap tends to have a bigger influence when an important
fraction of the textual comment is dedicated to discussing a
certain feature. The proposed model was validated on the
empirical data and provides better estimates of the ratings
actually submitted.
One assumption that we make is about the existence of an
objective quality value vf for the feature f. This is rarely
true, especially over large spans of time. Other 
explanations might account for the correlation of ratings with past
reports. For example, if ef (i) reflects the true value of f at a
point in time, the difference in the ratings following high and
low expectations can be explained by hotel revenue models
that are maximized when the value is modified accordingly.
However, the idea that variation in ratings is not primarily
a function of variation in value turns out to be a useful one.
Our approach to approximate this elusive "objective value" is
by no means perfect, but conforms neatly to the idea behind
the model.
A natural direction for future work is to examine 
concrete applications of our results. Significant improvements
of quality estimates are likely to be obtained by 
incorporating all empirical evidence about rating behavior. Exactly
how different factors affect the decisions of the users is not
clear. The answer might depend on the particular 
application, context and culture.
