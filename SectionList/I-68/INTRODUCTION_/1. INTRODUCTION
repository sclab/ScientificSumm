The development of algorithms for effective coordination of 
multiple agents acting as a team in uncertain and time critical domains
has recently become a very active research field with potential 
applications ranging from coordination of agents during a hostage 
rescue mission [11] to the coordination of Autonomous Mars 
Exploration Rovers [2]. Because of the uncertain and dynamic 
characteristics of such domains, decision-theoretic models have received
a lot of attention in recent years, mainly thanks to their 
expressiveness and the ability to reason about the utility of actions over
time.
Key decision-theoretic models that have become popular in the 
literature include Decentralized Markov Decision Processes 
(DECMDPs) and Decentralized, Partially Observable Markov Decision
Processes (DEC-POMDPs). Unfortunately, solving these models
optimally has been proven to be NEXP-complete [3], hence more
tractable subclasses of these models have been the subject of 
intensive research. In particular, Network Distributed POMDP [13]
which assume that not all the agents interact with each other, 
Transition Independent DEC-MDP [2] which assume that transition 
function is decomposable into local transition functions or DEC-MDP
with Event Driven Interactions [1] which assume that interactions
between agents happen at fixed time points constitute good 
examples of such subclasses. Although globally optimal algorithms for
these subclasses have demonstrated promising results, domains on
which these algorithms run are still small and time horizons are
limited to only a few time ticks.
To remedy that, locally optimal algorithms have been proposed
[12] [4] [5]. In particular, Opportunity Cost DEC-MDP [4] [5],
referred to as OC-DEC-MDP, is particularly notable, as it has been
shown to scale up to domains with hundreds of tasks and double
digit time horizons. Additionally, OC-DEC-MDP is unique in its
ability to address both temporal constraints and uncertain method
execution durations, which is an important factor for real-world 
domains. OC-DEC-MDP is able to scale up to such domains mainly
because instead of searching for the globally optimal solution, it
carries out a series of policy iterations; in each iteration it performs
a value iteration that reuses the data computed during the previous
policy iteration. However, OC-DEC-MDP is still slow, especially
as the time horizon and the number of methods approach large 
values. The reason for high runtimes of OC-DEC-MDP for such 
domains is a consequence of its huge state space, i.e., OC-DEC-MDP
introduces a separate state for each possible pair of method and
method execution interval. Furthermore, OC-DEC-MDP 
overestimates the reward that a method expects to receive for enabling
the execution of future methods. This reward, also referred to as
the opportunity cost, plays a crucial role in agent decision making,
and as we show later, its overestimation leads to highly suboptimal
policies.
In this context, we present VFP (= Value Function P ropagation),
an efficient solution technique for the DEC-MDP model with 
temporal constraints and uncertain method execution durations, that
builds on the success of OC-DEC-MDP. VFP introduces our two
orthogonal ideas: First, similarly to [7] [9] and [10], we maintain
830
978-81-904262-7-5 (RPS) c 2007 IFAAMAS
and manipulate a value function over time for each method rather
than a separate value for each pair of method and time interval.
Such representation allows us to group the time points for which
the value function changes at the same rate (= its slope is 
constant), which results in fast, functional propagation of value 
functions. Second, we prove (both theoretically and empirically) that
OC-DEC- MDP overestimates the opportunity cost, and to remedy
that, we introduce a set of heuristics, that correct the opportunity
cost overestimation problem.
This paper is organized as follows: In section 2 we motivate this
research by introducing a civilian rescue domain where a team of
fire- brigades must coordinate in order to rescue civilians trapped in
a burning building. In section 3 we provide a detailed description of
our DEC-MDP model with Temporal Constraints and in section 4
we discuss how one could solve the problems encoded in our model
using globally optimal and locally optimal solvers. Sections 5 and
6 discuss the two orthogonal improvements to the state-of-the-art
OC-DEC-MDP algorithm that our VFP algorithm implements. 
Finally, in section 7 we demonstrate empirically the impact of our two
orthogonal improvements, i.e., we show that: (i) The new 
heuristics correct the opportunity cost overestimation problem leading to
higher quality policies, and (ii) By allowing for a systematic 
tradeoff of solution quality for time, the VFP algorithm runs much faster
than the OC-DEC-MDP algorithm
