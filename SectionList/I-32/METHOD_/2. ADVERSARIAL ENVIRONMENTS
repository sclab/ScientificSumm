The adversarial environment model (denoted as AE) is 
intended to guide the design of agents by providing a 
specification of the capabilities and mental attitudes of an agent in
an adversarial environment. We focus here on specific types
of adversarial environments, specified as follows:
1. Zero-Sum Interactions: positive and negative utilities
of all agents sum to zero;
2. Simple AEs: all agents in the environment are 
adversarial agents;
3. Bilateral AEs: AE"s with exactly two agents;
4. Multilateral AEs": AE"s of three or more agents.
We will work on both bilateral and multilateral 
instantiations of zero-sum and simple environments. In particular,
our adversarial environment model will deal with 
interactions that consist of N agents (N ≥ 2), where all agents
are adversaries, and only one agent can succeed. Examples
of such environments range from board games (e.g., Chess,
Connect-Four, and Diplomacy) to certain economic 
environments (e.g., N-bidder auctions over a single good).
2.1 Model Overview
Our approach is to formalize the mental attitudes and
behaviors of a single adversarial agent; we consider how a
single agent perceives the AE. The following list specifies
the conditions and mental states of an agent in a simple,
zero-sum AE:
1. The agent has an individual intention that its own goal
will be completed;
2. The agent has an individual belief that it and its 
adversaries are pursuing full conflicting goals (defined 
below)there can be only one winner;
3. The agent has an individual belief that each adversary
has an intention to complete its own full conflicting goal;
4. The agent has an individual belief in the (partial) profile
of its adversaries.
Item 3 is required, since it might be the case that some
agent has a full conflicting goal, and is currently considering
adopting the intention to complete it, but is, as of yet, not
committed to achieving it. This might occur because the
agent has not yet deliberated about the effects that 
adopting that intention might have on the other intentions it is
currently holding. In such cases, it might not consider itself
to even be in an adversarial environment.
Item 4 states that the agent should hold some belief about
the profiles of its adversaries. The profile represents all the
knowledge the agent has about its adversary: its weaknesses,
strategic capabilities, goals, intentions, trustworthiness, and
more. It can be given explicitly or can be learned from
observations of past encounters.
2.2 Model Definitions for Mental States
We use Grosz and Kraus"s definitions of the modal 
operators, predicates, and meta-predicates, as defined in their
SharedPlan formalization [4]. We recall here some of the
predicates and operators that are used in that 
formalization: Int.To(Ai, α, Tn, Tα, C) represents Ai"s intentions at
time Tn to do an action α at time Tα in the context of
C. Int.Th(Ai, prop, Tn, Tprop, C) represents Ai"s intentions
at time Tn that a certain proposition prop holds at time
Tprop in the context of C. The potential intention 
operators, Pot.Int.To(...) and Pot.Int.Th(...), are used to 
represent the mental state when an agent considers adopting an
intention, but has not deliberated about the interaction of
the other intentions it holds. The operator Bel(Ai, f, Tf )
represents agent Ai believing in the statement expressed in
formula f, at time Tf . MB(A, f, Tf ) represents mutual 
belief for a group of agents A.
A snapshot of the system finds our environment to be in
some state e ∈ E of environmental variable states, and each
adversary in any LAi ∈ L of possible local states. At any
given time step, the system will be in some world w of the
set of all possible worlds w ∈ W, where w = E×LA1 ×LA2 ×
...LAn , and n is the number of adversaries. For example, in
a Texas Hold"em poker game, an agent"s local state might
be its own set of cards (which is unknown to its adversary)
while the environment will consist of the betting pot and
the community cards (which are visible to both players).
A utility function under this formalization is defined as a
mapping from a possible world w ∈ W to an element in ,
which expresses the desirability of the world, from a single
agent perspective. We usually normalize the range to [0,1],
where 0 represents the least desirable possible world, and
1 is the most desirable world. The implementation of the
utility function is dependent on the domain in question.
The following list specifies new predicates, functions, 
variables, and constants used in conjunction with the original
definitions for the adversarial environment formalization:
1. φ is a null action (the agent does not do anything).
2. GAi is the set of agent Ai"s goals. Each goal is a set of
predicates whose satisfaction makes the goal complete (we
use G∗
Ai
∈ GAi to represent an arbitrary goal of agent Ai).
3. gAi is the set of agent Ai"s subgoals. Subgoals are 
predicates whose satisfaction represents an important milestone
toward achievement of the full goal. gG∗
Ai
⊆ gAi is the set of
subgoals that are important to the completion of goal G∗
Ai
(we will use g∗
G∗
Ai
∈ gG∗
Ai
to represent an arbitrary subgoal).
4. P
Aj
Ai
is the profile object agent Ai holds about agent Aj.
5. CA is a general set of actions for all agents in A which
are derived from the environment"s constraints. CAi ⊆ CA
is the set of agent Ai"s possible actions.
6. Do(Ai, α, Tα, w) holds when Ai performs action α over
time interval Tα in world w.
7. Achieve(G∗
Ai
, α, w) is true when goal G∗
Ai
is achieved 
following the completion of action α in world w ∈ W, where
α ∈ CAi .
8. Profile(Ai, PAi
Ai
) is true when agent Ai holds an object
profile for agent Aj.
Definition 1. Full conflict (FulConf ) describes a 
zerosum interaction where only a single goal of the goals in 
conflict can be completed.
FulConf(G∗
Ai
, G∗
Aj
) ⇒ (∃α ∈ CAi , ∀w, β ∈ CAj )
(Achieve(G∗
Ai
, α, w) ⇒ ¬Achieve(G∗
Aj
, β, w)) ∨
(∃β ∈ CAj , ∀w, α ∈ CAi )(Achieve(G∗
Aj
, β, w) ⇒
¬Achieve(G∗
Ai
, α, w))
Definition 2. Adversarial Knowledge (AdvKnow) is a
function returning a value which represents the amount of
The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 551
knowledge agent Ai has on the profile of agent Aj, at time
Tn. The higher the value, the more knowledge agent Ai has.
AdvKnow : P
Aj
Ai
× Tn →
Definition 3. Eval - This evaluation function returns an
estimated expected utility value for an agent in A, after
completing an action from CA in some world state w.
Eval : A × CA × w →
Definition 4. TrH - (Threshold) is a numerical constant
in the [0,1] range that represents an evaluation function
(Eval) threshold value. An action that yields an estimated
utility evaluation above the TrH is regarded as a highly 
beneficial action.
The Eval value is an estimation and not the real utility
function, which is usually unknown. Using the real utility
value for a rational agent would easily yield the best outcome
for that agent. However, agents usually do not have the real
utility functions, but rather a heuristic estimate of it.
There are two important properties that should hold for
the evaluation function:
Property 1. The evaluation function should state that the
most desirable world state is one in which the goal is achieved.
Therefore, after the goal has been satisfied, there can be no
future action that can put the agent in a world state with
higher Eval value.
(∀Ai, G∗
Ai
, α, β ∈ CAi , w ∈ W)
Achieve(G∗
Ai
, α, w) ⇒ Eval(Ai, α, w) ≥ Eval(Ai, β, w)
Property 2. The evaluation function should project an 
action that causes a completion of a goal or a subgoal to a value
which is greater than TrH (a highly beneficial action).
(∀Ai, G∗
Ai
∈ GAi , α ∈ CAi , w ∈ W, g∗
GAi
∈ gGAi
)
Achieve(G∗
Ai
, α, w) ∨ Achieve(g∗
GAi
, α, w) ⇒
Eval(Ai, α, w) ≥ TrH.
Definition 5. SetAction We define a set action 
(SetAction) as a set of action operations (either complex or basic
actions) from some action sets CAi and CAj which, 
according to agent Ai"s belief, are attached together by a temporal
and consequential relationship, forming a chain of events
(action, and its following consequent action).
(∀α1
, . . . , αu
∈ CAi , β1
, . . . , βv
∈ CAj , w ∈ W)
SetAction(α1
, . . . , αu
, β1
, . . . , βv
, w) ⇒
((Do(Ai, α1
, Tα1 , w) ⇒ Do(Aj, β1
, Tβ1 , w)) ⇒
Do(Ai, α2
, Tα2 , w) ⇒ . . . ⇒ Do(Ai, αu
, Tαu , w))
The consequential relation might exist due to various 
environmental constraints (when one action forces the 
adversary to respond with a specific action) or due to the agent"s
knowledge about the profile of its adversary.
Property 3. As the knowledge we have about our 
adversary increases we will have additional beliefs about its 
behavior in different situations which in turn creates new set
actions. Formally, if our AdvKnow at time Tn+1 is greater
than AdvKnow at time Tn, then every SetAction known at
time Tn is also known at time Tn+1.
AdvKnow(P
Aj
Ai
, Tn+1) > AdvKnow(P
Aj
Ai
, Tn) ⇒
(∀α1
, . . . , αu
∈ CAi , β1
, . . . , βv
∈ CAj )
Bel(Aag, SetAction(α1
, . . . , αu
, β1
, . . . , βv
), Tn) ⇒
Bel(Aag, SetAction(α1
, . . . , αu
, β1
, . . . , βv
), Tn+1)
2.3 The Environment Formulation
The following axioms provide the formal definition for a
simple, zero-sum Adversarial Environment (AE). 
Satisfaction of these axioms means that the agent is situated in
such an environment. It provides specifications for agent
Aag to interact with its set of adversaries A with respect to
goals G∗
Aag
and G∗
A at time TCo at some world state w.
AE(Aag, A, G∗
Aag
, A1, . . . , Ak, G∗
A1
, . . . , G∗
Ak
, Tn, w)
1. Aag has an Int.Th his goal would be completed:
(∃α ∈ CAag , Tα)
Int.Th(Aag, Achieve(G∗
Aag
, α), Tn, Tα, AE)
2. Aag believes that it and each of its adversaries Ao are
pursuing full conflicting goals:
(∀Ao ∈ {A1, . . . , Ak})
Bel(Aag, FulConf(G∗
Aag
, G∗
Ao
), Tn)
3. Aag believes that each of his adversaries in Ao has the
Int.Th his conflicting goal G∗
Aoi
will be completed:
(∀Ao ∈ {A1, . . . , Ak})(∃β ∈ CAo , Tβ)
Bel(Aag, Int.Th(Ao, Achieve(G∗
Ao
, β), TCo, Tβ, AE), Tn)
4. Aag has beliefs about the (partial) profiles of its 
adversaries
(∀Ao ∈ {A1, . . . , Ak})
(∃PAo
Aag
∈ PAag )Bel(Aag, Profile(Ao, PAo
Aag
), Tn)
To build an agent that will be able to operate successfully
within such an AE, we must specify behavioral guidelines for
its interactions. Using a naive Eval maximization strategy
to a certain search depth will not always yield satisfactory
results for several reasons: (1) the search horizon problem
when searching for a fixed depth; (2) the strong assumption
of an optimally rational, unbounded resources adversary; (3)
using an estimated evaluation function which will not give
optimal results in all world states, and can be exploited [9].
The following axioms specify the behavioral principles
that can be used to differentiate between successful and
less successful agents in the above Adversarial Environment.
Those axioms should be used as specification principles when
designing and implementing agents that should be able to
perform well in such Adversarial Environments. The 
behavioral axioms represent situations in which the agent will
adopt potential intentions to (Pot.Int.To(...)) perform an
action, which will typically require some means-end 
reasoning to select a possible course of action. This reasoning will
lead to the adoption of an Int.To(...) (see [4]).
A1. Goal Achieving Axiom. The first axiom is the 
simplest case; when the agent Aag believes that it is one action
(α) away from achieving his conflicting goal G∗
Aag
, it should
adopt the potential intention to do α and complete its goal.
(∀Aag, α ∈ CAag , Tn, Tα, w ∈ W)
(Bel(Aag, Do(Aag, α, Tα, w) ⇒ Achieve(G∗
Aag
, α, w))
⇒ Pot.Int.To(Aag, α, Tn, Tα, w)
This somewhat trivial behavior is the first and strongest
axiom. In any situation, when the agent is an action away
from completing the goal, it should complete the action.
Any fair Eval function would naturally classify α as the
maximal value action (property 1). However, without 
explicit axiomatization of such behavior there might be 
situations where the agent will decide on taking another action
for various reasons, due to its bounded decision resources.
A2. Preventive Act Axiom. Being in an adversarial 
situation, agent Aag might decide to take actions that will
damage one of its adversary"s plans to complete its goal,
even if those actions do not explicitly advance Aag towards
its conflicting goal G∗
Aag
. Such preventive action will take
place when agent Aag has a belief about the possibility of
its adversary Ao doing an action β that will give it a high
552 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)
utility evaluation value (> TrH). Believing that taking 
action α will prevent the opponent from doing its β, it will
adopt a potential intention to do α.
(∀Aag, Ao ∈ A, α ∈ CAag , β ∈ CAo , Tn, Tβ, w ∈ W)
(Bel(Aag, Do(Ao, β, Tβ, w) ∧ Eval(Ao, β, w) > TrH, Tn) ∧
Bel(Aag, Do(Aag, α, Tα, w) ⇒ ¬Do(Ao, β, Tβ, w), Tn)
⇒ Pot.Int.To(Aag, α, Tn, Tα, w)
This axiom is a basic component of any adversarial 
environment. For example, looking at a Chess board game,
a player could realize that it is about to be checkmated by
its opponent, thus making a preventive move. Another 
example is a Connect Four game: when a player has a row of
three chips, its opponent must block it, or lose.
A specific instance of A1 occurs when the adversary is one
action away from achieving its goal, and immediate 
preventive action needs to be taken by the agent. Formally, we
have the same beliefs as stated above, with a changed belief
that doing action β will cause agent Ao to achieve its goal.
Proposition 1: Prevent or lose case.
(∀Aag, Ao ∈ A, α ∈ CAag , β ∈ CAo , G∗
Ao
, Tn, Tα, Tβ, w ∈
W)
Bel(Aag, Do(Ao, β, Tβ, w) ⇒ Achieve(G∗
Ao
, β, w), Tn) ∧
Bel(Aag, Do(Aag, α, Tα, w) ⇒ ¬Do(Ao, β, Tβ, w))
⇒ Pot.Int.To(Aag, α, Tn, Tα, w)
Sketch of proof: Proposition 1 can be easily derived
from axiom A1 and the property 2 of the Eval function,
which states that any action that causes a completion of a
goal is a highly beneficial action.
The preventive act behavior will occur implicitly when
the Eval function is equal to the real world utility function.
However, being bounded rational agents and dealing with an
estimated evaluation function we need to explicitly 
axiomatize such behavior, for it will not always occur implicitly
from the evaluation function.
A3. Suboptimal Tactical Move Axiom. In many 
scenarios a situation may occur where an agent will decide not
to take the current most beneficial action it can take (the
action with the maximal utility evaluation value), because it
believes that taking another action (with lower utility 
evaluation value) might yield (depending on the adversary"s 
response) a future possibility for a highly beneficial action.
This will occur most often when the Eval function is 
inaccurate and differs by a large extent from the Utility function.
Put formally, agent Aag believes in a certain SetAction that
will evolve according to its initial action and will yield a high
beneficial value (> TrH) solely for it.
(∀Aag, Ao ∈ A, Tn, w ∈ W)
(∃α1
, . . . , αu
∈ CAi , β1
, . . . , βv
∈ CAj , Tα1 )
Bel(Aag, SetAction(α1
, . . . , αu
, β1
, . . . , βv
), Tn) ∧
Bel(Aag, Eval(Ao, βv
, w) < TrH < Eval(Aag, αu
, w), Tn)
⇒ Pot.Int.To(Aag, α1
, Tn, Tα1 , w)
An agent might believe that a chain of events will 
occur for various reasons due to the inevitable nature of the
domain. For example, in Chess, we often observe the 
following: a move causes a check position, which in turn limits the
opponent"s moves to avoiding the check, to which the first
player might react with another check, and so on. The agent
might also believe in a chain of events based on its 
knowledge of its adversary"s profile, which allows it to foresee the
adversary"s movements with high accuracy.
A4. Profile Detection Axiom. The agent can adjust
its adversary"s profiles by observations and pattern study
(specifically, if there are repeated encounters with the same
adversary). However, instead of waiting for profile 
information to be revealed, an agent can also initiate actions that
will force its adversary to react in a way that will reveal
profile knowledge about it. Formally, the axiom states that
if all actions (γ) are not highly beneficial actions (< TrH),
the agent can do action α in time Tα if it believes that it will
result in a non-highly beneficial action β from its adversary,
which in turn teaches it about the adversary"s profile, i.e.,
gives a higher AdvKnow(P
Aj
Ai
, Tβ).
(∀Aag, Ao ∈ A, α ∈ CAag , β ∈ CAo , Tn, Tα, Tβ, w ∈ W)
Bel(Aag, (∀γ ∈ CAag )Eval(Aag, γ, w) < TrH, Tn) ∧
Bel(Aag, Do(Aag, α, Tα, w) ⇒ Do(Ao, β, Tβ, w), Tn) ∧
Bel(Aag, Eval(Ao, β, w) < TrH) ∧
Bel(Aag, AdvKnow(P
Aj
Ai
, Tβ) > AdvKnow(P
Aj
Ai
, Tn), Tn) ⇒
Pot.Int.To(Aag, α, Tn, Tα, w)
For example, going back to the Chess board game 
scenario, consider starting a game versus an opponent about
whom we know nothing, not even if it is a human or a 
computerized opponent. We might start playing a strategy that
will be suitable versus an average opponent, and adjust our
game according to its level of play.
A5. Alliance Formation Axiom The following 
behavioral axiom is relevant only in a multilateral instantiation
of the adversarial environment (obviously, an alliance 
cannot be formed in a bilateral, zero-sum encounter). In 
different situations during a multilateral interaction, a group
of agents might believe that it is in their best interests to
form a temporary alliance. Such an alliance is an agreement
that constrains its members" behavior, but is believed by its
members to enable them to achieve a higher utility value
than the one achievable outside of the alliance.
As an example, we can look at the classical Risk board
game, where each player has an individual goal of being the
sole conquerer of the world, a zero-sum game. However, in
order to achieve this goal, it might be strategically wise to
make short-term ceasefire agreements with other players, or
to join forces and attack an opponent who is stronger than
the rest.
An alliance"s terms defines the way its members should
act. It is a set of predicates, denoted as Terms, that is agreed
upon by the alliance members, and should remain true for
the duration of the alliance. For example, the set Terms in
the Risk scenario, could contain the following predicates:
1. Alliance members will not attack each other on territories
X, Y and Z;
2. Alliance members will contribute C units per turn for
attacking adversary Ao;
3. Members are obligated to stay as part of the alliance until
time Tk or until adversary"s Ao army is smaller than Q.
The set Terms specifies inter-group constraints on each
of the alliance member"s (∀Aal
i ∈ Aal
⊆ A) set of actions
Cal
i ⊆ C.
Definition 6. Al val - the total evaluation value that
agent Ai will achieve while being part of Aal
is the sum of
Evali (Eval for Ai) of each of Aal
j Eval values after taking
their own α actions (via the agent(α) predicate):
Al val(Ai, Cal
, Aal
, w) = α∈Cal Evali(Aal
j , agent(α), w)
Definition 7. Al TrH - is a number representing an Al val
The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 553
threshold; above it, the alliance can be said to be a highly
beneficial alliance.
The value of Al TrH will be calculated dynamically 
according to the progress of the interaction, as can be seen in [7].
After an alliance is formed, its members are now working in
their normal adversarial environment, as well as according
to the mental states and axioms required for their 
interactions as part of the alliance. The following Alliance model
(AL) specifies the conditions under which the group Aal
can
be said to be in an alliance and working with a new and 
constrained set of actions Cal
, at time Tn.
AL(Aal
, Cal
, w, Tn)
1. Aal
has a MB that all members are part of Aal
:
MB(Aal
, (∀Aal
i ∈ Aal
)member(Aal
i , Aal
), Tn)
2. Aal
has a MB that the group be maintained:
MB(Aal
, (∀Aal
i ∈ Aal
)Int.Th
(Ai, member(Ai, Aal
), Tn, Tn+1, Co), Tn)
3. Aal
has a MB that being members gives them high utility
value:
MB(Aal
, (∀Aal
i ∈ Aal
)Al val(Aal
i , Cal
, Aal
, w) ≥ Al TrH, Tn)
Members" profiles are a crucial part of successful alliances.
We assume that agents that have more accurate profiles of
their adversaries will be more successful in such 
environments. Such agents will be able to predict when a 
member is about to breach the alliance"s contract (item 2 in the
above model), and take counter measures (when item 3 will
falsify). The robustness of the alliance is in part a function
of its members" trustfulness measure, objective position 
estimation, and other profile properties. We should note that an
agent can simultaneously be part of more than one alliance.
Such a temporary alliance, where the group members do
not have a joint goal but act collaboratively for the interest
of their own individual goals, is classified as a Treatment
Group by modern psychologists [12] (in contrast to a Task
Group, where its members have a joint goal). The Shared
Activity model as presented in [5] modeled Treatment Group
behavior using the same SharedPlans formalization.
When comparing both definitions of an alliance and a
Treatment Group we found an unsurprising resemblance 
between both models: the environment model"s definitions
are almost identical (see SA"s definitions in [5]), and their
Selfish-Act and Cooperative Act axioms conform to our 
adversarial agent"s behavior. The main distinction between
both models is the integration of a Helpful-behavior act 
axiom, in the Shared Activity which cannot be part of ours.
This axiom states that an agent will consider taking action
that will lower its Eval value (to a certain lower bound), if it
believes that a group partner will gain a significant benefit.
Such behavior cannot occur in a pure adversarial 
environment (as a zero-sum game is), where the alliance members
are constantly on watch to manipulate their alliance to their
own advantage.
A6. Evaluation Maximization Axiom. In a case when
all other axioms are inapplicable, we will proceed with the
action that maximizes the heuristic value as computed in
the Eval function.
(∀Aag, Ao ∈ A, α ∈ Cag, Tn, w ∈ W)
Bel(Aag, (∀γ ∈ Cag)Eval(Aag, α, w) ≥ Eval(Aag, γ, w), Tn)
⇒ Pot.Int.To(Aag, α, Tn, Tα, w)
T1. Optimality on Eval = Utility The above axiomatic
model handles situations where the Utility is unknown and
the agents are bounded rational agents. The following 
theorem shows that in bilateral interactions, where the agents
have the real Utility function (i.e., Eval = Utility) and are
rational agents, the axioms provide the same optimal result
as classic adversarial search (e.g., Min-Max).
Theorem 1. Let Ae
ag be an unbounded rational AE agent
using the Eval heuristic evaluation function, Au
ag be the same
agent using the true Utility function, and Ao be a sole 
unbounded utility-based rational adversary. Given that Eval =
Utility:
(∀α ∈ CAu
ag
, α ∈ CAe
ag
, Tn, w ∈ W)
Pot.Int.To(Au
ag, α, Tn, Tα, w) →
Pot.Int.To(Ae
ag, α , Tn, Tα, w) ∧
((α = α ) ∨ (Utility(Au
ag, α, w) = Eval(Ae
ag, α , w)))
Sketch of proof - Given that Au
ag has the real utility
function and unbounded resources, it can generate the full
game tree and run the optimal MinMax algorithm to choose
the highest utility value action, which we denote by, α. The
proof will show that Ae
ag, using the AE axioms, will select
the same or equal utility α (when there is more than one
action with the same max utility) when Eval = Utility.
(A1) Goal achieving axiom - suppose there is an α such
that its completion will achieve Au
ag"s goal. It will obtain
the highest utility by Min-Max for Au
ag. The Ae
ag agent will
select α or another action with the same utility value via
A1. If such α does not exist, Ae
ag cannot apply this axiom,
and proceeds to A2.
(A2) Preventive act axiom - (1) Looking at the basic case
(see Prop1), if there is a β which leads Ao to achieve its
goal, then a preventive action α will yield the highest 
utility for Au
ag. Au
ag will choose it through the utility, while
Ae
ag will choose it through A2. (2) In the general case, β
is a highly beneficial action for Ao, thus yields low utility
for Au
ag, which will guide it to select an α that will prevent
β, while Ae
ag will choose it through A2.1
If such β does not
exist for Ao, then A2 is not applicable, and Ae
ag can proceed
to A3.
(A3) Suboptimal tactical move axiom - When using a
heuristic Eval function, Ae
ag has a partial belief in the profile
of its adversary (item 4 in AE model), which may lead it
to believe in SetActions (Prop1). In our case, Ae
ag is 
holding a full profile on its optimal adversary and knows that
Ao will behave optimally according to the real utility 
values on the complete search tree, therefore, any belief about
suboptimal SetAction cannot exist, yielding this axiom 
inapplicable. Ae
ag will proceed to A4.
(A4) Profile detection axiom - Given that Ae
ag has the full
profile of Ao, none of Ae
ag"s actions can increase its 
knowledge. That axiom will not be applied, and the agent will
proceed with A6 (A5 will be disregarded because the 
interaction is bilateral).
(A6) Evaluation maximization axiom - This axiom will 
select the max Eval for Ae
ag. Given that Eval = Utility, the
same α that was selected by Au
ag will be selected.
