Our approach is to organize search results by aspects
learned from search engine logs. Given an input query, the
general procedure of our approach is:
1. Get its related information from search engine logs.
All the information forms a working set.
2. Learn aspects from the information in the working set.
These aspects correspond to users" interests given the
input query. Each aspect is labeled with a 
representative query.
3. Categorize and organize the search results of the input
query according to the aspects learned above.
We now give a detailed presentation of each step.
4.1 Finding Related Past Queries
Given a query q, a search engine will return a ranked list
of Web pages. To know what the users are really interested
in given this query, we first retrieve its past similar queries
in our preprocessed history data collection.
Formally, assume we have N pseudo-documents in our
history data set: H = {Q1, Q2, ..., QN }. Each Qi 
corresponds to a unique query and is enriched with clickthrough
information as discussed in Section 3. To find q"s related
queries in H, a natural way is to use a text retrieval 
algorithm. Here we use the OKAPI method [17], one of the
state-of-the-art retrieval methods. Specifically, we use the
following formula to calculate the similarity between query
q and pseudo-document Qi:
 
w∈q ¡ Qi
c(w, q) × IDF(w) ×
(k1 + 1) × c(w, Qi)
k1((1 − b) + b |Qi|
avdl
) + c(w, Qi)
where k1 and b are OKAPI parameters set empirically, c(w, Qi)
and c(w, q) are the count of word w in Qi and q respectively,
IDF(w) is the inverse document frequency of word w, and
avdl is the average document length in our history 
collection.
Based on the similarity scores, we rank all the documents
in H. The top ranked documents provide us a working set to
learn the aspects that users are usually interested in. Each
document in H corresponds to a past query, and thus the
top ranked documents correspond to q"s related past queries.
4.2 Learning Aspects by Clustering
Given a query q, we use Hq = {d1, ..., dn} to represent the
top ranked pseudo-documents from the history collection
H. These pseudo-documents contain the aspects that users
are interested in. In this subsection, we propose to use a
clustering method to discover these aspects.
Any clustering algorithm could be applied here. In this
paper, we use an algorithm based on graph partition: the
star clustering algorithm [2]. A good property of the star
clustering in our setting is that it can suggest a good label
for each cluster naturally. We describe the star clustering
algorithm below.
4.2.1 Star Clustering
Given Hq, star clustering starts with constructing a 
pairwise similarity graph on this collection based on the vector
space model in information retrieval [18]. Then the clusters
are formed by dense subgraphs that are star-shaped. These
clusters form a cover of the similarity graph. Formally, for
each of the n pseudo-documents {d1, ..., dn} in the collection
Hq, we compute a TF-IDF vector. Then, for each pair of
documents di and dj (i = j), their similarity is computed
as the cosine score of their corresponding vectors vi and vj ,
that is
sim(di, dj ) = cos(vi, vj) =
vi · vj
|vi| · |vj |
.
A similarity graph Gσ can then be constructed as follows
using a similarity threshold parameter σ. Each document
di is a vertex of Gσ. If sim(di, dj) > σ, there would be an
edge connecting the corresponding two vertices. After the
similarity graph Gσ is built, the star clustering algorithm
clusters the documents using a greedy algorithm as follows:
1. Associate every vertex in Gσ with a flag, initialized as
unmarked.
2. From those unmarked vertices, find the one which has
the highest degree and let it be u.
3. Mark the flag of u as center.
4. Form a cluster C containing u and all its neighbors
that are not marked as center. Mark all the selected
neighbors as satellites.
5. Repeat from step 2 until all the vertices in Gσ are
marked.
Each cluster is star-shaped, which consists a single center
and several satellites. There is only one parameter σ in
the star clustering algorithm. A big σ enforces that the
connected documents have high similarities, and thus the
clusters tend to be small. On the other hand, a small σ will
make the clusters big and less coherent. We will study the
impact of this parameter in our experiments.
A good feature of the star clustering algorithm is that it
outputs a center for each cluster. In the past query 
collection Hq, each document corresponds to a query. This center
query can be regarded as the most representative one for
the whole cluster, and thus provides a label for the cluster
naturally. All the clusters obtained are related to the input
query q from different perspectives, and they represent the
possible aspects of interests about query q of users.
4.3 Categorizing Search Results
In order to organize the search results according to users"
interests, we use the learned aspects from the related past
queries to categorize the search results. Given the top m
Web pages returned by a search engine for q: {s1, ..., sm},
we group them into different aspects using a categorization
algorithm.
In principle, any categorization algorithm can be used
here. Here we use a simple centroid-based method for 
categorization. Naturally, more sophisticated methods such as
SVM [21] may be expected to achieve even better 
performance.
Based on the pseudo-documents in each discovered aspect
Ci, we build a centroid prototype pi by taking the average
of all the vectors of the documents in Ci:
pi =
1
|Ci|
 
l∈Ci
vl.
All these pi"s are used to categorize the search results. 
Specifically, for any search result sj, we build a TF-IDF vector.
The centroid-based method computes the cosine similarity
between the vector representation of sj and each centroid
prototype pi. We then assign sj to the aspect with which it
has the highest cosine similarity score.
All the aspects are finally ranked according to the number
of search results they have. Within each aspect, the search
results are ranked according to their original search engine
ranking.
