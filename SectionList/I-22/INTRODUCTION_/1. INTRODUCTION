The entire movement of agent paradigm was spawned,
at least in part, by the perceived importance of fostering
human-like adjustable autonomy. Human-centered 
multiagent teamwork has thus attracted increasing attentions in
multi-agent systems field [2, 10, 4]. Humans and autonomous
systems (agents) are generally thought to be 
complementary: while humans are limited by their cognitive capacity in
information processing, they are superior in spatial, 
heuristic, and analogical reasoning; autonomous systems can 
continuously learn expertise and tacit problem-solving 
knowledge from humans to improve system performance. In short,
humans and agents can team together to achieve better 
performance, given that they could establish certain mutual
awareness to coordinate their mixed-initiative activities.
However, the foundation of human-agent collaboration
keeps being challenged because of nonrealistic modeling of
mutual awareness of the state of affairs. In particular, few
researchers look beyond to assess the principles of modeling
shared mental constructs between a human and his/her 
assisting agent. Moreover, human-agent relationships can go
beyond partners to teams. Many informational processing
limitations of individuals can be alleviated by having a group
perform tasks. Although groups also can create additional
costs centered on communication, resolution of conflict, and
social acceptance, it is suggested that such limitations can
be overcome if people have shared cognitive structures for 
interpreting task and social requirements [8]. Therefore, there
is a clear demand for investigations to broaden and deepen
our understanding on the principles of shared mental 
modeling among members of a mixed human-agent team.
There are lines of research on multi-agent teamwork, both
theoretically and empirically. For instance, Joint Intention
[3] and SharedPlans [5] are two theoretical frameworks for
specifying agent collaborations. One of the drawbacks is
that, although both have a deep philosophical and 
cognitive root, they do not accommodate the modeling of 
human team members. Cognitive studies suggested that teams
which have shared mental models are expected to have 
common expectations of the task and team, which allow them
to predict the behavior and resource needs of team 
members more accurately [14, 6]. Cannon-Bowers et al. [14]
explicitly argue that team members should hold compatible
models that lead to common expectations. We agree on
this and believe that the establishment of shared 
expectations among human and agent team members is a critical
step to advance human-centered teamwork research.
It has to be noted that the concept of shared expectation
can broadly include role assignment and its dynamics, 
teamwork schemas and progresses, communication patterns and
intentions, etc. While the long-term goal of our research is
to understand how shared cognitive structures can enhance
human-agent team performance, the specific objective of the
work reported here is to develop a computational cognitive
395
978-81-904262-7-5 (RPS) c 2007 IFAAMAS
capacity model to facilitate the establishment of shared 
expectations. In particular, we argue that to favor 
humanagent collaboration, an agent system should be designed to
allow the estimation and prediction of human teammates"
(relative) cognitive loads, and use that to offer improvised,
unintrusive help. Ideally, being able to predict the 
cognitive/processing capacity curves of teammates could allow
a team member to help the right party at the right time,
avoiding unbalanced work/cognitive loads among the team.
The last point is on the modeling itself. Although an
agent"s cognitive model of its human peer is not necessarily
to be descriptively accurate, having at least a realistic model
can be beneficial in offering unintrusive help, bias 
reduction, as well as trustable and self-adjustable autonomy. For
example, although humans" use of cognitive simplification
mechanisms (e.g., heuristics) does not always lead to errors
in judgment, it can lead to predictable biases in responses
[8]. It is feasible to develop agents as cognitive aids to 
alleviate humans" biases, as long as an agent can be trained
to obtain a model of a human"s cognitive inclination. With
a realistic human cognitive model, an agent can also better
adjust its automation level. When its human peer is 
becoming overloaded, an agent can take over resource-consuming
tasks, shifting the human"s limited cognitive resources to
tasks where a human"s role is indispensable. When its 
human peer is underloaded, an agent can take the chance to
observe the human"s operations to refine its cognitive model
of the human. Many studies have documented that human
choices and behaviors do not agree with predictions from
rational models. If agents could make recommendations in
ways that humans appreciate, it would be easier to establish
trust relationships between agents and humans; this in turn,
will encourage humans" automation uses.
The rest of the paper is organized as follows. In Section
2 we review cognitive load theories and measurements. A
HMM-based cognitive load model is given in Section 3 to
support resource-bounded teamwork among 
human-agentpairs. Section 4 describes the key concept shared belief
map as implemented in SMMall, and Section 5 reports the
experiments for evaluating the cognitive models and their
impacts on the evolving of shared mental models.
