As we noted earlier, real web search user behavior can be
noisy in the sense that user behaviors are only probabilistically
related to explicit relevance judgments and preferences. Hence,
instead of treating each user as a reliable expert, we aggregate
information from many unreliable user search session traces. Our
main approach is to model user web search behavior as if it were
generated by two components: a relevance component - 
queryspecific behavior influenced by the apparent result relevance, and
a background component - users clicking indiscriminately.
Our general idea is to model the deviations from the expected
user behavior. Hence, in addition to basic features, which we
will describe in detail in Section 3.2, we compute derived
features that measure the deviation of the observed feature value
for a given search result from the expected values for a result,
with no query-dependent information. We motivate our
intuitions with a particularly important behavior feature, result
clickthrough, analyzed next, and then introduce our general
model of user behavior that incorporates other user actions
(Section 3.2).
3.1 A Case Study in Click Distributions
As we discussed, we aggregate statistics across many user
sessions. A click on a result may mean that some user found the
result summary promising; it could also be caused by people
clicking indiscriminately. In general, individual user behavior,
clickthrough and otherwise, is noisy, and cannot be relied upon
for accurate relevance judgments. The data set is described in
more detail in Section 5.2. For the present it suffices to note that
we focus on a random sample of 3,500 queries that were
randomly sampled from query logs. For these queries we
aggregate click data over more than 120,000 searches performed
over a three week period. We also have explicit relevance
judgments for the top 10 results for each query.
Figure 3.1 shows the relative clickthrough frequency as a
function of result position. The aggregated click frequency at
result position p is calculated by first computing the frequency of
a click at p for each query (i.e., approximating the probability
that a randomly chosen click for that query would land on
position p). These frequencies are then averaged across queries
and normalized so that relative frequency of a click at the top
position is 1. The resulting distribution agrees with previous
observations that users click more often on top-ranked results.
This reflects the fact that search engines do a reasonable job of
ranking results as well as biases to click top results and 
noisewe attempt to separate these components in the analysis that
follows.
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
1 3 5 7 9 11 13 15 17 19 21 23 25 27 29
result position
RelativeClickFrequency
Figure 3.1: Relative click frequency for top 30 result
positions over 3,500 queries and 120,000 searches.
First we consider the distribution of clicks for the relevant
documents for these queries. Figure 3.2 reports the aggregated
click distribution for queries with varying Position of Top
Relevant document (PTR). While there are many clicks above
the first relevant document for each distribution, there are
clearly peaks in click frequency for the first relevant result.
For example, for queries with top relevant result in position 2,
the relative click frequency at that position (second bar) is higher
than the click frequency at other positions for these queries.
Nevertheless, many users still click on the non-relevant results
in position 1 for such queries. This shows a stronger property of
the bias in the click distribution towards top results - users click
more often on results that are ranked higher, even when they are
not relevant.
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
1 2 3 5 10
result position
relativeclickfrequency
PTR=1
PTR=2
PTR=3
PTR=5
PTR=10
Background
Figure 3.2: Relative click frequency for queries with varying
PTR (Position of Top Relevant document).
-0.06
-0.04
-0.02
0
0.02
0.04
0.06
0.08
0.1
0.12
0.14
0.16
1 2 3 5 10
result position
correctedrelativeclickfrequency
PTR=1
PTR=2
PTR=3
PTR=5
PTR=10
Figure 3.3: Relative corrected click frequency for relevant
documents with varying PTR (Position of Top Relevant).
If we subtract the background distribution of Figure 3.1 from the
mixed distribution of Figure 3.2, we obtain the distribution in
Figure 3.3, where the remaining click frequency distribution can
be interpreted as the relevance component of the results. Note that
the corrected click distribution correlates closely with actual
result relevance as explicitly rated by human judges.
3.2 Robust User Behavior Model
Clicks on search results comprise only a small fraction of the
post-search activities typically performed by users. We now
introduce our techniques for going beyond the clickthrough
statistics and explicitly modeling post-search user behavior.
Although clickthrough distributions are heavily biased towards
top results, we have just shown how the â€˜relevance-driven" click
distribution can be recovered by correcting for the prior,
background distribution. We conjecture that other aspects of user
behavior (e.g., page dwell time) are similarly distorted. Our
general model includes two feature types for describing user
behavior: direct and deviational where the former is the directly
measured values, and latter is deviation from the expected values
estimated from the overall (query-independent) distributions for
the corresponding directly observed features.
More formally, we postulate that the observed value o of a
feature f for a query q and result r can be expressed as a mixture
of two components:
),,()(),,( frqrelfCfrqo += (1)
where )( fC is the prior background distribution for values of f
aggregated across all queries, and rel(q,r,f) is the component of
the behavior influenced by the relevance of the result r. As
illustrated above with the clickthrough feature, if we subtract the
background distribution (i.e., the expected clickthrough for a
result at a given position) from the observed clickthrough
frequency at a given position, we can approximate the relevance
component of the clickthrough value1
. In order to reduce the
effect of individual user variations in behavior, we average
observed feature values across all users and search sessions for
each query-URL pair. This aggregation gives additional
robustness of not relying on individual noisy user interactions.
In summary, the user behavior for a query-URL pair is
represented by a feature vector that includes both the directly
observed features and the derived, corrected feature values.
We now describe the actual features we use to represent user
behavior.
3.3 Features for Representing User Behavior
Our goal is to devise a sufficiently rich set of features that
allow us to characterize when a user will be satisfied with a web
search result. Once the user has submitted a query, they perform
many different actions (reading snippets, clicking results,
navigating, refining their query) which we capture and
summarize. This information was obtained via opt-in client-side
instrumentation from users of a major web search engine.
This rich representation of user behavior is similar in many
respects to the recent work by Fox et al. [7]. An important
difference is that many of our features are (by design) query
specific whereas theirs was (by design) a general, 
queryindependent model of user behavior. Furthermore, we include
derived, distributional features computed as described above.
The features we use to represent user search interactions are
summarized in Table 3.1. For clarity, we organize the features
into the groups Query-text, Clickthrough, and Browsing.
Query-text features: Users decide which results to examine in
more detail by looking at the result title, URL, and summary - in
some cases, looking at the original document is not even
necessary. To model this aspect of user experience we defined
features to characterize the nature of the query and its relation to
the snippet text. These include features such as overlap between
the words in title and in query (TitleOverlap), the fraction of
words shared by the query and the result summary
(SummaryOverlap), etc.
Browsing features: Simple aspects of the user web page
interactions can be captured and quantified. These features are
used to characterize interactions with pages beyond the results
page. For example, we compute how long users dwell on a page
(TimeOnPage) or domain (TimeOnDomain), and the deviation
of dwell time from expected page dwell time for a query. These
features allows us to model intra-query diversity of page
browsing behavior (e.g., navigational queries, on average, are
likely to have shorter page dwell time than transactional or
informational queries). We include both the direct features and
the derived features described above.
Clickthrough features: Clicks are a special case of user
interaction with the search engine. We include all the features
necessary to learn the clickthrough-based strategies described
in Sections 4.1 and 4.4. For example, for a query-URL pair we
provide the number of clicks for the result (ClickFrequency), as
1
Of course, this is just a rough estimate, as the observed
background distribution also includes the relevance
component.
well as whether there was a click on result below or above the
current URL (IsClickBelow, IsClickAbove). The derived feature
values such as ClickRelativeFrequency and ClickDeviation are
computed as described in Equation 1.
Query-text features
TitleOverlap Fraction of shared words between query and title
SummaryOverlap Fraction of shared words between query and summary
QueryURLOverlap Fraction of shared words between query and URL
QueryDomainOverlap Fraction of shared words between query and domain
QueryLength Number of tokens in query
QueryNextOverlap Average fraction of words shared with next query
Browsing features
TimeOnPage Page dwell time
CumulativeTimeOnPage Cumulative time for all subsequent pages after search
TimeOnDomain Cumulative dwell time for this domain
TimeOnShortUrl Cumulative time on URL prefix, dropping parameters
IsFollowedLink 1 if followed link to result, 0 otherwise
IsExactUrlMatch 0 if aggressive normalization used, 1 otherwise
IsRedirected 1 if initial URL same as final URL, 0 otherwise
IsPathFromSearch 1 if only followed links after query, 0 otherwise
ClicksFromSearch Number of hops to reach page from query
AverageDwellTime Average time on page for this query
DwellTimeDeviation Deviation from overall average dwell time on page
CumulativeDeviation Deviation from average cumulative time on page
DomainDeviation Deviation from average time on domain
ShortURLDeviation Deviation from average time on short URL
Clickthrough features
Position Position of the URL in Current ranking
ClickFrequency Number of clicks for this query, URL pair
ClickRelativeFrequency Relative frequency of a click for this query and URL
ClickDeviation Deviation from expected click frequency
IsNextClicked 1 if there is a click on next position, 0 otherwise
IsPreviousClicked 1 if there is a click on previous position, 0 otherwise
IsClickAbove 1 if there is a click above, 0 otherwise
IsClickBelow 1 if there is click below, 0 otherwise
Table 3.1: Features used to represent post-search interactions
for a given query and search result URL
3.4 Learning a Predictive Behavior Model
Having described our features, we now turn to the actual
method of mapping the features to user preferences. We attempt
to learn a general implicit feedback interpretation strategy
automatically instead of relying on heuristics or insights. We
consider this approach to be preferable to heuristic strategies,
because we can always mine more data instead of relying (only)
on our intuition and limited laboratory evidence. Our general
approach is to train a classifier to induce weights for the user
behavior features, and consequently derive a predictive model of
user preferences. The training is done by comparing a wide range
of implicit behavior measures with explicit user judgments for a
set of queries.
For this, we use a large random sample of queries in the search
query log of a popular web search engine, the sets of results
(identified by URLs) returned for each of the queries, and any
explicit relevance judgments available for each query/result pair.
We can then analyze the user behavior for all the instances where
these queries were submitted to the search engine.
To learn the mapping from features to relevance preferences,
we use a scalable implementation of neural networks, RankNet
[4], capable of learning to rank a set of given items. More
specifically, for each judged query we check if a result link has
been judged. If so, the label is assigned to the query/URL pair and
to the corresponding feature vector for that search result. These
vectors of feature values corresponding to URLs judged relevant
or non-relevant by human annotators become our training set.
RankNet has demonstrated excellent performance in learning to
rank objects in a supervised setting, hence we use RankNet for
our experiments.
