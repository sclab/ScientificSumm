In information retrieval, we often assume that the 
representations of documents exist in some high-dimensional 
vector space. For example, given a vocabulary, V, this vector
space may be an arbitrary |V|-dimensional space with cosine
inner-product or a multinomial simplex with a 
distributionbased distance measure. An embedding space is often 
selected to respect topical proximity; if two documents are
near, they are more likely to share a topic.
Because of the prevalence and success of spatial models
of information retrieval, we believe that the application of
spatial data analysis techniques are appropriate. Whereas
in information retrieval, we are concerned with the score at
a point in a space, in spatial data analysis, we are concerned
with the value of a function at a point or location in a space.
We use the term function here to mean a mapping from a
location to a real value. For example, we might be interested
in the prevalence of a disease in the neighborhood of some
city. The function would map the location of a neighborhood
to an infection rate.
If we want to quantify the spatial dependencies of a 
function, we would employ a measure referred to as the spatial
autocorrelation [5, 10]. High spatial autocorrelation suggests
that knowing the value of a function at location a will tell
us a great deal about the value at a neighboring location
b. There is a high spatial autocorrelation for a function
representing the temperature of a location since knowing
the temperature at a location a will tell us a lot about the
temperature at a neighboring location b. Low spatial 
autocorrelation suggests that knowing the value of a function
at location a tells us little about the value at a neighboring
location b. There is low spatial autocorrelation in a function
measuring the outcome of a coin toss at a and b.
In this section, we will begin by describing what we mean
by spatial proximity for documents and then define a 
measure of spatial autocorrelation. We conclude by extending
this model to include information from multiple retrievals
from multiple systems for a single query.
3.1 Spatial Representation of Documents
Our work does not focus on improving a specific similarity
measure or defining a novel vector space. Instead, we choose
an inner product known to be effective at detecting 
interdocument topical relationships. Specifically, we adopt tf.idf
document vectors,
˜di = di log
„
(n + 0.5) − ci
0.5 + ci
«
(1)
where d is a vector of term frequencies, c is the length-|V|
document frequency vector. We use this weighting scheme
due to its success for topical link detection in the context
of Topic Detection and Tracking (TDT) evaluations [6]. 
Assuming vectors are scaled by their L2 norm, we use the inner
product, ˜di, ˜dj , to define similarity.
Given documents and some similarity measure, we can
construct a matrix which encodes the similarity between
pairs of documents. Recall that we are given the top n
documents retrieved in y. We can compute an n × n 
similarity matrix, W. An element of this matrix, Wij represents
the similarity between documents ranked i and j. In 
practice, we only include the affinities for a document"s k-nearest
neighbors. In all of our experiments, we have fixed k to 5.
We leave exploration of parameter sensitivity to future work.
We also row normalize the matrix so that
Pn
j=1 Wij = 1 for
all i.
3.2 Spatial Autocorrelation of a Retrieval
Recall that we are interested in measuring the similarity
between the scores of spatially-close documents. One such
suitable measure is the Moran coefficient of spatial 
autocorrelation. Assuming the function y over n locations, this is
defined as
˜IM =
n
eTWe
P
i,j Wijyiyj
P
i y2
i
=
n
eTWe
yT
Wy
yTy
(2)
where eT
We =
P
ij Wij.
We would like to compare autocorrelation values for 
different retrievals. Unfortunately, the bound for Equation 2
is not consistent for different W and y. Therefore, we use
the Cauchy-Schwartz inequality to establish a bound,
˜IM ≤
n
eTWe
s
yTWTWy
yTy
And we define the normalized spatial autocorrelation as
IM =
yT
Wy
p
yTy × yTWTWy
Notice that if we let ˜y = Wy, then we can write this formula
as,
IM =
yT
˜y
y 2 ˜y 2
(3)
which can be interpreted as the correlation between the 
original retrieval scores and a set of retrieval scores diffused
in the space.
We present some examples of autocorrelations of functions
on a grid in Figure 1.
3.3 Correlation with Other Retrievals
Sometimes we are interested in the performance of a single
retrieval but have access to scores from multiple systems for
(a) IM = 0.006 (b) IM = 0.241 (c) IM = 0.487
Figure 1: The Moran coefficient, IM for a several
binary functions on a grid. The Moran coefficient
is a local measure of function consistency. From the
perspective of information retrieval, each of these
grid spaces would represent a document and 
documents would be organized so that they lay next to
topically-related documents. Binary retrieval scores
would define a pattern on this grid. Notice that,
as the Moran coefficient increases, neighboring cells
tend to have similar values.
the same query. In this situation, we can use combined
information from these scores to construct a surrogate for
a high-quality ranking [17]. We can treat the correlation
between the retrieval we are interested in and the combined
scores as a predictor of performance.
Assume that we are given m score functions, yi, for the
same n documents. We will represent the mean of these
vectors as yµ =
Pm
i=1 yi. We use the mean vector as an 
approximation to relevance. Since we use zero mean and unit
variance normalization, work in metasearch suggests that
this assumption is justified [15]. Because yµ represents a
very good retrieval, we hypothesize that a strong similarity
between yµ and y will correlate positively with system 
performance. We use Pearson"s product-moment correlation to
measure the similarity between these vectors,
ρ(y, yµ) =
yT
yµ
y 2 yµ 2
(4)
We will comment on the similarity between Equation 3 and
4 in Section 7.
Of course, we can combine ρ(y, ˜y) and ρ(y, yµ) if we 
assume that they capture different factors in the prediction.
One way to accomplish this is to combine these predictors
as independent variables in a linear regression. An 
alternative means of combination is suggested by the mathematical
form of our predictors. Since ˜y encodes the spatial 
dependencies in y and yµ encodes the spatial properties of the
multiple runs, we can compute a third correlation between
these two vectors,
ρ(˜y, yµ) =
˜yT
yµ
˜y 2 yµ 2
(5)
We can interpret Equation 5 as measuring the correlation
between a high quality ranking (yµ) and a spatially smoothed
version of the retrieval (˜y).
