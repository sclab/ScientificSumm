Our experiments focus on testing the predictive power of
each of our predictors: ρ(y, ˜y), ρ(y, yµ), and ρ(˜y, yµ). As
stated in Section 2, we are interested in predicting the 
performance of the retrieval generated by an arbitrary system.
Our methodology is consistent with previous research in that
we predict the relative performance of a retrieval by 
comparing a ranking based on our predictor to a ranking based on
average precision.
We present results for two sets of experiments. The first
set of experiments presents detailed comparisons of our 
predictors to previously-proposed predictors using identical data
sets. Our second set of experiments demonstrates the 
generalizability of our approach to arbitrary retrieval methods,
corpus types, and corpus languages.
5.1 Detailed Experiments
In these experiments, we will predict the performance of
language modeling scores using our autocorrelation 
predictor, ρ(y, ˜y); we do not consider ρ(y, yµ) or ρ(˜y, yµ) 
because, in these detailed experiments, we focus on ranking
the retrievals from a single system. We use retrievals, values
for baseline predictors, and evaluation measures reported in
previous work [19].
5.1.1 Topics and Collections
These performance prediction experiments use language
model retrievals performed for queries associated with 
collections in the TREC corpora. Using TREC collections 
allows us to confidently associate an average precision with a
retrieval. In these experiments, we use the following topic
collections: TREC 4 ad-hoc, TREC 5 ad-hoc, Robust 2004,
Terabyte 2004, and Terabyte 2005.
5.1.2 Baselines
We provide two baselines. Our first baseline is the 
classic Clarity predictor presented in Equation 6. Clarity is
designed to be used with language modeling systems. Our
second baseline is Zhou and Croft"s ranking robustness
predictor. This predictor corrupts the top k documents
from retrieval and re-computes the language model scores
for these corrupted documents. The value of the predictor
is the Spearman rank correlation between the original 
ranking and the corrupted ranking. In our tables, we will label
results for Clarity using DV
KL and the ranking robustness
predictor using P.
5.2 Generalizability Experiments
Our predictors do not require a particular baseline 
retrieval system; the predictors can be computed for an 
arbitrary retrieval, regardless of how scores were generated. We
believe that that is one of the most attractive aspects of our
algorithm. Therefore, in a second set of experiments, we
demonstrate the ability of our techniques to generalize to a
variety of collections, topics, and retrieval systems.
5.2.1 Topics and Collections
We gathered a diverse set of collections from all possible
TREC corpora. We cast a wide net in order to locate 
collections where our predictors might fail. Our hypothesis is that
documents with high topical similarity should have 
correlated scores. Therefore, we avoided collections where scores
were unlikely to be correlated (eg, question-answering) or
were likely to be negatively correlated (eg, novelty). 
Nevertheless, our collections include corpora where correlations
are weakly justified (eg, non-English corpora) or not 
justified at all (eg, expert search). We use the ad-hoc tracks from
TREC3-8, TREC Robust 2003-2005, TREC Terabyte 
20042005, TREC4-5 Spanish, TREC5-6 Chinese, and TREC 
Enterprise Expert Search 2005. In all cases, we use only the
automatic runs for ad-hoc tracks submitted to NIST.
For all English and Spanish corpora, we construct the 
matrix W according to the process described in Section 3.1. For
Chinese corpora, we use na¨ıve character-based tf.idf vectors.
For entities, entries in W are proportional to the number of
documents in which two entities cooccur.
5.2.2 Baselines
In our detailed experiments, we used the Clarity measure
as a baseline. Since we are predicting the performance of
retrievals which are not based on language modeling, we
use a version of Clarity referred to as ranked-list Clarity
[7]. Ranked-list clarity converts document ranks to P(Q|θi)
values. This conversion begins by replacing all of the scores
in y with the respective ranks. Our estimation of P(Q|θi)
from the ranks, then is,
P(Q|θi) =
(
2(c+1−yi)
c(c+1)
if yi ≤ c
0 otherwise
(7)
where c is a cutoff parameter. As suggested by the authors,
we fix the algorithm parameters c and λ2 so that c = 60
and λ2 = 0.10. We use Equation 6 to estimate P(w|θQ) and
DV
KL(θQ θC ) to compute the value of the predictor. We
will refer to this predictor as DV
KL, superscripted by V to
indicate that the Kullback-Leibler divergence is with respect
to the term embedding space.
When information from multiple runs on the same query is
available, we use Aslam and Pavlu"s document-space 
multinomial divergence as a baseline [1]. This rank-based method
first normalizes the scores in a retrieval as an n-dimensional
multinomial. As with ranked-list Clarity, we begin by 
replacing all of the scores in y with their respective ranks.
Then, we adjust the elements of y in the following way,
ˆyi =
1
2n
0
@1 +
nX
k=yi
1
k
1
A (8)
In our multirun experiments, we only use the top 75 
documents from each retrieval (n = 75); this is within the range
of parameter values suggested by the authors. However, we
admit not tuning this parameter for either our system or the
baseline. The predictor is the divergence between the 
candidate distribution, y, and the mean distribution, yµ . With
the uniform linear combination of these m retrievals 
represented as yµ, we can compute the divergence as Dn
KL(ˆy ˆyµ)
where we use the superscript n to indicate that the 
summation is over the set of n documents. This baseline was
developed in the context of predicting query difficulty but
we adopt it as a reasonable baseline for predicting retrieval
performance.
5.2.3 Parameter Settings
When given multiple retrievals, we use documents in the
union of the top k = 75 documents from each of the m 
retrievals for that query. If the size of this union is ˜n, then
yµ and each yi is of length ˜n. In some cases, a system
did not score a document in the union. Since we are 
making a Gaussian assumption about our scores, we can sample
scores for these unseen documents from the negative tail
of the distribution. Specifically, we sample from the part
of the distribution lower than the minimum value of in the
normalized retrieval. This introduces randomness into our
algorithm but we believe it is more appropriate than 
assigning an arbitrary fixed value.
We optimized the linear regression using the square root
of each predictor. We found that this substantially improved
fits for all predictors, including the baselines. We considered
linear combinations of pairs of predictors (labeled by the
components) and all predictors (labeled as β).
5.3 Evaluation
Given a set of retrievals, potentially from a combination
of queries and systems, we measure the correlation of the
rank ordering of this set by the predictor and by the 
performance metric. In order to ensure comparability with 
previous results, we present Kendall"s τ correlation between the
predictor"s ranking and ranking based on average precision
of the retrieval. Unless explicitly noted, all correlations are
significant with p < 0.05.
Predictors can sometimes perform better when linearly
combined [9, 11]. Although previous work has presented
the coefficient of determination (R2
) to measure the quality
of the regression, this measure cannot be reliably used when
comparing slight improvements from combining predictors.
Therefore, we adopt the adjusted coefficient of 
determination which penalizes models with more variables. The 
adjusted R2
allows us to evaluate the improvement in 
prediction achieved by adding a parameter but loses the statistical
interpretation of R2
. We will use Kendall"s τ to evaluate the
magnitude of the correlation and the adjusted R2
to 
evaluate the combination of variables.
