The most important result from our experiments involves
prediction when no information is available from multiple
runs (Tables 1 and 2a). This situation arises often in system
design. For example, a system may need to, at retrieval
time, assess its performance before deciding to conduct more
intensive processing such as pseudo-relevance feedback or
interaction. Assuming the presence of multiple retrievals is
unrealistic in this case.
We believe that autocorrelation is, like multiple-retrieval
algorithms, approximating a good ranking; in this case by
diffusing scores. Why is Ëœy a reasonable surrogate? We know
that diffusion of scores on the web graph and language model
graphs improves performance [14, 16]. Therefore, if score
diffusion tends to, in general, improve performance, then
diffused scores will, in general, provide a good surrogate for
relevance. Our results demonstrate that this approximation
is not as powerful as information from multiple retrievals.
Nevertheless, in situations where this information is lacking,
autocorrelation provides substantial information.
The success of autocorrelation as a predictor may also
have roots in the clustering hypothesis. Recall that we
regard autocorrelation as the degree to which a retrieval
satisfies the clustering hypothesis. Our experiments, then,
demonstrate that a failure to respect the clustering 
hypothesis correlates with poor performance. Why might systems
fail to conform to the cluster hypothesis? Query-based 
information retrieval systems often score documents 
independently. The score of document a may be computed by 
examining query term or phrase matches, the document length,
and perhaps global collection statistics. Once computed,
a system rarely compares the score of a to the score of a
topically-related document b. With some exceptions, the
correlation of document scores has largely been ignored.
We should make it clear that we have selected tasks where
topical autocorrelation is appropriate. There are certainly
cases where there is no reason to believe that retrieval scores
will have topical autocorrelation. For example, ranked lists
which incorporate document novelty should not exhibit 
spatial autocorrelation; if anything autocorrelation should be
negative for this task. Similarly, answer candidates in a
question-answering task may or may not exhibit 
autocorrelation; in this case, the semantics of links is questionable
too. It is important before applying this measure to confirm
that, given the semantics for some link between two retrieved
items, we should expect a correlation between scores.
