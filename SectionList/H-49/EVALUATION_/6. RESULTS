We present results for our detailed experiments comparing
the prediction of language model scores in Table 1. Although
the Clarity measure is theoretically designed for language
model scores, it consistently underperforms our system-agnostic
predictor. Ranking robustness was presented as an 
improvement to Clarity for web collections (represented in our 
experiments by the terabyte04 and terabyte05 collections), 
shifting the τ correlation from 0.139 to 0.150 for terabyte04 and
0.171 to 0.208 for terabyte05. However, these improvements
are slight compared to the performance of autocorrelation
on these collections. Our predictor achieves a τ correlation
of 0.454 for terabyte04 and 0.383 for terabyte05. Though
not always the strongest, autocorrelation achieves 
correlations competitive with baseline predictors. When 
examining the performance of linear combinations of predictors, we
note that in every case, autocorrelation factors as a 
necessary component of a strong predictor. We also note that the
adjusted R2
for individual baselines are always significantly
improved by incorporating autocorrelation.
We present our generalizability results in Table 2. We
begin by examining the situation in column (a) where we
are presented with a single retrieval and no information
from additional retrievals. For every collection except one,
we achieve significantly better correlations than ranked-list
Clarity. Surprisingly, we achieve relatively strong 
correlations for Spanish and Chinese collections despite our na¨ıve
processing. We do not have a ranked-list clarity correlation
for ent05 because entity modeling is itself an open research
question. However, our autocorrelation measure does not
achieve high correlations perhaps because relevance for 
entity retrieval does not propagate according to the 
cooccurrence links we use.
As noted above, the poor Clarity performance on web
data is consistent with our findings in the detailed 
experiments. Clarity also notably underperforms for several news
corpora (trec5, trec7, and robust04). On the other hand, 
autocorrelation seems robust to the changes between different
corpora.
Next, we turn to the introduction of information from
multiple retrievals. We compare the correlations between
those predictors which do not use this information in column
(a) and those which do in column (b). For every collection,
the predictors in column (b) outperform the predictors in
column (a), indicating that the information from additional
runs can be critical to making good predictions.
Inspecting the predictors in column (b), we only draw
weak conclusions. Our new predictors tend to perform 
better on news corpora. And between our new predictors, the
hybrid ρ(˜y, yµ) predictor tends to perform better. Recall
that our ρ(˜y, yµ) measure incorporates both spatial and
multiple retrieval information. Therefore, we believe that
the improvement in correlation is the result of 
incorporating information from spatial behavior.
In column (c), we can investigate the utility of 
incorporating spatial information with information from 
multiple retrievals. Notice that in the cases where 
autocorrelation, ρ(y, ˜y), alone performs well (trec3, trec5-spanish, and
trec6-chinese), it is substantially improved by 
incorporating multiple-retrieval information from ρ(y, yµ) in the 
linear regression, β. In the cases where ρ(y, yµ) performs well,
incorporating autocorrelation rarely results in a significant
improvement in performance. In fact, in every case where
our predictor outperforms the baseline, it includes 
information from multiple runs.
