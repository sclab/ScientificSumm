We have presented a new method for predicting the 
performance of a retrieval ranking without any relevance 
judgments. We consider two cases. First, when making 
predictions in the absence of retrievals from other systems, our
predictors demonstrate robust, strong correlations with 
average precision. This performance, combined with a simple
implementation, makes our predictors, in particular, very 
attractive. We have demonstrated this improvement for many,
diverse settings. To our knowledge, this is the first large
scale examination of zero-judgment, single-retrieval 
performance prediction. Second, when provided retrievals from
other systems, our extended methods demonstrate 
competitive performance with state of the art baselines. Our 
experiments also demonstrate the limits of the usefulness of our
predictors when information from multiple runs is provided.
Our results suggest two conclusions. First, our results
could affect retrieval algorithm design. Retrieval algorithms
designed to consider spatial autocorrelation will conform to
the cluster hypothesis and improve performance. Second,
our results could affect the design of minimal test collection
algorithms. Much of the recent work in ranking systems
sometimes ignores correlations between document labels and
scores. We believe that these two directions could be 
rewarding given the theoretical and experimental evidence in this
paper.
