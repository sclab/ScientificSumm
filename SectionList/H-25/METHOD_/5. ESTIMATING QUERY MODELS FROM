TERM FEEDBACK
In this section, we present several algorithms for exploiting term
feedback. The algorithms take as input the original query q, the
clusters {θi} as generated by the theme discovery algorithm, the set
of feedback terms T and their relevance judgment R, and outputs
an updated query language model θq that makes best use of the
feedback evidence to capture the user"s information need.
First we describe our notations:
• θq: The original query model, derived from query terms only:
p(w|θq) =
c(w; q)
|q|
where c(w; q) is the count of w in q, and |q| = w∈q c(w; q)
is the query length.
• θq : The updated query model which we need to estimate
from term feedback.
• θi (i = 1, 2, . . . K): The unigram language model of cluster
Ci, as estimated using the theme discovery algorithm.
• T = {ti,j} (i = 1 . . . K, j = 1 . . . L): The set of terms 
presented to the user for judgment. ti,j is the j-th term chosen
from cluster Ci.
• R = {δw|w ∈ T}: δw is an indicator variable that is 1 if w
is judged relevant or 0 otherwise.
5.1 TFB (Direct Term Feedback)
This is a straight-forward form of term feedback that does not
involve any secondary structure. We give a weight of 1 to terms
judged relevant by the user, a weight of μ to query terms, zero
weight to other terms, and then apply normalization:
p(w|θq ) =
δw + μ c(w; q)
w ∈T δw + μ|q|
where w ∈T δw is the total number of terms that are judged 
relevant. We call this method TFB (direct Term FeedBack).
If we let μ = 1, this approach is equivalent to appending the
relevant terms after the original query, which is what standard query
expansion (without term reweighting) does. If we set μ > 1, we are
putting more emphasis on the query terms than the checked ones.
Note that the result model will be more biased toward θq if the
original query is long or the user feedback is weak, which makes
sense, as we can trust more on the original query in either case.
Figure 1: Filled clarification form for Topic 363
363 transportation tunnel disasters
Please select all terms that are relevant to the topic.
traffic railway
harbor rail
bridge kilometer
construct swiss
cross link
kong hong
river project
meter shanghai
fire truck
french smoke
car italian
firefights blaze
blanc mont
victim franc
rescue driver
chamonix emerge
toll amtrak
train airport
turnpike lui
jersey pass
rome z
center electron
road boston
speed bu
submit
5.2 CFB (Cluster Feedback)
Here we exploit the cluster structure that played an important
role when we selected the presentation terms. The clusters 
represent different aspects of the query topic, each of which may or
may not be relevant. If we are able to identify the relevant clusters,
we can combine them to generate a query model that is good at
discovering documents belonging to these clusters (instead of the
irrelevant ones). We could ask the user to directly judge the 
relevance of a cluster after viewing representative terms in that cluster,
but this would sometimes be a difficult task for the user, who has to
guess the semantics of a cluster via its set of terms, which may not
be well connected to one another due to a lack of context. 
Therefore, we propose to learn cluster feedback indirectly, inferring the
relevance of a cluster through the relevance of its feedback terms.
Because each cluster has an equal number of terms presented to
the user, the simplest measure of a cluster"s relevance is the number
of terms that are judged relevant in it. Intuitively, the more terms
are marked relevant in a cluster, the closer the cluster is to the query
topic, and the more the cluster should participate in query 
modification. If we combine the cluster models using weights determined
this way and then interpolate with the original query model, we
get the following formula for query updating, which we call CFB
(Cluster FeedBack):
p(w|θq ) = λp(w|θq) + (1 − λ)
K
i=1
L
j=1 δti,j
K
k=1
L
j=1 δtk,j
p(w|θi)
where L
j=1 δti,j is the number of relevant terms in cluster Ci, and
K
k=1
L
j=1 δtk,j is the total number of relevant terms.
We note that when there is only one cluster (K = 1), the above
formula degenerates to
p(w|θq ) = λp(w|θq) + (1 − λ)p(w|θ1)
which is merely pseudo-feedback of the form proposed in [25].
5.3 TCFB (Term-cluster Feedback)
TFB and CFB both have their drawbacks. TFB assigns non-zero
probabilities to the presented terms that are marked relevant, but
completely ignores (a lot more) others, which may be left unchecked
due to the user"s ignorance, or simply not included in the 
presentation list, but we should be able to infer their relevance from the
checked ones. For example, in Figure 1, since as many as 5 terms
in the middle cluster (the third and fourth columns) are checked,
we should have high confidence in the relevance of other terms in
that cluster. CFB remedies TFB"s problem by treating the terms
in a cluster collectively, so that unchecked/unpresented terms 
receive weights when presented terms in their clusters are judged as
relevant, but it does not distinguish which terms in a cluster are
presented or judged. Intuitively, the judged relevant terms should
receive larger weights because they are explicitly indicated as 
relevant by the user. Therefore, we try to combine the two methods,
hoping to get the best out of both.
We do this by interpolating the TFB model with the CFB model,
and call it TCFB:
p(w|θq ) = αp(w|θqT F B
) + (1 − α)p(w|θqCF B
)
