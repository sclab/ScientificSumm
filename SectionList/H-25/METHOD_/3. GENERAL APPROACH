We follow the language modeling approach, and base our method
on the KL-divergence retrieval model proposed in [25]. With this
model, the retrieval task involves estimating a query language model
θq from a given query, a document language model θd from each
document, and calculating their KL-divergence D(θq||θd), which
is then used to score the documents. [25] treats relevance feedback
as a query model re-estimation problem, i.e., computing an updated
query model θq given the original query text and the extra evidence
carried by the judged relevant documents. We adopt this view, and
cast our task as updating the query model from user term feedback.
There are two key subtasks here: First, how to choose the best terms
to present to the user for judgment, in order to gather maximal 
evidence about the user"s information need. Second, how to compute
an updated query model based on this term feedback evidence, so
that it captures the user"s information need and translates into good
retrieval performance.
