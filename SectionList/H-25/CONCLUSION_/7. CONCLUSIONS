In this paper we studied the use of term feedback for 
interactive information retrieval in the language modeling approach. We
proposed a cluster-based method for selecting presentation terms
as well as algorithms to estimate refined query models from user
term feedback. We saw significant improvement in retrieval 
accuracy brought by term feedback, in spite of the fact that a user often
makes mistakes in relevance judgment that hurts its performance.
We found the best-performing algorithm to be TCFB, which 
benefits from the combination of directly observed term evidence with
TFB and indirectly learned cluster relevance with CFB. When we
reduced the number of presentation terms, term feedback is still
able to keep much of its performance gain over the baseline. 
Finally, we compared term feedback to document-level relevance 
feedback, and found that TCFB3C"s performance is on a par with the
latter with 5 feedback documents. We regarded term feedback as a
viable alternative to traditional relevance feedback, especially when
there are no relevant documents in the top.
We propose to extend our work in several ways. First, we want
to study whether the use of various contexts can help the user to
better identify term relevance, while not sacrificing the simplicity
and compactness of term feedback. Second, currently all terms are
presented to the user in a single batch. We could instead consider 
iterative term feedback, by presenting a small number of terms first,
and show more terms after receiving user feedback or stop when
the refined query is good enough. The presented terms should be
selected dynamically to maximize learning benefits at any moment.
Third, we have plans to incorporate term feedback into our UCAIR
toolbar[20], an Internet Explorer plugin, to make it work for web
search. We are also interested in studying how to combine term
feedback with relevance feedback or implicit feedback. We could,
for example, allow the user to dynamically modify terms in a 
language model learned from feedback documents.
