We have reviewed a wide variety of parametric methods for 
producing probability estimates from the raw scores of a discriminative
classifier and for recalibrating an uncalibrated probabilistic 
classifier. In addition, we have introduced two new families that attempt
to capitalize on the asymmetric behavior that tends to arise from
learning a discrimination function. We have given an efficient way
to estimate the parameters of these distributions.
While these distributions attempt to strike a balance between the
generalization power of parametric distributions and the flexibility
that the added asymmetric parameters give, the asymmetric 
Gaussian appears to have too great of an emphasis away from the modes.
In striking contrast, the asymmetric Laplace distribution appears to
be preferable over several large text domains and a variety of 
performance measures to the primary competing parametric methods,
though comparable performance is sometimes achieved with one
of two varieties of logistic regression. Given the ease of 
estimating the parameters of this distribution, it is a good first choice for
producing quality probability estimates.
Acknowledgments
We are grateful to Francisco Pereira for the sign test code, Anton
Likhodedov for logistic regression code, and John Platt for the code
support for the linear SVM classifier toolkit Smox. Also, we 
sincerely thank Chris Meek and John Platt for the very useful advice
provided in the early stages of this work. Thanks also to Jaime
Carbonell and John Lafferty for their useful feedback on the final
versions of this paper.
