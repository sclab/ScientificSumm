This paper focuses on security for agents patrolling in hostile 
environments. In these environments, intentional threats are caused
by adversaries about whom the security patrolling agents have 
incomplete information. Specifically, we deal with situations where
the adversaries" actions and payoffs are known but the exact 
adversary type is unknown to the security agent. Agents acting in the
real world quite frequently have such incomplete information about
other agents. Bayesian games have been a popular choice to model
such incomplete information games [3]. The Gala toolkit is one
method for defining such games [9] without requiring the game to
be represented in normal form via the Harsanyi transformation [8];
Gala"s guarantees are focused on fully competitive games. Much
work has been done on finding optimal Bayes-Nash equilbria for
The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 317
Figure 3: Reward for ASAP using multisets of 10, 30, and 80
elements
subclasses of Bayesian games, finding single Bayes-Nash 
equilibria for general Bayesian games [10] or approximate Bayes-Nash
equilibria [18]. Less attention has been paid to finding the optimal
strategy to commit to in a Bayesian game (the Stackelberg scenario
[15]). However, the complexity of this problem was shown to be
NP-hard in the general case [5], which also provides algorithms for
this problem in the non-Bayesian case.
Therefore, we present a heuristic called ASAP, with three key
advantages towards addressing this problem. First, ASAP searches
for the highest reward strategy, rather than a Bayes-Nash 
equilibrium, allowing it to find feasible strategies that exploit the 
natural first-mover advantage of the game. Second, it provides 
strategies which are simple to understand, represent, and implement.
Third, it operates directly on the compact, Bayesian game 
representation, without requiring conversion to normal form. We provide
an efficient Mixed Integer Linear Program (MILP) implementation
for ASAP, along with experimental results illustrating significant
speedups and higher rewards over other approaches.
Our k-uniform strategies are similar to the k-uniform strategies
of [12]. While that work provides epsilon error-bounds based on
the k-uniform strategies, their solution concept is still that of a
Nash equilibrium, and they do not provide efficient algorithms for
obtaining such k-uniform strategies. This contrasts with ASAP,
where our emphasis is on a highly efficient heuristic approach that
is not focused on equilibrium solutions.
Finally the patrolling problem which motivated our work has 
recently received growing attention from the multiagent community
due to its wide range of applications [4, 13]. However most of this
work is focused on either limiting energy consumption involved in
patrolling [7] or optimizing on criteria like the length of the path
traveled [4, 13], without reasoning about any explicit model of an
adversary[14].
Acknowledgments : This research is supported by the United States
Department of Homeland Security through Center for Risk and Economic
Analysis of Terrorism Events (CREATE). It is also supported by the 
Defense Advanced Research Projects Agency (DARPA), through the 
Department of the Interior, NBC, Acquisition Services Division, under Contract
No. NBCHD030010. Sarit Kraus is also affiliated with UMIACS.
