The multi agent approach to air traffic flow management
we present is predicated on adaptive agents taking 
independent actions that maximize the system evaluation function
discussed above. To that end, there are four critical 
decisions that need to be made: agent selection, agent action
set selection, agent learning algorithm selection and agent
reward structure selection.
3.1 Agent Selection
Selecting the aircraft as agents is perhaps the most 
obvious choice for defining an agent. That selection has the
advantage that agent actions can be intuitive (e.g., change
of flight plan, increase or decrease speed and altitude) and
offer a high level of granularity, in that each agent can have
its own policy. However, there are several problems with
that approach. First, there are in excess of 40,000 aircraft
in a given day, leading to a massively large multi-agent 
system. Second, as the agents would not be able to sample their
state space sufficiently, learning would be prohibitively slow.
As an alternative, we assign agents to individual ground 
locations throughout the airspace called fixes. Each agent is
then responsible for any aircraft going through its fix. Fixes
offer many advantages as agents:
1. Their number can vary depending on need. The 
system can have as many agents as required for a given
situation (e.g., agents coming live around an area
with developing weather conditions).
2. Because fixes are stationary, collecting data and 
matching behavior to reward is easier.
3. Because aircraft flight plans consist of fixes, agent will
have the ability to affect traffic flow patterns.
4. They can be deployed within the current air traffic
routing procedures, and can be used as tools to help air
traffic controllers rather than compete with or replace
them.
Figure 2 shows a schematic of this agent based system.
Agents surrounding a congestion or weather condition affect
the flow of traffic to reduce the burden on particular regions.
3.2 Agent Actions
The second issue that needs to be addressed, is 
determining the action set of the agents. Again, an obvious choice
may be for fixes to bid on aircraft, affecting their flight
plans. Though appealing from a free flight perspective, that
approach makes the flight plans too unreliable and 
significantly complicates the scheduling problem (e.g., arrival at
airports and the subsequent gate assignment process).
Instead, we set the actions of an agent to determining
the separation (distance between aircraft) that aircraft have
344 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)
to maintain, when going through the agent"s fix. This is
known as setting the Miles in Trail or MIT. When an agent
sets the MIT value to d, aircraft going towards its fix are
instructed to line up and keep d miles of separation (though
aircraft will always keep a safe distance from each other
regardless of the value of d). When there are many aircraft
going through a fix, the effect of issuing higher MIT values
is to slow down the rate of aircraft that go through the fix.
By increasing the value of d, an agent can limit the amount
of air traffic downstream of its fix, reducing congestion at
the expense of increasing the delays upstream.
Figure 2: Schematic of agent architecture. The
agents corresponding to fixes surrounding a 
possible congestion become live and start setting new
separation times.
3.3 Agent Learning
The objective of each agent is to learn the best values of
d that will lead to the best system performance, G. In this
paper we assume that each agent will have a reward 
function and will aim to maximize its reward using its own 
reinforcement learner [15] (though alternatives such as 
evolving neuro-controllers are also effective [1]). For complex
delayed-reward problems, relatively sophisticated 
reinforcement learning systems such as temporal difference may have
to be used. However, due to our agent selection and agent
action set, the air traffic congestion domain modeled in this
paper only needs to utilize immediate rewards. As a 
consequence, simple table-based immediate reward reinforcement
learning is used. Our reinforcement learner is equivalent to
an -greedy Q-learner with a discount rate of 0 [15]. At every
episode an agent takes an action and then receives a reward
evaluating that action. After taking action a and receiving
reward R an agent updates its Q table (which contains its
estimate of the value for taking that action [15]) as follows:
Q (a) = (1 − l)Q(a) + l(R), (6)
where l is the learning rate. At every time step the agent
chooses the action with the highest table value with 
probability 1 − and chooses a random action with probability
. In the experiments described in this paper, α is equal
to 0.5 and is equal to 0.25. The parameters were chosen
experimentally, though system performance was not overly
sensitive to these parameters.
3.4 Agent Reward Structure
The final issue that needs to be addressed is selecting the
reward structure for the learning agents. The first and most
direct approach is to let each agent receive the system 
performance as its reward. However, in many domains such
a reward structure leads to slow learning. We will 
therefore also set up a second set of reward structures based on
agent-specific rewards. Given that agents aim to maximize
their own rewards, a critical task is to create good agent
rewards, or rewards that when pursued by the agents lead
to good overall system performance. In this work we focus
on difference rewards which aim to provide a reward that is
both sensitive to that agent"s actions and aligned with the
overall system reward [2, 17, 18].
3.4.1 Difference Rewards
Consider difference rewards of the form [2, 17, 18]:
Di ≡ G(z) − G(z − zi + ci) , (7)
where zi is the action of agent i. All the components of
z that are affected by agent i are replaced with the fixed
constant ci
2
.
In many situations it is possible to use a ci that is 
equivalent to taking agent i out of the system. Intuitively this
causes the second term of the difference reward to 
evaluate the performance of the system without i and therefore
D evaluates the agent"s contribution to the system 
performance. There are two advantages to using D: First, because
the second term removes a significant portion of the impact
of other agents in the system, it provides an agent with
a cleaner signal than G. This benefit has been dubbed
learnability (agents have an easier time learning) in 
previous work [2, 17]. Second, because the second term does not
depend on the actions of agent i, any action by agent i that
improves D, also improves G. This term which measures the
amount of alignment between two rewards has been dubbed
factoredness in previous work [2, 17].
3.4.2 Estimates of Difference Rewards
Though providing a good compromise between aiming
for system performance and removing the impact of other
agents from an agent"s reward, one issue that may plague D
is computational cost. Because it relies on the computation
of the counterfactual term G(z − zi + ci) (i.e., the system
performance without agent i) it may be difficult or 
impossible to compute, particularly when the exact mathematical
form of G is not known. Let us focus on G functions in the
following form:
G(z) = Gf (f(z)), (8)
where Gf () is non-linear with a known functional form and,
f(z) =
X
i
fi(zi) , (9)
where each fi is an unknown non-linear function. We 
assume that we can sample values from f(z), enabling us to
compute G, but that we cannot sample from each fi(zi).
2
This notation uses zero padding and vector addition rather
than concatenation to form full state vectors from partial
state vectors. The vector zi in our notation would be ziei
in standard vector notation, where ei is a vector with a value
of 1 in the ith component and is zero everywhere else.
The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 345
In addition, we assume that Gf is much easier to compute
than f(z), or that we may not be able to even compute
f(z) directly and must sample it from a black box 
computation. This form of G matches our system evaluation
in the air traffic domain. When we arrange agents so that
each aircraft is typically only affected by a single agent, each
agent"s impact of the counts of the number of aircraft in a
sector, kt,s, will be mostly independent of the other agents.
These values of kt,s are the f(z)s in our formulation and
the penalty functions form Gf . Note that given aircraft
counts, the penalty functions (Gf ) can be easily computed
in microseconds, while aircraft counts (f) can only be 
computed by running FACET taking on the order of seconds.
To compute our counterfactual G(z − zi + ci) we need to
compute:
Gf (f(z − zi + ci)) = Gf
0
@
X
j=i
fj(zj) + fi(ci)
1
A (10)
= Gf (f(z) − fi(zi) + fi(ci)) .(11)
Unfortunately, we cannot compute this directly as the values
of fi(zi) are unknown. However, if agents take actions 
independently (it does not observe how other agents act before
taking its own action) we can take advantage of the linear
form of f(z) in the fis with the following equality:
E(f−i(z−i)|zi) = E(f−i(z−i)|ci) (12)
where E(f−i(z−i)|zi) is the expected value of all of the fs
other than fi given the value of zi and E(f−i(z−i)|ci) is the
expected value of all of the fs other than fi given the value
of zi is changed to ci. We can then estimate f(z − zi + ci):
f(z) − fi(zi) + fi(ci) = f(z) − fi(zi) + fi(ci)
+ E(f−i(z−i)|ci) − E(f−i(z−i)|zi)
= f(z) − E(fi(zi)|zi) + E(fi(ci)|ci)
+ E(f−i(z−i)|ci) − E(f−i(z−i)|zi)
= f(z) − E(f(z)|zi) + E(f(z)|ci) .
Therefore we can evaluate Di = G(z) − G(z − zi + ci) as:
Dest1
i = Gf (f(z)) − Gf (f(z) − E(f(z)|zi) + E(f(z)|ci)) , (13)
leaving us with the task of estimating the values of E(f(z)|zi)
and E(f(z)|ci)). These estimates can be computed by 
keeping a table of averages where we average the values of the 
observed f(z) for each value of zi that we have seen. This 
estimate should improve as the number of samples increases. To
improve our estimates, we can set ci = E(z) and if we make
the mean squared approximation of f(E(z)) ≈ E(f(z)) then
we can estimate G(z) − G(z − zi + ci) as:
Dest2
i = Gf (f(z)) − Gf (f(z) − E(f(z)|zi) + E(f(z))) . (14)
This formulation has the advantage in that we have more
samples at our disposal to estimate E(f(z)) than we do to
estimate E(f(z)|ci)).
