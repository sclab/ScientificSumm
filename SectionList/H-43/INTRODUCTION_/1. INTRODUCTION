With the advance of the World Wide Web, more and more 
hypertext documents become available on the Web. Some examples of
such data include organizational and personal web pages (e.g, the
WebKB benchmark data set, which contains university web pages),
research papers (e.g., data in CiteSeer), online news articles, and
customer-generated media (e.g., blogs). Comparing to data in 
traditional information management, in addition to content, these data
on the Web also contain links: e.g., hyperlinks from a student"s
homepage pointing to the homepage of her advisor, paper citations,
sources of a news article, comments of one blogger on posts from
another blogger, and so on. Performing information management
tasks on such structured data raises many new research challenges.
In the following discussion, we use the task of web page 
classification as an illustrating example, while the techniques we develop
in later sections are applicable equally well to many other tasks in
information retrieval and data mining.
For the classification problem of web pages, a simple approach
is to treat web pages as independent documents. The advantage
of this approach is that many off-the-shelf classification tools can
be directly applied to the problem. However, this approach 
relies only on the content of web pages and ignores the structure of
links among them. Link structures provide invaluable information
about properties of the documents as well as relationships among
them. For example, in the WebKB dataset, the link structure 
provides additional insights about the relationship among documents
(e.g., links often pointing from a student to her advisor or from
a faculty member to his projects). Since some links among these
documents imply the inter-dependence among the documents, the
usual i.i.d. (independent and identical distributed) assumption of
documents does not hold any more. From this point of view, the
traditional classification methods that ignore the link structure may
not be suitable.
On the other hand, a few studies, for example [25], rely solely on
link structures. It is however a very rare case that content 
information can be ignorable. For example, in the Cora dataset, the content
of a research article abstract largely determines the category of the
article.
To improve the performance of web page classification, 
therefore, both link structure and content information should be taken
into consideration. To achieve this goal, a simple approach is to
convert one type of information to the other. For example, in spam
blog classification, Kolari et al. [13] concatenate outlink features
with the content features of the blog. In document classification,
Kurland and Lee [14] convert content similarity among documents
into weights of links. However, link and content information have
different properties. For example, a link is an actual piece of 
evidence that represents an asymmetric relationship whereas the 
content similarity is usually defined conceptually for every pair of 
documents in a symmetric way. Therefore, directly converting one type
of information to the other usually degrades the quality of 
information. On the other hand, there exist some studies, as we will discuss
in detail in related work, that consider link information and content
information separately and then combine them. We argue that such
an approach ignores the inherent consistency between link and 
content information and therefore fails to combine the two seamlessly.
Some work, such as [3], incorporates link information using 
cocitation similarity, but this may not fully capture the global link
structure. In Figure 1, for example, web pages v6 and v7 co-cite
web page v8, implying that v6 and v7 are similar to each other.
In turns, v4 and v5 should be similar to each other, since v4 and
v5 cite similar web pages v6 and v7, respectively. But using 
cocitation similarity, the similarity between v4 and v5 is zero without
considering other information.
v1
v2
v3
v4
v5
v6
v7
v8
Figure 1: An example of link structure
In this paper, we propose a simple technique for analyzing
inter-connected documents, such as web pages, using factor 
analysis[18]. In the proposed technique, both content information and
link structures are seamlessly combined through a single set of 
latent factors. Our model contains two components. The first 
component captures the content information. This component has a form
similar to that of the latent topics in the Latent Semantic Indexing
(LSI) [8] in traditional information retrieval. That is, documents
are decomposed into latent topics/factors, which in turn are 
represented as term vectors. The second component captures the 
information contained in the underlying link structure, such as links
from homepages of students to those of faculty members. A 
factor can be loosely considered as a type of documents (e.g., those
homepages belonging to students). It is worth noting that we do
not explicitly define the semantic of a factor a priori. Instead, 
similar to LSI, the factors are learned from the data. Traditional factor
analysis models the variables associated with entities through the
factors. However, in analysis of link structures, we need to model
the relationship of two ends of links, i.e., edges between vertex
pairs. Therefore, the model should involve factors of both vertices
of the edge. This is a key difference between traditional factor
analysis and our model. In our model, we connect two 
components through a set of shared factors, that is, the latent factors in the
second component (for contents) are tied to the factors in the first
component (for links). By doing this, we search for a unified set
of latent factors that best explains both content and link structures
simultaneously and seamlessly.
In the formulation, we perform factor analysis based on matrix
factorization: solution to the first component is based on 
factorizing the term-document matrix derived from content features; 
solution to the second component is based on factorizing the adjacency
matrix derived from links. Because the two factorizations share
a common base, the discovered bases (latent factors) explain both
content information and link structures, and are then used in further
information management tasks such as classification.
This paper is organized as follows. Section 2 reviews related
work. Section 3 presents the proposed approach to analyze the web
page based on the combined information of links and content. 
Section 4 extends the basic framework and a few variants for fine tune.
Section 5 shows the experiment results. Section 6 discusses the
details of this approach and Section 7 concludes.
