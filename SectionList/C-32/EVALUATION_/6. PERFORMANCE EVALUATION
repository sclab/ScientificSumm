BuddyCache redirection supports the performance 
benefits of avoiding communication with the servers but 
introduces extra processing cost due to availability mechanisms
and request forwarding. Is the cure worse then the 
disease? To answer the question, we have implemented a 
BuddyCache prototype for the OCC protocol and conducted 
experiments to analyze the performance benefits and costs over
a range of network latencies.
6.1 Analysis
The performance benefits of peer fetch and peer update
are due to avoided server interactions. This section presents
a simple analytical performance model for this benefit. The
avoided server interactions correspond to different types of
client cache misses. These can be cold misses, invalidation
misses and capacity misses. Our analysis focuses on cold
misses and invalidation misses, since the benefit of avoiding
capacity misses can be derived from the cold misses. 
Moreover, technology trends indicate that memory and storage
capacity will continue to grow and therefore a typical 
BuddyCache configuration is likely not to be cache limited.
The client cache misses are determined by several 
variables, including the workload and the cache configuration.
Our analysis tries, as much as possible, to separate these
variables so they can be controlled in the validation 
experiments.
To study the benefit of avoiding cold misses, we consider
cold cache performance in a read-only workload (no 
invalidation misses). We expect peer fetch to improve the latency
cost for client cold cache misses by fetching objects from
nearby cache. We evaluate how the redirection cost affects
this benefit by comparing and analyzing the performance
of an application running in a storage system with 
BuddyCache and without (called Base).
To study the benefit of avoiding invalidation misses, we
consider hot cache performance in a workload with 
modifications (with no cold misses). In hot caches we expect
BuddyCache to provide two complementary benefits, both
of which reduce the latency of access to shared modified 
objects. Peer update lets a client access an object modified by
a nearby collaborating peer without the delay imposed by
invalidation-only protocols. In groups where peers share a
read-only interest in the modified objects, peer fetch allows
a client to access a modified object as soon as a collaborating
peer has it, which avoids the delay of server fetch without
the high cost imposed by the update-only protocols.
Technology trends indicate that both benefits will remain
important in the foreseeable future. The trend toward 
increase in available network bandwidth decreases the cost
of the update-only protocols. However, the trend toward
increasingly large caches, that are updated when cached 
objects are modified, makes invalidation-base protocols more
attractive.
To evaluate these two benefits we consider the 
performance of an application running without BuddyCache with
an application running BuddyCache in two configurations.
One, where a peer in the group modifies the objects, and
another where the objects are modified by a peer outside
the group.
Peer update can also avoid invalidation misses due to
false-sharing, introduced when multiple peers update 
different objects on the same page concurrently. We do not 
analyze this benefit (demonstrated by earlier work [2]) because
our benchmarks do not allow us to control object layout,
and also because this benefit can be derived given the cache
hit rate and workload contention.
6.1.1 The Model
The model considers how the time to complete an 
execution with and without BuddyCache is affected by 
invalidation misses and cold misses.
Consider k clients running concurrently accessing uniformly
a shared set of N pages in BuddyCache (BC) and Base. Let
tfetch(S), tredirect(S), tcommit(S), and tcompute(S) be the
time it takes a client to, respectively, fetch from server, peer
fetch, commit a transaction and compute in a transaction,
in a system S, where S is either a system with BuddyCache
(BC) or without (Base). For simplicity, our model assumes
the fetch and commit times are constant. In general they
may vary with the server load, e.g. they depend on the total
number of clients in the system.
The number of misses avoided by peer fetch depends on k,
the number of clients in the BuddyCache, and on the client
co-interest in the shared data. In a specific BuddyCache 
execution it is modeled by the variable r, defined as a number
of fetches arriving at the redirector for a given version of
page P (i.e. until an object on the page is invalidated).
Consider an execution with cold misses. A client starts
with a cold cache and runs read-only workload until it 
accesses all N pages while committing l transactions. We 
assume there are no capacity misses, i.e. the client cache is
large enough to hold N pages. In BC, r cold misses for
page P reach the redirector. The first of the misses fetches
P from the server, and the subsequent r − 1 misses are 
redirected. Since each client accesses the entire shared set r = k.
Let Tcold(Base) and Tcold(BC) be the time it takes to
complete the l transactions in Base and BC.
33
Tcold(Base) = N ∗ tfetch(Base)
+(tcompute + tcommit(Base)) ∗ l (1)
Tcold(BC) =
N ∗
1
k
∗ tfetch(BC) + (1 −
1
k
) ∗ tredirect
+(tcompute + tcommit(BC)) ∗ l (2)
Consider next an execution with invalidation misses. A
client starts with a hot cache containing the working set of N
pages. We focus on a simple case where one client (writer)
runs a workload with modifications, and the other clients
(readers) run a read-only workload.
In a group containing the writer (BCW ), peer update
eliminates all invalidation misses. In a group containing
only readers (BCR), during a steady state execution with
uniform updates, a client transaction has missinv 
invalidation misses. Consider the sequence of r client misses on
page P that arrive at the redirector in BCR between two
consequent invalidations of page P. The first miss goes to
the server, and the r − 1 subsequent misses are redirected.
Unlike with cold misses, r ≤ k because the second 
invalidation disables redirection for P until the next miss on P
causes a server fetch.
Assuming uniform access, a client invalidation miss has a
chance of 1/r to be the first miss (resulting in server fetch),
and a chance of (1 − 1/r) to be redirected.
Let Tinval(Base), Tinval(BCR) and Tinval(BCW ) be the
time it takes to complete a single transaction in the Base,
BCR and BCW systems.
Tinval(Base) = missinv ∗ tfetch(Base)
+tcompute + tcommit(Base) (3)
Tinval(BCR) = missinv ∗ (
1
r
∗ tfetch(BCR)
+(1 −
1
r
) ∗ tredirect(BCR))
+tcompute + tcommit(BCR) (4)
Tinval(BCW ) = tcompute + tcommit(BCW ) (5)
In the experiments described below, we measure the 
parameters N, r, missinv, tfetch(S), tredirect(S), tcommit(S),
and tcompute(S). We compute the completion times derived
using the above model and derive the benefits. We then
validate the model by comparing the derived values to the
completion times and benefits measured directly in the 
experiments.
6.2 Experimental Setup
Before presenting our results we describe our experimental
setup. We use two systems in our experiments. The Base
system runs Thor distributed object storage system [23] with
clients connecting directly to the servers. The Buddy system
runs our implementation of BuddyCache prototype in Thor,
supporting peer fetch, peer update, and solo commit, but
not the failover.
Our workloads are based on the multi-user OO7 
benchmark [8]; this benchmark is intended to capture the 
characteristics of many different multi-user CAD/CAM/CASE 
applications, but does not model any specific application. We
use OO7 because it is a standard benchmark for measuring
object storage system performance. The OO7 database 
contains a tree of assembly objects with leaves pointing to three
composite parts chosen randomly from among 500 such 
objects. Each composite part contains a graph of atomic parts
linked by connection objects; each atomic part has 3 
outgoing connections. We use a medium database that has 200
atomic parts per composite part. The multi-user database
allocates for each client a private module consisting of one
tree of assembly objects, and adds an extra shared module
that scales proportionally to the number of clients.
We expect a typical BuddyCache configuration not to be
cache limited and therefore focus on workloads where the
objects in the client working set fit in the cache. Since the
goal of our study is to evaluate how effectively our 
techniques deal with access to shared objects, in our study we
limit client access to shared data only. This allows us to
study the effect our techniques have on cold cache and cache
consistency misses and isolate as much as possible the effect
of cache capacity misses.
To keep the length of our experiments reasonable, we use
small caches. The OO7 benchmark generates database 
modules of predefined size. In our implementation of OO7, the
private module size is about 38MB. To make sure that the
entire working set fits into the cache we use a single private
module and choose a cache size of 40MB for each client. The
OO7 database is generated with modules for 3 clients, only
one of which is used in our experiments as we explain above.
The objects in the database are clustered in 8K pages, which
are also the unit of transfer in the fetch requests.
We consider two types of transaction workloads in our
analysis, read-only and read-write. In OO7 benchmark,
read-only transactions use the T1 traversal that performs
a depth-first traversal of entire composite part graph. Write
transactions use the T2b traversal that is identical to T1
except that it modifies all the atomic parts in a single 
composite. A single transaction includes one traversal and there
is no sleep time between transactions. Both read-only and
read-write transactions always work with data from the same
module. Clients running read-write transactions don"t 
modify in every transaction, instead they have a 50% probability
of running read-only transactions.
The database was stored by a server on a 40GB IBM
7200RPM hard drive, with a 8.5 average seek time and 40
MB/sec data transfer rates. In Base system clients 
connect directly to the database. In Buddy system clients 
connect to the redirector that connects to the database. We
run the experiments with 1-10 clients in Base, and one or
two 1-10 client groups in Buddy. The server, the clients
and the redirectors ran on a 850MHz Intel Pentium III 
processor based PC, 512MB of memory, and Linux Red Hat
6.2. They were connected by a 100Mb/s Ethernet. The
server was configured with a 50MB cache (of which 6MB
were used for the modified object buffer), the client had a
40MB cache. The experiments ran in Utah experimental
testbed emulab.net [1].
34
Latency [ms]
Base Buddy
3 group 5 group 3 group 5 group
Fetch 1.3 1.4 2.4 2.6
Commit 2.5 5.5 2.4 5.7
Table 1: Commit and Server fetch
Operation Latency [ms]
PeerFetch 1.8 - 5.5
−AlertHelper 0.3 - 4.6
−CopyUnswizzle 0.24
−CrossRedirector 0.16
Table 2: Peer fetch
6.3 Basic Costs
This section analyzes the basic cost of the requests in the
Buddy system during the OO7 runs.
6.3.1 Redirection
Fetch and commit requests in the BuddyCache cross the
redirector, a cost not incurred in the Base system. For a
request redirected to the server (server fetch) the extra cost
of redirection includes a local request from the client to 
redirector on the way to and from the server. We evaluate this
latency overhead indirectly by comparing the measured 
latency of the Buddy system server fetch or commit request
with the measured latency of the corresponding request in
the Base system.
Table 1 shows the latency for the commit and server fetch
requests in the Base and Buddy system for 3 client and 5
client groups in a fast local area network. All the numbers
were computed by averaging measured request latency over
1000 requests. The measurements show that the redirection
cost of crossing the redirector in not very high even in a
local area network. The commit cost increases with the
number of clients since commits are processed sequentially.
The fetch cost does not increase as much because the server
cache reduces this cost. In a large system with many groups,
however, the server cache becomes less efficient.
To evaluate the overheads of the peer fetch, we measure
the peer fetch latency (PeerFetch) at the requesting client
and break down its component costs. In peer fetch, the cost
of the redirection includes, in addition to the local network
request cost, the CPU processing latency of crossing the
redirector and crossing the helper, the latter including the
time to process the help request and the time to copy, and
unswizzle the requested page.
We directly measured the time to copy and unswizzle the
requested page at the helper, (CopyUnswizzle), and timed
the crossing times using a null crossing request. Table 2
summarizes the latencies that allows us to break down the
peer fetch costs. CrossRedirector, includes the CPU latency
of crossing the redirector plus a local network round-trip
and is measured by timing a round-trip null request issued
by a client to the redirector. AlertHelper, includes the time
for the helper to notice the request plus a network 
roundtrip, and is measured by timing a round-trip null request
issued from an auxiliary client to the helper client. The
local network latency is fixed and less than 0.1 ms.
The AlertHelper latency which includes the elapsed time
from the help request arrival until the start of help request
processing is highly variable and therefore contributes to
the high variability of the PeerFetch time. This is because
the client in Buddy system is currently single threaded and
therefore only starts processing a help request when blocked
waiting for a fetch- or commit reply. This overhead is not
inherent to the BuddyCache architecture and could be 
mitigated by a multi-threaded implementation in a system with
pre-emptive scheduling.
6.3.2 Version Cache
The solo commit allows a fast client modifying an object
to commit independently of a slow peer. The solo 
commit mechanism introduces extra processing at the server
at transaction validation time, and extra processing at the
client at transaction commit time and at update or 
invalidation processing time.
The server side overheads are minimal and consist of a
page version number update at commit time, and a version
number comparison at transaction validation time.
The version cache has an entry only when invalidations or
updates arrive out of order. This may happen when a 
transaction accesses objects in multiple servers. Our experiments
run in a single server system and therefore, the commit time
overhead of version cache management at the client does not
contribute in the results presented in the section below. To
gauge these client side overheads in a multiple server 
system, we instrumented the version cache implementation to
run with a workload trace that included reordered 
invalidations and timed the basic operations.
The extra client commit time processing includes a version
cache lookup operation for each object read by the 
transaction at commit request preparation time, and a version
cache insert operation for each object updated by a 
transaction at commit reply processing time, but only if the 
updated page is missing some earlier invalidations or updates.
It is important that the extra commit time costs are kept
to a minimum since client is synchronously waiting for the
commit completion. The measurements show that in the
worst case, when a large number of invalidations arrive out
of order, and about half of the objects modified by T2a (200
objects) reside on reordered pages, the cost of updating the
version cache is 0.6 ms. The invalidation time cost are 
comparable, but since invalidations and updates are processed
in the background this cost is less important for the overall
performance. We are currently working on optimizing the
version cache implementation to further reduce these costs.
6.4 Overall Performance
This section examines the performance gains seen by an
application running OO7 benchmark with a BuddyCache in
a wide area network.
6.4.1 Cold Misses
To evaluate the performance gains from avoiding cold
misses we compare the cold cache performance of OO7 
benchmark running read-only workload in the Buddy and Base
systems. We derive the times by timing the execution of the
systems in the local area network environment and 
substituting 40 ms and 80 ms delays for the requests crossing the
redirector and the server to estimate the performance in the
wide-area-network. Figures 7 and 8 show the overall time to
complete 1000 cold cache transactions. The numbers were
35
0
5 0
100
150
200
250
Base Buddy Base Buddy Base Buddy
3 Clients 5 Clients 10 Clients
[ms]
CPU Commit Server Fetch Peer Fetch
Figure 7: Breakdown for cold read-only 40ms RTT
0
5 0
100
150
200
250
300
350
400
Base Buddy Base Buddy Base Buddy
3 Clients 5 Clients 10 Clients
[ms]
CPU Commit Server Fetch Peer Fetch
Figure 8: Breakdown for cold read-only 80ms RTT
obtained by averaging the overall time of each client in the
group.
The results show that in a 40 ms network Buddy 
system reduces significantly the overall time compared to the
Base system, providing a 39% improvement in a three client
group, 46% improvement in the five client group and 56%
improvement in the ten client case.
The overall time includes time spent performing client
computation, direct fetch requests, peer fetches, and commit
requests.
In the three client group, Buddy and Base incur almost
the same commit cost and therefore the entire performance
benefit of Buddy is due to peer fetch avoiding direct fetches.
In the five and ten client group the server fetch cost for
individual client decreases because with more clients faulting
in a fixed size shared module into BuddyCache, each client
needs to perform less server fetches.
Figure 8 shows the overall time and cost break down in
the 80 ms network. The BuddyCache provides similar 
performance improvements as with the 40ms network. Higher
network latency increases the relative performance 
advantage provided by peer fetch relative to direct fetch but this
benefit is offset by the increased commit times.
Figure 9 shows the relative latency improvement provided
by BuddyCache (computed as the overall measured time
difference between Buddy and Base relative to Base) as a
-10%
0%
10%
20%
30%
40%
50%
60%
70%
1 5 1 0 2 0 3 0 4 0 5 0 6 0 7 0 8 0 9 0 100
Latency [ms]
3 Clients 3 Clients (Perf model) 5 Clients
5 Clients (Perf model) 10 Clients 10 FEs (perf model)
Figure 9: Cold miss benefit
0
2 0
4 0
6 0
8 0
100
120
140
Base Buddy Reader Buddy Writer
[ms]
CPU Commit Server Fetch Peer Fetch
Figure 10: Breakdown for hot read-write 40ms RTT
function of network latency, with a fixed server load. The
cost of the extra mechanism dominates BuddyCache benefit
when network latency is low. At typical Internet latencies
20ms-60ms the benefit increases with latency and levels off
around 60ms with significant (up to 62% for ten clients)
improvement.
Figure 9 includes both the measured improvement and the
improvement derived using the analytical model.Remarkably,
the analytical results predict the measured improvement
very closely, albeit being somewhat higher than the 
empirical values. The main reason why the simplified model works
well is it captures the dominant performance component,
network latency cost.
6.4.2 Invalidation Misses
To evaluate the performance benefits provided by 
BuddyCache due to avoided invalidation misses, we compared the
hot cache performance of the Base system with two 
different Buddy system configurations. One of the Buddy system
configurations represents a collaborating peer group 
modifying shared objects (Writer group), the other represents a
group where the peers share a read-only interest in the 
modified objects (Reader group) and the writer resides outside
the BuddyCache group.
In each of the three systems, a single client runs a 
readwrite workload (writer) and three other clients run read-only
workload (readers). Buddy system with one group 
contain36
0
5 0
100
150
200
250
300
Base Buddy Reader Buddy Writer
[ms]
CPU Commit Server Fetch Peer Fetch
Figure 11: Breakdown for hot read-write 80ms RTT
ing a single reader and another group containing two readers
and one writer models the Writer group. Buddy system with
one group containing a single writer and another group 
running three readers models the Reader group. In Base, one
writer and three readers access the server directly. This
simple configuration is sufficient to show the impact of 
BuddyCache techniques.
Figures 10 and 11 show the overall time to complete 1000
hot cache OO7 read-only transactions. We obtain the 
numbers by running 2000 transactions to filter out cold misses
and then time the next 1000 transactions. Here again, the
reported numbers are derived from the local area network
experiment results.
The results show that the BuddyCache reduces 
significantly the completion time compared to the Base system.
In a 40 ms network, the overall time in the Writer group
improves by 62% compared to Base. This benefit is due
to peer update that avoids all misses due to updates. The
overall time in the Reader group improves by 30% and is
due to peer fetch that allows a client to access an 
invalidated object at the cost of a local fetch avoiding the delay
of fetching from the server. The latter is an important 
benefit because it shows that on workloads with updates, peer
fetch allows an invalidation-based protocol to provide some
of the benefits of update-based protocol.
Note that the performance benefit delivered by the peer
fetch in the Reader group is approximately 50% less than the
performance benefit delivered by peer update in the Writer
group. This difference is similar in 80ms network.
Figure 12 shows the relative latency improvement 
provided by BuddyCache in Buddy Reader and Buddy Writer
configurations (computed as the overall time difference 
between BuddyReader and Base relative to Base, and Buddy
Writer and Base relative to Base) in a hot cache experiment
as a function of increasing network latency, for fixed server
load.
The peer update benefit dominates overhead in Writer
configuration even in low-latency network (peer update 
incurs minimal overhead) and offers significant 44-64% 
improvement for entire latency range.
The figure includes both the measured improvement and
the improvement derived using the analytical model. As
in cold cache experiments, here the analytical results 
predict the measured improvement closely. The difference is
-10%
0%
10%
20%
30%
40%
50%
60%
70%
1 5 1 0 2 0 3 0 4 0 5 0 6 0 7 0 8 0 9 0 100
Latency [ms]
Benefits[%]
Buddy Reader Buddy Reader (perf model)
Buddy Writer Buddy Writer (perf model)
Figure 12: Invalidation miss benefit
minimal in the "writer group", and somewhat higher in the
"reader group" (consistent with the results in the cold cache
experiments). As in cold cache case, the reason why the 
simplified analytical model works well is because it captures the
costs of network latency, the dominant performance cost.
