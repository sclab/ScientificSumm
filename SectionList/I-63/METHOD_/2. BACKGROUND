Similarly to the model used in previous work on 
resourceallocation with MDP-induced preferences [6, 7], we define
the value of a set of resources to an agent as the value of the
best MDP policy that is realizable, given those resources.
However, since the focus of our work is on scheduling 
problems, and a large part of the optimization problem is to
decide how resources are allocated in time among agents
with finite arrival and departure times, we model the agents"
planning problems as finite-horizon MDPs, in contrast to
previous work that used infinite-horizon discounted MDPs.
In the rest of this section, we first introduce some 
necessary background on finite-horizon MDPs and present a
linear-programming formulation that serves as the basis for
our solution algorithm developed in Section 4. We also 
outline the standard methods for combinatorial resource 
scheduling with flat resource values, which serve as a comparison
benchmark for the new model developed here.
2.1 Markov Decision Processes
A stationary, finite-domain, discrete-time MDP (see, for
example, [13] for a thorough and detailed development) can
be described as S, A, p, r , where: S is a finite set of 
system states; A is a finite set of actions that are available to
the agent; p is a stationary stochastic transition function,
where p(σ|s, a) is the probability of transitioning to state σ
upon executing action a in state s; r is a stationary reward
function, where r(s, a) specifies the reward obtained upon
executing action a in state s.
Given such an MDP, a decision problem under a finite
horizon T is to choose an optimal action at every time step
to maximize the expected value of the total reward accrued
during the agent"s (finite) lifetime. The agent"s optimal 
policy is then a function of current state s and the time until
the horizon. An optimal policy for such a problem is to act
greedily with respect to the optimal value function, defined
recursively by the following system of finite-time Bellman
equations [2]:
v(s, t) = max
a
r(s, a) +
X
σ
p(σ|s, a)v(σ, t + 1),
∀s ∈ S, t ∈ [1, T − 1];
v(s, T) = 0, ∀s ∈ S;
where v(s, t) is the optimal value of being in state s at time
t ∈ [1, T].
This optimal value function can be easily computed using
dynamic programming, leading to the following optimal 
policy π, where π(s, a, t) is the probability of executing action
a in state s at time t:
π(s, a, t) =
(
1, a = argmaxa r(s, a) +
P
σ p(σ|s, a)v(σ, t + 1),
0, otherwise.
The above is the most common way of computing the
optimal value function (and therefore an optimal policy) for
a finite-horizon MDP. However, we can also formulate the
problem as the following linear program (similarly to the
dual LP for infinite-horizon discounted MDPs [13, 6, 7]):
max
X
s
X
a
r(s, a)
X
t
x(s, a, t)
subject to:
X
a
x(σ, a, t + 1) =
X
s,a
p(σ|s, a)x(s, a, t) ∀σ, t ∈ [1, T − 1];
X
a
x(s, a, 1) = α(s), ∀s ∈ S;
(1)
where α(s) is the initial distribution over the state space, and
x is the (non-stationary) occupation measure (x(s, a, t) ∈
[0, 1] is the total expected number of times action a is 
executed in state s at time t). An optimal (non-stationary)
policy is obtained from the occupation measure as follows:
π(s, a, t) = x(s, a, t)/
X
a
x(s, a, t) ∀s ∈ S, t ∈ [1, T]. (2)
Note that the standard unconstrained finite-horizon MDP,
as described above, always has a uniformly-optimal 
solution (optimal for any initial distribution α(s)). Therefore,
an optimal policy can be obtained by using an arbitrary
constant α(s) > 0 (in particular, α(s) = 1 will result in
x(s, a, t) = π(s, a, t)).
However, for MDPs with resource constraints (as defined
below in Section 3), uniformly-optimal policies do not in
general exist. In such cases, α becomes a part of the 
problem input, and a resulting policy is only optimal for that
particular α. This result is well known for infinite-horizon
MDPs with various types of constraints [1, 6], and it also
holds for our finite-horizon model, which can be easily 
established via a line of reasoning completely analogous to the
arguments in [6].
2.2 Combinatorial Resource Scheduling
A straightforward approach to resource scheduling for a
set of agents M, whose values for the resources are induced
by stochastic planning problems (in our case, finite-horizon
MDPs) would be to have each agent enumerate all possible
resource assignments over time and, for each one, compute
its value by solving the corresponding MDP. Then, each
agent would provide valuations for each possible resource
bundle over time to a centralized coordinator, who would
compute the optimal resource assignments across time based
on these valuations.
When resources can be allocated at different times to 
different agents, each agent must submit valuations for 
every combination of possible time horizons. Let each agent
m ∈ M execute its MDP within the arrival-departure time
interval τ ∈ [τa
m, τd
m]. Hence, agent m will execute an MDP
with time horizon no greater than Tm = τd
m−τa
m+1. Let bτ be
the global time horizon for the problem, before which all of
the agents" MDPs must finish. We assume τd
m < bτ, ∀m ∈ M.
The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1221
For the scheduling problem where agents have static 
resource requirements within their finite-horizon MDPs, the
agents provide a valuation for each resource bundle for each
possible time horizon (from [1, Tm]) that they may use. Let
Ω be the set of resources to be allocated among the agents.
An agent will get at most one resource bundle for one of the
time horizons. Let the variable ψ ∈ Ψm enumerate all 
possible pairs of resource bundles and time horizons for agent
m, so there are 2|Ω|
× Tm values for ψ (the space of bundles
is exponential in the number of resource types |Ω|).
The agent m must provide a value vψ
m for each ψ, and
the coordinator will allocate at most one ψ (resource, time
horizon) pair to each agent. This allocation is expressed as
an indicator variable zψ
m ∈ {0, 1} that shows whether ψ is
assigned to agent m. For time τ and resource ω, the function
nm(ψ, τ, ω) ∈ {0, 1} indicates whether the bundle in ψ uses
resource ω at time τ (we make the assumption that agents
have binary resource requirements). This allocation problem
is NP-complete, even when considering only a single time
step, and its difficulty increases significantly with multiple
time steps because of the increasing number of values of ψ.
The problem of finding an optimal allocation that satisfies
the global constraint that the amount of each resource ω
allocated to all agents does not exceed the available amount
bϕ(ω) can be expressed as the following integer program:
max
X
m∈M
X
ψ∈Ψm
zψ
mvψ
m
subject to:
X
ψ∈Ψm
zψ
m ≤ 1, ∀m ∈ M;
X
m∈M
X
ψ∈Ψm
zψ
mnm(ψ, τ, ω) ≤ bϕ(ω), ∀τ ∈ [1, bτ], ∀ω ∈ Ω;
(3)
The first constraint in equation 3 says that no agent can
receive more than one bundle, and the second constraint
ensures that the total assignment of resource ω does not, at
any time, exceed the resource bound.
For the scheduling problem where the agents are able to
dynamically reallocate resources, each agent must specify
a value for every combination of bundles and time steps
within its time horizon. Let the variable ψ ∈ Ψm in this case
enumerate all possible resource bundles for which at most
one bundle may be assigned to agent m at each time step.
Therefore, in this case there are
P
t∈[1,Tm](2|Ω|
)t
∼ 2|Ω|Tm
possibilities of resource bundles assigned to different time
slots, for the Tm different time horizons.
The same set of equations (3) can be used to solve this
dynamic scheduling problem, but the integer program is 
different because of the difference in how ψ is defined. In this
case, the number of ψ values is exponential in each agent"s
planning horizon Tm, resulting in a much larger program.
This straightforward approach to solving both of these
scheduling problems requires an enumeration and solution
of either 2|Ω|
Tm (static allocation) or
P
t∈[1,Tm] 2|Ω|t

(dynamic reallocation) MDPs for each agent, which very quickly
becomes intractable with the growth of the number of 
resources |Ω| or the time horizon Tm.
