We have presented a new approach to pseudo-relevance
feedback based on document and query sampling. The use
of sampling is a very flexible and powerful device and is 
motivated by our general desire to extend current models of 
retrieval by estimating the risk or variance associated with the
parameters or output of retrieval processes. Such variance
estimates, for example, may be naturally used in a Bayesian
framework for improved model estimation and combination.
Applications such as selective expansion may then be 
implemented in a principled way.
While our study uses the language modeling approach as a
framework for experiments, we make few assumptions about
the actual workings of the feedback algorithm. We believe
it is likely that any reasonably effective baseline feedback
algorithm would benefit from our approach. Our results on
standard TREC collections show that our framework 
improves the robustness of a strong baseline feedback method
across a variety of collections, without sacrificing average
precision. It also gives small but consistent gains in 
top10 precision. In future work, we envision an investigation
into how varying the set of sampling methods used and the
number of samples controls the trade-off between 
robustness, accuracy, and efficiency.
Acknowledgements
We thank Paul Bennett for valuable discussions related to
this work, which was supported by NSF grants #IIS-0534345
and #CNS-0454018, and U.S. Dept. of Education grant
#R305G03123. Any opinions, findings, and conclusions or
recommendations expressed in this material are the authors.
and do not necessarily reflect those of the sponsors.
