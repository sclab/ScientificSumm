In Sections 2.1-2.5 we describe a general method for 
estimating a probability distribution over the set of possible
language models. In Sections 2.6 and 2.7 we summarize how
different query samples are used to generate multiple 
feedback models, which are then combined.
2.1 Modeling Feedback Uncertainty
Given a query Q and a collection C, we assume a 
probabilistic retrieval system that assigns a real-valued document
score f(D, Q) to each document D in C, such that the score
is proportional to the estimated probability of relevance. We
make no other assumptions about f(D, Q). The nature of
f(D, Q) may be complex: for example, if the retrieval 
system supports structured query languages [12], then f(D, Q)
may represent the output of an arbitrarily complex 
inference network defined by the structured query operators. In
theory, the scoring function can vary from query to query,
although in this study for simplicity we keep the scoring
function the same for all queries. Our specific query method
is given in Section 3.
We treat the feedback algorithm as a black box and 
assume that the inputs to the feedback algorithm are the 
original query and the corresponding top-retrieved documents,
with a score being given to each document. We assume that
the output of the feedback algorithm is a vector of term
weights to be used to add or reweight the terms in the 
representation of the original query, with the vector normalized
to form a probability distribution. We view the the inputs
to the feedback black box as random variables, and analyze
the feedback model as a random variable that changes in 
response to changes in the inputs. Like the document scoring
function f(D, Q), the feedback algorithm may implement
a complex, non-linear scoring formula, and so as its inputs
vary, the resulting feedback models may have a complex
distribution over the space of feedback models (the sample
space). Because of this potential complexity, we do not 
attempt to derive a posterior distribution in closed form, but
instead use simulation. We call this distribution over 
possible feedback models the feedback model distribution. Our
goal in this section is to estimate a useful approximation to
the feedback model distribution.
For a specific framework for experiments, we use the 
language modeling (LM) approach for information retrieval [15].
The score of a document D with respect to a query Q and
collection C is given by p(Q|D) with respect to language
models ˆθQ and ˆθD estimated for the query and document
respectively. We denote the set of k top-retrieved 
documents from collection C in response to Q by DQ(k, C). For
simplicity, we assume that queries and documents are 
generated by multinomial distributions whose parameters are
represented by unigram language models.
To incorporate feedback in the LM approach, we assume a
model-based scheme in which our goal is take the query and
resulting ranked documents DQ(k, C) as input, and output
an expansion language model ˆθE, which is then interpolated
with the original query model ˆθQ:
ˆθNew = (1 − α) · ˆθQ + α · ˆθE (1)
This includes the possibility of α = 1 where the original
query mode is completely replaced by the feedback model.
Our sample space is the set of all possible language 
models LF that may be output as feedback models. Our 
approach is to take samples from this space and then fit a
distribution to the samples using maximum likelihood. For
simplicity, we start by assuming the latent feedback 
distribution has the form of a Dirichlet distribution. Although the
Dirichlet is a unimodal distribution, and in general quite
limited in its expressiveness in the sample space, it is a 
natural match for the multinomial language model, can be 
estimated quickly, and can capture the most salient features of
confident and uncertain feedback models, such as the overall
spread of the distibution.
2.2 Resampling document models
We would like an approximation to the posterior 
distribution of the feedback model LF . To accomplish this, we
apply a widely-used simulation technique called bootstrap
sampling ([7], p. 474) on the input parameters, namely, the
set of top-retrieved documents.
Bootstrap sampling allows us to simulate the approximate
effect of perturbing the parameters within the black box
feedback algorithm by perturbing the inputs to that 
algorithm in a systematic way, while making no assumptions
about the nature of the feedback algorithm.
Specifically, we sample k documents with replacement from
DQ(k, C), and calculate an expansion language model θb 
using the black box feedback method. We repeat this process
B times to obtain a set of B feedback language models, to
which we then fit a Dirichlet distribution. Typically B is
in the range of 20 to 50 samples, with performance being
relatively stable in this range. Note that instead of treating
each top document as equally likely, we sample according to
the estimated probabilities of relevance of each document in
DQ(k, C). Thus, a document is more likely to be chosen the
higher it is in the ranking.
2.3 Justification for a sampling approach
The rationale for our sampling approach has two parts.
First, we want to improve the quality of individual 
feedback models by smoothing out variation when the baseline
feedback model is unstable. In this respect, our approach
resembles bagging [4], an ensemble approach which 
generates multiple versions of a predictor by making bootstrap
copies of the training set, and then averages the (numerical)
predictors. In our application, top-retrieved documents can
be seen as a kind of noisy training set for relevance.
Second, sampling is an effective way to estimate basic
properties of the feedback posterior distribution, which can
then be used for improved model combination. For 
example, a model may be weighted by its prediction confidence,
estimated as a function of the variability of the posterior
around the model.
foo2-401.map-Dim:5434,Size:12*12units,gaussianneighborhood
(a) Topic 401
Foreign
minorities,
Germany
foo2-402.map-Dim:5698,Size:12*12units,gaussianneighborhood
(b) Topic 402
Behavioral
genetics
foo2-459.map-Dim:8969,Size:12*12units,gaussianneighborhood
(c) Topic 459
When can a
lender foreclose
on property
Figure 2: Visualization of expansion language model 
variance using self-organizing maps, showing the distribution of
language models that results from resampling the inputs to
the baseline expansion method. The language model that
would have been chosen by the baseline expansion is at
the center of each map. The similarity function is 
JensenShannon divergence.
2.4 Visualizing feedback distributions
Before describing how we fit and use the Dirichlet 
distribution over feedback models, it is instructive to view some
examples of actual feedback model distributions that result
from bootstrap sampling the top-retrieved documents from
different TREC topics.
Each point in our sample space is a language model, which
typically has several thousand dimensions. To help analyze
the behavior of our method we used a Self-Organizing Map
(via the SOM-PAK package [9]), to ‘flatten" and visualize
the high-dimensional density function2
.
The density maps for three TREC topics are shown in
Figure 2 above. The dark areas represent regions of high
similarity between language models. The light areas 
represent regions of low similarity - the ‘valleys" between 
clusters. Each diagram is centered on the language model that
would have been chosen by the baseline expansion. A single
peak (mode) is evident in some examples, but more complex
structure appears in others. Also, while the distribution is
usually close to the baseline feedback model, for some topics
they are a significant distance apart (as measured by 
JensenShannon divergence), as in Subfigure 2c. In such cases, the
mode or mean of the feedback distribution often performs
significantly better than the baseline (and in a smaller 
proportion of cases, significantly worse).
2.5 Fitting a posterior feedback distribution
After obtaining feedback model samples by resampling
the feedback model inputs, we estimate the feedback 
distribution. We assume that the multinomial feedback 
models {ˆθ1, . . . , ˆθB} were generated by a latent Dirichlet 
distribution with parameters {α1, . . . , αN }. To estimate the
{α1, . . . , αN }, we fit the Dirichlet parameters to the B 
language model samples according to maximum likelihood 
using a generalized Newton procedure, details of which are
given in Minka [13]. We assume a simple Dirichlet prior over
the {α1, . . . , αN }, setting each to αi = μ · p(wi | C), where μ
is a parameter and p(· | C) is the collection language model
estimated from a set of documents from collection C. The
parameter fitting converges very quickly - typically just 2 or
2
Because our points are language models in the 
multinomial simplex, we extended SOM-PAK to support 
JensenShannon divergence, a widely-used similarity measure 
between probability distributions.
3 iterations are enough - so that it is practical to apply at
query-time when computational overhead must be small. In
practice, we can restrict the calculation to the vocabulary of
the top-retrieved documents, instead of the entire collection.
Note that for this step we are re-using the existing retrieved
documents and not performing additional queries.
Given the parameters of an N-dimensional Dirichlet 
distribution Dir(α) the mean μ and mode x vectors are easy
to calculate and are given respectively by
μi = αiP
αi
(2) and xi = αi−1P
αi−N
. (3)
We can then choose the language model at the mean or the
mode of the posterior as the final enhanced feedback model.
(We found the mode to give slightly better performance.)
For information retrieval, the number of samples we will
have available is likely to be quite small for performance 
reasons - usually less than ten. Moreover, while random 
sampling is useful in certain cases, it is perfectly acceptable to
allow deterministic sampling distributions, but these must
be designed carefully in order to approximate an accurate
output variance. We leave this for future study.
2.6 Query variants
We use the following methods for generating variants of
the original query. Each variant corresponds to a different
assumption about which aspects of the original query may
be important. This is a form of deterministic sampling.
We selected three simple methods that cover complimentary
assumptions about the query.
No-expansion Use only the original query. The 
assumption is that the given terms are a complete description
of the information need.
Leave-one-out A single term is left out of the original
query. The assumption is that one of the query terms
is a noise term.
Single-term A single term is chosen from the original query.
This assumes that only one aspect of the query, namely,
that represented by the term, is most important.
After generating a variant of the original query, we combine
it with the original query using a weight αSUB so that we
do not stray too ‘far". In this study, we set αSUB = 0.5. For
example, using the Indri [12] query language, a 
leave-oneout variant of the initial query that omits the term ‘ireland"
for TREC topic 404 is:
#weight(0.5 #combine(ireland peace talks)
0.5 #combine(peace talks))
2.7 Combining enhanced feedback models
from multiple query variants
When using multiple query variants, the resulting 
enhanced feedback models are combined using Bayesian model
combination. To do this, we treat each word as an item to
be classified as belonging to a relevant or non-relevant class,
and derive a class probability for each word by combining
the scores from each query variant. Each score is given by
that term"s probability in the Dirichlet distribution. The
term scores are weighted by the inverse of the variance of
the term in the enhanced feedback model"s Dirichlet 
distribution. The prior probability of a word"s membership in
the relevant class is given by the probability of the original
query in the entire enhanced expansion model.
