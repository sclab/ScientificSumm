The proposed evaluation framework allows us to measure
the degree to which different IR systems overcome (or don"t
overcome) term mismatch between queries and relevant 
documents. Evaluations of IR systems employing QE performed
only on the entire collection do not take into account that
the purpose of QE is to mitigate the effects of term mismatch
in retrieval. By systematically removing query terms from
relevant documents, we can measure the degree to which
QE contributes to a search by showing the difference 
between the performances of a QE system and its 
keywordonly baseline when query terms have been removed from
known relevant documents. Further, we can model the 
behavior of expert versus non-expert users by manipulating
the amount of query-document term mismatch introduced
into the collection.
The evaluation framework proposed in this paper is 
attractive for several reasons. Most importantly, it provides
a controlled manner in which to measure the performance
of QE with respect to query-document term mismatch. In
addition, this framework takes advantage and stretches the
amount of information we can get from existing test 
collections. Further, this evaluation framework is not 
metricspecific: information in terms of any metric (MAP, P@10,
etc.) can be gained from evaluating an IR system this way.
It should also be noted that this framework is 
generalizable to any IR system, in that it evaluates how well IR
systems evaluate users" information needs as represented by
their queries. An IR system that is easy to use should be
good at retrieving documents that are relevant to users" 
information needs, even if the queries provided by the users do
not contain the same keywords as the relevant documents.
