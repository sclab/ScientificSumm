This article deals with the problem of collaborative 
concept learning in a multi-agent system. [6] introduces a 
characterisation of learning in multi-agent system according to
the level of awareness of the agents. At level 1, agents learn
âˆ—The primary author of this paper is a student.
in the system without taking into account the presence of
other agents, except through the modification brought upon
the environment by their action. Level 2 implies direct 
interaction between the agents as they can exchange messages
to improve their learning. Level 3 would require agents to
take into account the competencies of other agents, and be
able to learn from observation of the other agents" behaviour
(while considering them as independant entities and not 
indetermined part of the environment as in level 1). We focus
in this paper on level 2, studying direct interaction between
agents involved in a learning process.
Each agent is assumed to be able to learn incrementally from
the data he receives, meaning that each agent can update
his belief set B to keep it consistent with the whole set of
information K that he has received from the environment
or from other agents. In such a case, we will say that he is
a-consistent. Here, the belief set B represents hypothetical
knowledge that can therefore be revised, whereas the set of
information K represents certain knowledge, consisting of
non revisable observations and facts. Moreover, we suppose
that at least a part Bc of the beliefs of each agent is 
common to all agents and must stay that way. Therefore, an
update of this common set Bc by agent r must provoke an
update of Bc for the whole community of agents. It leads
us to define what is the mas-consistency of an agent with
respect to the community. The update process of the 
community beliefs when one of its members gets new information
can then be defined as the consistency maintenance process
ensuring that every agent in the community will stay 
masconsistent. This mas-consistency maintenance process of an
agent getting new information gives him the role of a learner
and implies communication with other agents acting as 
critics. However, agents are not specialised and can in turn be
learners or critics, none of them being kept to a specific role.
Pieces of information are distributed among the agents, but
can be redundant. There is no central memory.
The work described here has its origin in a former work 
concerning learning in an intentional multi-agent system using
a BDI formalism [6]. In that work, agents had plans, each
of them being associated with a context defining in which
conditions it can be triggered. Plans (each of them having
its own context) were common to the whole set of agents
in the community. Agents had to adapt their plan contexts
depending on the failure or success of executed plans, using
a learning mechanism and asking other agents for examples
(plans successes or failures). However this work lacked a
collective learning protocol enabling a real autonomy of the
multi-agent system. The study of such a protocol is the 
object of the present paper.
In section 2 we formally define the mas-consistency of an
update mechanism for the whole MAS and we propose a
generic update mechanism proved to be mas consistent. In
section 3 we describe SMILE, an incremental multi agent
concept learner applying our mas consistent update 
mechanism to collaborative concept learning. Section 4 describes
various experiments on SMILE and discusses various issues
including how the accuracy and the simplicity of the current
hypothesis vary when comparing single agent learning and
mas learning. In section 5 we briefly present some related
works and then conclude in section 6 by discussing further
investigations on mas consistent learning.
