In this section, we evaluate our methodology that uses
Web search results for improving query classification.
3.1 Taxonomy
Our choice of taxonomy was guided by a Web 
advertising application. Since we want the classes to be useful for
matching ads to queries, the taxonomy needs to be 
elaborate enough to facilitate ample classification specificity. For
example, classifying all medical queries into one node will
likely result in poor ad matching, as both sore foot and
flu queries will end up in the same node. The ads 
appropriate for these two queries are, however, very different. To
avoid such situations, the taxonomy needs to provide 
sufficient discrimination between common commercial topics.
Therefore, in this paper we employ an elaborate taxonomy
of approximately 6000 nodes, arranged in a hierarchy with
median depth 5 and maximum depth 9. Figure 1 shows the
distribution of categories by taxonomy levels. Human 
editors populated the taxonomy with labeled queries (approx.
150 queries per node), which were used as a training set; a
small fraction of queries have been assigned to more than
one category.
3.2 Digression: the basics of sponsored search
To discuss our set of evaluation queries, we need a brief 
introduction to some basic concepts of Web advertising. 
Sponsored search (or paid search) advertising is placing textual
ads on the result pages of web search engines, with ads 
being driven by the originating query. All major search 
engines (Google, Yahoo!, and MSN) support such ads and act
simultaneously as a search engine and an ad agency. These
textual ads are characterized by one or more bid phrases
representing those queries where the advertisers would like
to have their ad displayed. (The name bid phrase comes
from the fact that advertisers bid various amounts to secure
their position in the tower of ads associated to a query. A
discussion of bidding and placement mechanisms is beyond
the scope of this paper [13].
However, many searches do not explicitly use phrases that
someone bids on. Consequently, advertisers also buy broad
matches, that is, they pay to place their advertisements
on queries that constitute some modification of the desired
bid phrase. In broad match, several syntactic modifications
can be applied to the query to match it to the bid phrase,
e.g., dropping or adding words, synonym substitution, etc.
These transformations are based on rules and dictionaries.
As advertisers tend to cover high-volume and high-revenue
queries, broad-match queries fall into the tail of the 
distribution with respect to both volume and revenue.
3.3 Data sets
We used two representative sets of 1000 queries. Both sets
contain queries that cannot be directly matched to 
advertisements, that is, none of the queries contains a bid phrase
(this means we eliminated practically all popular queries).
The first set of queries can be matched to at least one ad
using broad match as described above. Queries in the second
set cannot be matched even by broad match, and therefore
the search engine used in our study does not currently 
display any advertising for them. In a sense, these are even
more rare queries and further away from common queries.
As a measure of query rarity, we estimated their frequency
in a month worth of query logs for a major US search 
engine; the median frequency was 1 for queries in Set 1 and 0
for queries in Set 2.
The queries in the two sets differ in their classification
difficulty. In fact, queries in Set 2 are difficult to interpret
even for human evaluators. Queries in Set 1 have on average
3.50 words, with the longest one having 11 words; queries
in Set 2 have on average 4.39 words, with the longest query
of 81 words. Recent studies estimate the average length of
web queries to be just under 3 words2
, which is lower than
in our test sets. As another measure of query difficulty,
we measured the fraction of queries that contain quotation
marks, as the latter assist query interpretation by 
meaningfully grouping the words. Only 8% queries in Set 1 and 14%
in Set 2 contained quotation marks.
3.4 Methodology and evaluation metrics
The two sets of queries were classified into the target 
taxonomy using the techniques presented in section 2. Based
on the confidence values assigned, the top 3 classes for each
query were presented to human evaluators. These 
evaluators were trained editorial staff who possessed knowledge
about the taxonomy. The editors considered every 
queryclass pair, and rated them on the scale 1 to 4, with 1 
meaning the classification is highly relevant and 4 meaning it is
irrelevant for the query. About 2.4% queries in Set 1 and
5.4% queries in Set 2 were judged to be unclassifiable (e.g.,
random strings of characters), and were consequently 
excluded from evaluation. To compute evaluation metrics, we
treated classifications with ratings 1 and 2 to be correct, and
those with ratings 3 and 4 to be incorrect.
We used standard evaluation metrics: precision, recall and
F1. In what follows, we plot precision-recall graphs for all
the experiments. For comparison with other published 
studies, we also report precision and F1 values corresponding to
complete recall (R = 1). Owing to the lack of space, we
only show graphs for query Set 1; however, we show the
numerical results for both sets in the tables.
3.5 Results
We compared our method to a baseline query classifier
that does not use any external knowledge. Our baseline
classifier expanded queries using standard query expansion
techniques, grouped their terms using a phrase recognizer,
boosted certain phrases in the query based on their 
statistical properties, and performed classification using the
2

http://www.rankstat.com/html/en/seo-news1-most-peopleuse-2-word-phrases-in-search-engines.html
0.4
0.5
0.6
0.7
0.8
0.9
1
1.00.90.80.70.60.50.40.30.20.1
Precision
Recall
Baseline
Engine A full-page
Engine A summary
Engine B full-page
Engine B summary
Figure 2: The effect of external knowledge
nearest-neighbor approach. This baseline classifier is 
actually a production version of the query classifier running in a
major US search engine.
In our experiments, we varied values of pertinent 
parameters that characterize the exact way of using search results.
In what follows, we start with the general assessment of the
effect of using Web search results. We then proceed to 
exploring more refined techniques, such as using only search
summaries versus actually crawling the returned URLs. We
also experimented with using different numbers of search
results per query, as well as with varying the number of
classifications considered for each search result. For lack of
space, we only show graphs for Set 1 queries and omit the
graphs for Set 2 queries, which exhibit similar phenomena.
3.5.1 The effect of external knowledge
Queries by themselves are very short and difficult to 
classify. We use top search engine results for collecting 
background knowledge for queries. We employed two major US
search engines, and used their results in two ways, either
only summaries or the full text of crawled result pages.
Figure 2 and Table 1 show that such extra knowledge 
considerably improves classification accuracy. Interestingly, we
found that search engine A performs consistently better with
full-page text, while search engine B performs better when
summaries are used.
Engine Context Prec. F1 Prec. F1
Set 1 Set 1 Set 2 Set 2
A full-page 0.72 0.84 0.509 0.721
B full-page 0.706 0.827 0.497 0.665
A summary 0.586 0.744 0.396 0.572
B summary 0.645 0.788 0.467 0.638
Baseline 0.534 0.696 0.365 0.536
Table 1: The effect of using external knowledge
3.5.2 Aggregation techniques
There are two major ways to use search results as 
additional knowledge. First, individual results can be classified
separately, with subsequent voting among individual 
classifications. Alternatively, individual search results can be
bundled together as one meta-document and classified as
such using the document classifier. Figure 3 presents the
results of these two approaches When full-text pages are
used, the technique using individual classifications of search
results evidently outperforms the bundling approach by a
wide margin. However, in the case of summaries, bundling
together is found to be consistently better than individual
classification. This is because summaries by themselves are
too short to be classified correctly individually, but when
bundled together they are much more stable.
0.4
0.5
0.6
0.7
0.8
0.9
1
1.00.90.80.70.60.50.40.30.20.1
Precision
Recall
Baseline
Bundled full-page
Voting full-page
Bundled summary
Voting summary
Figure 3: Voting vs. Bundling
3.5.3 Full page text vs. summary
To summarize the two preceding sections, background
knowledge for each query is obtained by using either the
full-page text or only the summaries of the top search 
results. Full page text was found to be more in conjunction
with voted classification, while summaries were found to be
useful when bundled together. The best results overall were
obtained with full-page results classified individually, with
subsequent voting used to determine the final query 
classification. This observation differs from findings by Shen et al.
[20], who found summaries to be more useful. We attribute
this distinction to the fact that the queries we used in this
study are tail ones, which are rare and difficult to classify.
3.5.4 Varying the number of classes per search result
We also varied the number of classifications per search 
result, i.e., each result was permitted to have either 1, 3, or
5 classes. Figure 4 shows the corresponding precision-recall
graphs for both full-page and summary-only settings. As can
be readily seen, all three variants produce very similar 
results. However, the precision-recall curve for the 1-class 
experiment has higher fluctuations. Using 3 classes per search
result yields a more stable curve, while with 5 classes per
result the precision-recall curve is very smooth. Thus, as we
increase the number of classes per result, we observe higher
stability in query classification.
3.5.5 Varying the number of search results obtained
We also experimented with different numbers of search
results per query. Figure 5 and Table 2 present the results
of this experiment. In line with our intuition, we observed
that classification accuracy steadily rises as we increase the
number of search results used from 10 to 40, with a slight
drop as we continue to use even more results (50). This
is because using too few search results provides too little
external knowledge, while using too many results introduces
extra noise.
Using paired t-test, we assessed the statistical significance
0.4
0.5
0.6
0.7
0.8
0.9
1
1.00.90.80.70.60.50.40.30.20.1
Precision
Recall
Baseline
1 class full-page
3 classes full-page
5 classes full-page
1 class summary
3 classes summary
5 classes summary
Figure 4: Varying the number of classes per page
0.4
0.5
0.6
0.7
0.8
0.9
1
1.00.90.80.70.60.50.40.30.20.1
Precision
Recall
10
20
30
40
50
Baseline
Figure 5: Varying the number of results per query
of the improvements due to our methodology versus the
baseline. We found the results to be highly significant (p <
0.0005), thus confirming the value of external knowledge for
query classification.
3.6 Voting versus alternative methods
As explained in Section 2.2, one may use several methods
to classify queries from search engine results based on our
relevance model. As we have seen, the voting method works
quite well. In this section, we compare the performance of
voting top-ten search results to the following two methods:
• A: Discriminative learning of query-classification based
on logistic regression, described in Section 2.5.
• B: Learning weights based on quality score returned
by a search engine. We discretize the quality score
s(d, q) of a query/document pair into {high, medium,
low}, and learn the three weights w on a set of training
queries, and test the performance on holdout queries.
The classification formula, as explained at the end of
Section 2.4, is P(Cj|q) = d w(s(d, q))P(Cj|d).
Method B requires a training/testing split. Neither voting
nor method A requires such a split; however, for consistency,
we randomly draw 50-50 training/testing splits for ten times,
and report the mean performance ± standard deviation on
the test-split for all three methods. For this experiment,
instead of precision and recall, we use DCG-k (k = 1, 5),
popular in search engine evaluation. The DCG (discounted
cumulated gain) metric, described in [8], is a ranking 
measure where the system is asked to rank a set of candidates (in
Number of results Precision F1
baseline 0.534 0.696
10 0.706 0.827
20 0.751 0.857
30 0.796 0.886
40 0.807 0.893
50 0.798 0.887
Table 2: Varying the number of search results
our case, judged categories for each query), and computes
for each query q: DCGk(q) = k
i=1 g(Ci(q))/ log2(i + 1),
where Ci(q) is the i-th category for query q ranked by the
system, and g(Ci) is the grade of Ci: we assign grade of
10, 5, 1, 0 to the 4-point judgment scale described earlier to
compute DCG. The decaying choice of log2(i + 1) is 
conventional, which does not have particular importance. The
overall DCG of a system is the averaged DCG over queries.
We use this metric instead of precision/recall in this 
experiment because it can directly handle multi-grade output.
Therefore as a single metric, it is convenient for comparing
the methods. Note that precision/recall curves used in the
earlier sections yield some additional insights not 
immediately apparent from the DCG numbers.
Set 1
Method DCG-1 DCG-5
Oracle 7.58 ± 0.19 14.52 ± 0.40
Voting 5.28 ± 0.15 11.80 ± 0.31
Method A 5.48 ± 0.16 12.22 ± 0.34
Method B 5.36 ± 0.18 12.15 ± 0.35
Set 2
Method DCG-1 DCG-5
Oracle 5.69 ± 0.18 9.94 ± 0.32
Voting 3.50 ± 0.17 7.80 ± 0.28
Method A 3.63 ± 0.23 8.11 ± 0.33
Method B 3.55 ± 0.18 7.99 ± 0.31
Table 3: Voting and alternative methods
Results from our experiments are given in Table 3. The
oracle method is the best ranking of categories for each query
after seeing human judgments. It cannot be achieved by any
realistic algorithm, but is included here as an absolute 
upper bound on DCG performance. The simple voting method
performs very well in our experiments. The more 
complicated methods may lead to moderate performance gain
(especially method A, which uses discriminative training in
Section 2.5). However, both methods are computationally
more costly, and the potential gain is minor enough to be
neglected. This means that as a simple method, voting is
quite effective.
We can observe that method B, which uses quality score
returned by a search engine to adjust importance weights
of returned pages for a query, does not yield appreciable
improvement. This implies that putting equal weights 
(voting) performs similarly as putting higher weights to higher
quality documents and lower weights to lower quality 
documents (method B), at least for the top search results. It
may be possible to improve this method by including other
page-features that can differentiate top-ranked search 
results. However, the effectiveness will require further 
investigation which we did not test. We may also observe that
the performance on Set 2 is lower than that on Set 1, which
means queries in Set 2 are harder than those in Set 1.
3.7 Failure analysis
We scrutinized the cases when external knowledge did
not improve query classification, and identified three main
causes for such lack of improvement. (1)Queries containing
random strings, such as telephone numbers - these queries
do not yield coherent search results, and so the latter cannot
help classification (around 5% of queries were of this kind).
(2) Queries that yield no search results at all; there were 8%
such queries in Set 1 and 15% in Set 2. (3) Queries 
corresponding to recent events, for which the search engine did
not yet have ample coverage (around 5% of queries). One
notable example of such queries are entire names of news
articles-if the exact article has not yet been indexed by
the search engine, search results are likely to be of little use.
