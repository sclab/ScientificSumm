CONTEXTSENSITIVEINFORMATIONRETRIEVAL
Intuitively, the query history HQ and clickthrough history HC
are both useful for improving search accuracy for the current query
Qk. An important research question is how we can exploit such 
information effectively. We propose to use statistical language 
models to model a user"s information need and develop four specific
context-sensitive language models to incorporate context 
information into a basic retrieval model.
3.1 Basic retrieval model
We use the Kullback-Leibler (KL) divergence method [19] as
our basic retrieval method. According to this model, the retrieval
task involves computing a query language model θQ for a given
query and a document language model θD for a document and then
computing their KL divergence D(θQ||θD), which serves as the
score of the document.
One advantage of this approach is that we can naturally 
incorporate the search context as additional evidence to improve our 
estimate of the query language model.
Formally, let HQ = (Q1, ..., Qk−1) be the query history and
the current query be Qk. Let HC = (C1, ..., Ck−1) be the 
clickthrough history. Note that Ci is the concatenation of all clicked
documents" summaries in the i-th round of retrieval since we may
reasonably treat all these summaries equally. Our task is to estimate
a context query model, which we denote by p(w|θk), based on the
current query Qk, as well as the query history HQ and clickthrough
history HC . We now describe several different language models for
exploiting HQ and HC to estimate p(w|θk). We will use c(w, X)
to denote the count of word w in text X, which could be either a
query or a clicked document"s summary or any other text. We will
use |X| to denote the length of text X or the total number of words
in X.
3.2 Fixed Coefficient Interpolation (FixInt)
Our first idea is to summarize the query history HQ with a 
unigram language model p(w|HQ) and the clickthrough history HC
with another unigram language model p(w|HC ). Then we linearly
interpolate these two history models to obtain the history model
p(w|H). Finally, we interpolate the history model p(w|H) with
the current query model p(w|Qk). These models are defined as
follows.
p(w|Qi) =
c(w, Qi)
|Qi|
p(w|HQ) =
1
k − 1
i=k−1
i=1
p(w|Qi)
p(w|Ci) =
c(w, Ci)
|Ci|
p(w|HC ) =
1
k − 1
i=k−1
i=1
p(w|Ci)
p(w|H) = βp(w|HC ) + (1 − β)p(w|HQ)
p(w|θk) = αp(w|Qk) + (1 − α)p(w|H)
where β ∈ [0, 1] is a parameter to control the weight on each 
history model, and where α ∈ [0, 1] is a parameter to control the
weight on the current query and the history information.
If we combine these equations, we see that
p(w|θk) = αp(w|Qk) + (1 − α)[βp(w|HC ) + (1 − β)p(w|HQ)]
That is, the estimated context query model is just a fixed coefficient
interpolation of three models p(w|Qk), p(w|HQ), and p(w|HC ).
3.3 Bayesian Interpolation (BayesInt)
One possible problem with the FixInt approach is that the 
coefficients, especially α, are fixed across all the queries. But intuitively,
if our current query Qk is very long, we should trust the current
query more, whereas if Qk has just one word, it may be beneficial
to put more weight on the history. To capture this intuition, we treat
p(w|HQ) and p(w|HC ) as Dirichlet priors and Qk as the observed
data to estimate a context query model using Bayesian estimator.
The estimated model is given by
p(w|θk) =
c(w, Qk) + µp(w|HQ) + νp(w|HC )
|Qk| + µ + ν
=
|Qk|
|Qk| + µ + ν
p(w|Qk)+
µ + ν
|Qk| + µ + ν
[
µ
µ + ν
p(w|HQ)+
ν
µ + ν
p(w|HC )]
where µ is the prior sample size for p(w|HQ) and ν is the prior
sample size for p(w|HC ). We see that the only difference between
BayesInt and FixInt is the interpolation coefficients are now 
adaptive to the query length. Indeed, when viewing BayesInt as FixInt,
we see that α = |Qk|
|Qk|+µ+ν
, β = ν
ν+µ
, thus with fixed µ and ν,
we will have a query-dependent α. Later we will show that such an
adaptive α empirically performs better than a fixed α.
3.4 Online Bayesian Updating (OnlineUp)
Both FixInt and BayesInt summarize the history information by
averaging the unigram language models estimated based on 
previous queries or clicked summaries. This means that all previous
queries are treated equally and so are all clicked summaries. 
However, as the user interacts with the system and acquires more 
knowledge about the information in the collection, presumably, the 
reformulated queries will become better and better. Thus assigning 
decaying weights to the previous queries so as to trust a recent query
more than an earlier query appears to be reasonable. Interestingly,
if we incrementally update our belief about the user"s information
need after seeing each query, we could naturally obtain decaying
weights on the previous queries. Since such an incremental online
updating strategy can be used to exploit any evidence in an 
interactive retrieval system, we present it in a more general way.
In a typical retrieval system, the retrieval system responds to
every new query entered by the user by presenting a ranked list
of documents. In order to rank documents, the system must have
some model for the user"s information need. In the KL divergence
retrieval model, this means that the system must compute a query
model whenever a user enters a (new) query. A principled way of
updating the query model is to use Bayesian estimation, which we
discuss below.
3.4.1 Bayesian updating
We first discuss how we apply Bayesian estimation to update a
query model in general. Let p(w|φ) be our current query model
and T be a new piece of text evidence observed (e.g., T can be a
query or a clicked summary). To update the query model based on
T, we use φ to define a Dirichlet prior parameterized as
Dir(µT p(w1|φ), ..., µT p(wN |φ))
where µT is the equivalent sample size of the prior. We use 
Dirichlet prior because it is a conjugate prior for multinomial 
distributions. With such a conjugate prior, the predictive distribution of φ
(or equivalently, the mean of the posterior distribution of φ is given
by
p(w|φ) =
c(w, T) + µT p(w|φ)
|T| + µT
(1)
where c(w, T) is the count of w in T and |T| is the length of T.
Parameter µT indicates our confidence in the prior expressed in
terms of an equivalent text sample comparable with T. For 
example, µT = 1 indicates that the influence of the prior is equivalent to
adding one extra word to T.
3.4.2 Sequential query model updating
We now discuss how we can update our query model over time
during an interactive retrieval process using Bayesian estimation.
In general, we assume that the retrieval system maintains a current
query model φi at any moment. As soon as we obtain some implicit
feedback evidence in the form of a piece of text Ti, we will update
the query model.
Initially, before we see any user query, we may already have
some information about the user. For example, we may have some
information about what documents the user has viewed in the past.
We use such information to define a prior on the query model,
which is denoted by φ0. After we observe the first query Q1, we
can update the query model based on the new observed data Q1.
The updated query model φ1 can then be used for ranking 
documents in response to Q1. As the user views some documents, the
displayed summary text for such documents C1 (i.e., clicked 
summaries) can serve as some new data for us to further update the
query model to obtain φ1. As we obtain the second query Q2 from
the user, we can update φ1 to obtain a new model φ2. In general,
we may repeat such an updating process to iteratively update the
query model.
Clearly, we see two types of updating: (1) updating based on a
new query Qi; (2) updating based on a new clicked summary Ci. In
both cases, we can treat the current model as a prior of the context
query model and treat the new observed query or clicked summary
as observed data. Thus we have the following updating equations:
p(w|φi) =
c(w, Qi) + µip(w|φi−1)
|Qi| + µi
p(w|φi) =
c(w, Ci) + νip(w|φi)
|Ci| + νi
where µi is the equivalent sample size for the prior when updating
the model based on a query, while νi is the equivalent sample size
for the prior when updating the model based on a clicked summary.
If we set µi = 0 (or νi = 0) we essentially ignore the prior model,
thus would start a completely new query model based on the query
Qi (or the clicked summary Ci). On the other hand, if we set µi =
+∞ (or νi = +∞) we essentially ignore the observed query (or
the clicked summary) and do not update our model. Thus the model
remains the same as if we do not observe any new text evidence. In
general, the parameters µi and νi may have different values for
different i. For example, at the very beginning, we may have very
sparse query history, thus we could use a smaller µi, but later as the
query history is richer, we can consider using a larger µi. But in
our experiments, unless otherwise stated, we set them to the same
constants, i.e., ∀i, j, µi = µj, νi = νj.
Note that we can take either p(w|φi) or p(w|φi) as our context
query model for ranking documents. This suggests that we do not
have to wait until a user enters a new query to initiate a new round
of retrieval; instead, as soon as we collect clicked summary Ci, we
can update the query model and use p(w|φi) to immediately rerank
any documents that a user has not yet seen.
To score documents after seeing query Qk, we use p(w|φk), i.e.,
p(w|θk) = p(w|φk)
3.5 Batch Bayesian updating (BatchUp)
If we set the equivalent sample size parameters to fixed 
constant, the OnlineUp algorithm would introduce a decaying factor
- repeated interpolation would cause the early data to have a low
weight. This may be appropriate for the query history as it is 
reasonable to believe that the user becomes better and better at query
formulation as time goes on, but it is not necessarily appropriate for
the clickthrough information, especially because we use the 
displayed summary, rather than the actual content of a clicked 
document. One way to avoid applying a decaying interpolation to
the clickthrough data is to do OnlineUp only for the query history
Q = (Q1, ..., Qi−1), but not for the clickthrough data C. We first
buffer all the clickthrough data together and use the whole chunk
of clickthrough data to update the model generated through 
running OnlineUp on previous queries. The updating equations are as
follows.
p(w|φi) =
c(w, Qi) + µip(w|φi−1)
|Qi| + µi
p(w|ψi) =
i−1
j=1 c(w, Cj) + νip(w|φi)
i−1
j=1 |Cj| + νi
where µi has the same interpretation as in OnlineUp, but νi now
indicates to what extent we want to trust the clicked summaries. As
in OnlineUp, we set all µi"s and νi"s to the same value. And to rank
documents after seeing the current query Qk, we use
p(w|θk) = p(w|ψk)
