Our goal is to segment documents and align the segments
across documents (Figure 1). Let T be the set of terms
{t1, t2, ..., tl}, which appear in the unlabelled set of 
documents D = {d1, d2, ..., dm}. Let Sd be the set of sentences
for document d ∈ D, i.e.{s1, s2, ..., snd }. We have a 3D 
matrix of term frequency, in which the three dimensions are
random variables of D, Sd, and T. Sd actually is a random
vector including a random variable for each d ∈ D. The
term frequency can be used to estimate the joint probability
distribution P(D, Sd, T), which is p(t, d, s) = T(t, d, s)/ND,
where T(t, d, s) is the number of t in d"s sentence s and ND
is the total number of terms in D. ˆS represents the set of
segments {ˆs1, ˆs2, ..., ˆsp} after segmentation and alignment
among multiple documents, where the number of segments
| ˆS| = p. A segment ˆsi of document d is a sequence of 
adjacent sentences in d. Since for different documents si may
discuss different sub-topics, our goal is to cluster adjacent
sentences in each document into segments, and align similar
segments among documents, so that for different documents
ˆsi is about the same sub-topic. The goal is to find the 
optimal topic segmentation and alignment mapping
Segd(si) : {s1, s2, ..., snd } → {ˆs1, ˆs2, ..., ˆsp}
and Alid(ˆsi) : {ˆs1, ˆs2, ..., ˆsp} → {ˆs1, ˆs2, ..., ˆsp}, for all d ∈
D, where ˆsi is ith
segment with the constraint that only
adjacent sentences can be mapped to the same segment,
i.e. for d, {si, si+1, ..., sj} → {ˆsq}, where q ∈ {1, ..., p},
where p is the segment number, and if i > j, then for d,
ˆsq is missing. After segmentation and alignment, random
vector Sd becomes an aligned random variable ˆS. Thus,
P(D, Sd, T) becomes P(D, ˆS, T).
Term co-clustering is a technique that has been employed
[10] to improve the accuracy of document clustering. We
evaluate the effect of it for topic segmentation. A term t
is mapped to exactly one term cluster. Term co-clustering
involves simultaneously finding the optimal term clustering
mapping Clu(t) : {t1, t2, ..., tl} → {ˆt1, ˆt2, ..., ˆtk}, where k ≤
l, l is the total number of words in all the documents, and
k is the number of clusters.
