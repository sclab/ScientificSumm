Many researchers have worked on topic detection and 
tracking (TDT) [26] and topic segmentation during the past decade.
Topic segmentation intends to identify the boundaries in a
document with the goal to capture the latent topical 
structure. Topic segmentation tasks usually fall into two 
categories [15]: text stream segmentation where topic transition
is identified, and coherent document segmentation in which
documents are split into sub-topics. The former category
has applications in automatic speech recognition, while the
latter one has more applications such as partial-text query
of long documents in information retrieval, text summary,
and quality measurement of multiple documents. Previous
research in connection with TDT falls into the former 
category, targeted on topic tracking of broadcast speech data
and newswire text, while the latter category has not been
studied very well.
Traditional approaches perform topic segmentation on 
documents one at a time [15, 25, 6]. Most of them perform
badly in subtle tasks like coherent document segmentation
[15]. Often, end-users seek documents that have the 
similar content. Search engines, like, Google, provide links to
obtain similar pages. At a finer granularity, users may 
actually be looking to obtain sections of a document similar
to a particular section that presumably discusses a topic of
the users interest. Thus, the extension of topic 
segmentation from single documents to identifying similar segments
from multiple similar documents with the same topic is a
natural and necessary direction, and multi-document topic
segmentation is expected to have a better performance since
more information is utilized.
Traditional approaches using similarity measurement based
on term frequency generally have the same assumption that
similar vocabulary tends to be in a coherent topic segment
[15, 25, 6]. However, they usually suffer the issue of 
identifying stop words. For example, additional document-dependent
stop words are removed together with the generic stop words
in [15]. There are two reasons that we do not remove stop
words directly. First, identifying stop words is another 
issue [12] that requires estimation in each domain. Removing
common stop words may result in the loss of useful 
information in a specific domain. Second, even though stop words
can be identified, hard classification of stop words and 
nonstop words cannot represent the gradually changing amount
of information content of each word. We employ a soft 
classification using term weights.
In this paper, we view the problem of topic segmentation
as an optimization issue using information theoretic 
techniques to find the optimal boundaries of a document given
the number of text segments so as to minimize the loss of
mutual information (MI) (or a weighted mutual information
(WMI)) after segmentation and alignment. This is equal to
maximizing the MI (or WMI). The MI focuses on 
measuring the difference among segments whereas previous research
focused on finding the similarity (e.g. cosine distance) of
segments [15, 25, 6]. Topic alignment of multiple similar
documents can be achieved by clustering sentences on the
same topic into the same cluster. Single-document topic
segmentation is just a special case of the multi-document
topic segmentation and alignment problem. Terms can be
co-clustered as in [10] at the same time, given the number of
clusters, but our experimental results show that this method
results in a worse segmentation (see Tables 1, 4, and 6). 
Usually, human readers can identify topic transition based on
cue words, and can ignore stop words. Inspired by this, we
give each term (or term cluster) a weight based on entropy
among different documents and different segments of 
documents. Not only can this approach increase the contribution
of cue words, but it can also decrease the effect of common
stop words, noisy word, and document-dependent stop words.
These words are common in a document. Many methods
based on sentence similarity require that these words are
removed before topic segmentation can be performed [15].
Our results in Figure 3 show that term weights are useful
for multi-document topic segmentation and alignment.
The major contribution of this paper is that it introduces
a novel method for topic segmentation using MI and shows
that this method performs better than previously used 
criteria. Also, we have addressed the problem of topic 
segmentation and alignment across multiple documents, whereas
most existing research focused on segmentation of single
documents. Multi-document segmentation and alignment
can utilize information from similar documents and improves
the performance of topic segmentation greatly. Obviously,
our approach can handle single documents as a special case
when multiple documents are unavailable. It can detect
shared topics among documents to judge if they are multiple
documents on the same topic. We also introduce the new
criterion of WMI based on term weights learned from 
multiple similar documents, which can improve performance of
topic segmentation further. We propose an iterative greedy
algorithm based on dynamic programming and show that it
works well in practice. Some of our prior work is in [24].
The rest of this paper is organized as follows: In Section 2,
we review related work. Section 3 contains a formulation of
the problem of topic segmentation and alignment of multiple
documents with term co-clustering, a review of the criterion
of MI for clustering, and finally an introduction to WMI. In
Section 4, we first propose the iterative greedy algorithm of
topic segmentation and alignment with term co-clustering,
and then describe how the algorithm can be optimized by 
usFigure 1: Illustration of multi-document 
segmentation and alignment.
ing dynamic programming. In Section 5, experiments about
single-document segmentation, shared topic detection, and
multi-document segmentation are described, and results are
presented and discussed to evaluate the performance of our
algorithm. Conclusions and some future directions of the
research work are discussed in Section 6.
