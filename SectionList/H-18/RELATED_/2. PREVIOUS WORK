Generally, the existing approaches to text segmentation
fall into two categories: supervised learning [19, 17, 23]
and unsupervised learning [3, 27, 5, 6, 15, 25, 21]. 
Supervised learning usually has good performance, since it learns
functions from labelled training sets. However, often 
getting large training sets with manual labels on document
sentences is prohibitively expensive, so unsupervised 
approaches are desired. Some models consider dependence 
between sentences and sections, such as Hidden Markov Model
[3, 27], Maximum Entropy Markov Model [19], and 
Conditional Random Fields [17], while many other approaches are
based on lexical cohesion or similarity of sentences [5, 6, 15,
25, 21]. Some approaches also focus on cue words as hints
of topic transitions [11]. While some existing methods only
consider information in single documents [6, 15], others 
utilize multiple documents [16, 14]. There are not many works
in the latter category, even though the performance of 
segmentation is expected to be better with utilization of 
information from multiple documents. Previous research studied
methods to find shared topics [16] and topic segmentation
and summarization between just a pair of documents [14].
Text classification and clustering is a related research area
which categorizes documents into groups using supervised or
unsupervised methods. Topical classification or clustering is
an important direction in this area, especially co-clustering
of documents and terms, such as LSA [9], PLSA [13], and
approaches based on distances and bipartite graph 
partitioning [28] or maximum MI [2, 10], or maximum entropy
[1, 18]. Criteria of these approaches can be utilized in the 
issue of topic segmentation. Some of those methods have been
extended into the area of topic segmentation, such as PLSA
[5] and maximum entropy [7], but to our best knowledge,
using MI for topic segmentation has not been studied.
