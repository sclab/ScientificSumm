In this section, we discuss the requirements of the 
interaction protocols that govern the exchange of messages between
agents, and provide some example instantiation of such 
protocols. To clarify the presentation, we distinguish two 
levels: the local level, which is concerned with the regulation
of bilateral exchanges; and the global level,which essentially
regulates the way agents can actually engage into a 
conversation. At each level, we separate what is specified by the
protocol, and what is left to agents" strategies.
Local Protocol and Strategies
We start by inspecting local protocols and strategies that
will regulate the communication between the agents of the
system. As we limit ourselves to bilateral communication,
these protocols will simply involve two agents. Such protocol
will have to meet one basic requirement to be satisfying.
• consistency (CONS)- a local protocol has to 
guarantee the mutual consistency of agents upon termination
(which implies termination of course).
Figure 1: A Hypotheses Exchange Protocol [1]
One example such protocol is the protocol described in [1]
that is pictured in Fig. 1. To further illustrate how such 
protocol can be used by agents, we give some details on a 
possible strategy: upon receiving a hypothesis h1 (propose(h1) or
counterpropose(h1)) from a1, agent a2 is in state 2 and has
the following possible replies: counterexample (if the agent
knows an example contradicting the hypothesis, or not 
explained by this hypothesis), challenge (if the agents lacks
evidence to accept this hypothesis), counterpropose (if the
agent agrees with the hypothesis but prefers another one),
or accept (if it is indeed as good as its favourite 
hypothesis). This strategy guarantees, among other properties, the
eventual mutual logical consistency of the involved agents
[1].
Global Protocol
The global protocol regulates the way bilateral exchanges
will be initiated between agents. At each turn, agents will
concurrently send one weighted request to communicate to
other agents. This weight is a value measuring the agent"s
willingness to converse with the targeted agent (in practice,
this can be based on different heuristics, but we shall make
some assumptions on agents" strategies, see below). Sending
such a request is a kind of conditional commitment for the
agent. An agent sending a weighted request commits to
engage in conversation with the target if he does not receive
and accept himself another request. Once all request have
been received, each agent replies with either an acccept or
a reject. By answering with an accept, an agent makes a
full commitment to engage in conversation with the sender.
Therefore, it can only send one accept in a given round, as an
agent can only participate in one conversation per time step.
When all response have been received, each agent receiving
an accept can either initiate a conversation using the local
protocol or send a cancel if it has accepted another request.
At the end of all the bilateral exchanges, the agents 
engaged in conversation are discarded from the protocol. Then
each of the remaining agents resends a request and the 
process iterates until no more requests are sent.
Global Strategy
We now define four requirements for the strategies used by
agents, depending on their role in the protocol: two are
concerned with the requestee role (how to decide who the
1000 The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07)
agent wishes to communicate with?), the other two with the
responder role (how to decide which communication request
to accept or not?).
• Willingness to solve inconsistancies (SOLVE)-agents
want to communicate with any other agents unless
they know they are mutually consistent.
• Focus on solving inconsistencies (FOCUS)-agents do
not request communication with an agent with whom
they know they are mutually consistent.
• Willingness to communicate (COMM)-agents cannot
refuse a weighted communication request, unless they
have just received or send a request with a greater
weight.
• Commitment to communication request 
(REQU)agents cannot accept a weighted communication 
request if they have themselves sent a communication
request with a greater weight. Therefore, they will
not cancel their request unless they have received a
communicational request with greater weight.
Now the protocol structure, together with the properties
COMM+REQU, ensure that a request can only be rejected
if its target agent engages in communication with another
agent. Suppose indeed that agent ai wants to communicate
with aj by sending a request with weight w. COMM 
guarantees that an agent receiving a weighted request will either
accept this communication, accept a communication with a
greater weight or wait for the answer to a request with a
greater weight. This ensures that the request with 
maximal weight will be accepted and not cancelled (as REQU
ensures that an agent sending a request can only cancel it if
he accepts another request with greater weight). Therefore
at least two agents will engage in conversation per round
of the global protocol. As the protocol ensures that ai can
resend its request while aj is not engaged in a conversation,
there will be a turn in which aj must engage in a 
conversation, either with ai or another agent.
These requirements concern request sending and 
acceptation, but agents also need some strategy of weight 
attribution. We describe below an altruist strategy, used in our
experiments. Being cooperative, an agent may want to know
more of the communication wishes of other agents in order to
improve the overall allocation of exchanges to agents. A 
context request step is then added to the global protocol. Before
sending their chosen weighted request, agents attribute a
weight to all agents they are prepared to communicate with,
according to some internal factors. In the simplest case, this
weight will be 1 for all agent with whom the agent is not
sure of being mutually consistent (ensuring SOLVE), other
agent being not considered for communication (ensuring 
FOCUS). The agent then sends a context request to all agents
with whom communication is considered. This request also
provides information about the sender (list of considered
communications along with their weight). After reception
of all the context requests, agents will either reply with a
deny, iff they are already engaged in a conversation (in which
case, the requesting agent will not consider communication
with them anymore in this turn), or an inform giving the
requester information about the requests it has sent and 
received. When all replies have been received, each agent can
calculate the weight of all requests concerning it. It does so
by substracting from the weight of its request the weight of
all requests concerning either it or its target (that is, the 
final weight of the request from ai to aj is Wi,j = wi,j +wj,i −
(
P
k∈R(i)−{j} wi,k +
P
k∈S(i)−{j} wk,i +
P
k∈R(j)−{i} wj,k +
P
k∈S(j)−{i} wk,j) where wi,j is the weight of the request of
ai to aj, R(i) is the set of indice of agents having received a
request from ai and S(i) is the set of indice of agents 
having send a request to ai). It then finally sends a weighted
request to the agents who maximise this weight (or wait for
a request) as described in the global protocol.
4. (CONDITIONAL) CONVERGENCE TO
GLOBAL CONSISTENCY
In this section we will show that the requirements 
regarding protocols and strategies just discussed will be sufficient
to ensure that the system will eventually converge towards
global consistency, under some conditions. We first show
that, if two agents are not mutually consistent at some time,
then there will be necessarily a time in the future such that
an agent will learn a new observation, being it because it is
new for the system, or by learning it from another agent.
Lemma 1. Let S be a system populated by n agents
a1, a2, ..., an, temporaly connex, and involving bounded 
perceptions for these agents. Let n1 be the sum of 
cardinalities of the intersection of pairwise observation sets.
(n1 =
P
(i,j)∈[1,n]2 |Oi ∩ Oj|) Let n2 be the cardinality of
the union of all agents" observations sets. (n2 = | ∪N
i=1 Oi|).
If ¬MCons(ai, aj) at time t0, there is necessarily a time
t > t0 s.t. either n1 or n2 will increase.
Proof. Suppose that there exist a time t0
and indices (i, j) s.t. ¬MCons(ai, aj). We will
use mt0 =
P
(k,l)∈[1,n]2 εComm(ak, al, t0) where
εComm(ak, al, t0) = 1 if ak and al have 
communicated at least once since t0, and 0 otherwise. 
Temporal connexity guarantees that there exist t1, ..., tm+1
and k1, ..., km s.t. C(ai, ak1 , t1), C(akm , aj, tm+1), and
∀p ∈ [1, m], C(akp , akp+1 , tp). Clearly, if MCons(ai, ak1 ),
MCons(akm , aj) and ∀p, MCons(akp , akp+1 ), we have
MCons(ai, aj) which contradicts our hypothesis (MCons
being transitive, MCons(ai, ak1 )∧MCons(ak1 , ak2 ) implies
that MCons(ai, ak2 ) and so on till MCons(ai, akm )∧
MCons(akm , aj) which implies MCons(ai, aj) ).
At least two agents are then necessarily 
inconsistent (¬MCons(ai, ak1 ), or ¬MCons(akm , aj), or ∃p0 t.q.
¬MCons(akp0
, akp0+1 )). Let ak and al be these two 
neighbours at a time t > t0
1
. The SOLVE property ensures
that either ak or al will send a communication request to
the other agent at time t . As shown before, this in turn
ensures that at least one of these agents will be involved in
a communication. Then there are two possibilities:
(case i) ak and al communicate at time t . In this case,
we know that ¬MCons(ak, al). This and the CONS 
property ensures that at least one of the agents must change its
1
Strictly speaking, the transitivity of MCons only ensure
that ak and al are inconsistent at a time t ≥ t0 that can
be different from the time t at which they can 
communicate. But if they become consistent between t and t (or
inconsistent between t and t ), it means that at least one
of them have changed its hypothesis between t and t , that
is, after t0. We can then apply the reasoning of case iib.
The Sixth Intl. Joint Conf. on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1001
hypothesis, which in turn, since agents are autonomous, 
implies at least one exchange of observation. But then |Ok ∩Ol|
is bound to increase: n1(t ) > n1(t0).
(case ii) ak communicates with ap at time t . We then have
again two possibilities:
(case iia) ak and ap did not communicate since t0. But then
εComm(ak, ap, t0) had value 0 and takes value 1. Hence mt0
increases.
(case iib) ak and ap did communicate at some time t0 >
t0. The CONS property of the protocol ensures that
MCons(ak, ap) at that time. Now the fact that they 
communicate and FOCUS implies that at least one of them did
change its hypothesis in the meantime. The fact that agents
are autonomous implies in turn that a new observation 
(perceived or received from another agent) necessarily provoked
this change. The latter case would ensure the existence of a
time t > t0 and an agent aq s.t. either |Op ∩Oq| or |Ok ∩Oq|
increases of 1 at that time (implying n1(t ) > n1(t0)). The
former case means that the agent gets a new perception o
at time t . If that observation was unknown in the system
before, then n2(t ) > n2(t0). If some agent aq already knew
this observation before, then either Op ∩ Oq or Ok ∩ Oq 
increases of 1 at time t (which implies that n1(t ) > n1(t0)).
Hence, ¬MCons(ai, aj) at time t0 guarantees that, either:
−∃t > t0 t.q. n1(t ) > n1(t0); or
−∃t > t0 t.q. n2(t ) > n2(t0); or
−∃t > t0 t.q. mt0 increases of 1 at time t .
By iterating the reasoning with t (but keeping t0 as the
time reference for mt0 ), we can eliminate the third case
(mt0 is integer and bounded by n2
, which means that 
after a maximum of n2
iterations, we necessarily will be in
one of two other cases.) As a result, we have proven that if
¬MCons(ai, aj) at time t0, there is necessarily a time t s.t.
either n1 or n2 will increase.
Theorem 1 (Global consistency). Let S be a 
system populated by n agents a1, a2, ..., an, temporaly connex,
and involving bounded perceptions for these agents. Let
Cons(ai, aj) be a transitive consistency property. Then any
protocol and strategies satisfying properties CONS, SOLVE,
FOCUS, COMM and REQU guarantees that the system will
converge towards global consistency.
Proof. For the sake of contradiction, let us assume
∃I, J ∈ [1, N] s.t. ∀t, ∃t0 > t, t.q. ¬Cons(aI , aJ , t0).
Using the lemma, this implies that ∃t > t0 s.t. either
n1(t ) > n1(t0) or n2(t ) > n2(t0). But we can apply
the same reasoning taking t = t , which would give us
t1 > t > t0 s.t. ¬Cons(aI , aJ , t1), which gives us t > t1
s.t. either n1(t ) > n1(t1) or n2(t ) > n2(t1). By 
successive iterations we can then construct a sequence t0, t1, ..., tn,
which can be divided in two sub-sequences t0, t1, ...tn and
t0 , t1 , ..., tn s.t. n1(t0) < n1(t1) < ... < n1(tn) and
n2(t0 ) < n2(t1 ) < ... < n2(tn). One of these sub-sequences
has to be infinite. However, n1(ti) and n2(ti ) are strictly
growing, integer, and bounded, which implies that both are
finite. Contradiction.
What the previous result essentially shows is that, in a
system where no agent will be isolated from the rest of the
agents for ever, only very mild assumptions on the protocols
and strategies used by agents suffice to guarantee 
convergence towards system consistency in a finite amount of time
(although it might take very long). Unfortunately, in many
critical situations, it will not be possible to assume this
temporal connexity. As distributed approaches as the one
advocated in this paper are precisely often presented as a
good way to tackle problems of reliability or problems of 
dependence to a center that are of utmost importance in these
critical applications, it is certainly interesting to further 
explore how such a system would behave when we relax this
assumption.
