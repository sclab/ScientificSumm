Having described the proxy-level index structure, we turn to the
mechanisms at the sensor tier. TSAR implements two key 
mechanisms at the sensor tier. The first is a local archival store at each
sensor node that is optimized for resource-constrained devices. The
second is an adaptive summarization technique that enables each
sensor to adapt to changing data and query characteristics. The rest
of this section describes these mechanisms in detail.
4.1 Local Storage at Sensors
Interval skip graphs provide an efficient mechanism to lookup
sensor nodes containing data relevant to a query. These queries are
then routed to the sensors, which locate the relevant data records
in the local archive and respond back to the proxy. To enable such
lookups, each sensor node in TSAR maintains an archival store of
sensor data. While the implementation of such an archival store
is straightforward on resource-rich devices that can run a database,
sensors are often power and resource-constrained. Consequently,
the sensor archiving subsystem in TSAR is explicitly designed to
exploit characteristics of sensor data in a resource-constrained 
setting.
Timestamp
Calibration
Parameters
Opaque DataData/Event Attributes size
Figure 6: Single storage record
Sensor data has very distinct characteristics that inform our 
design of the TSAR archival store. Sensors produce time-series data
streams, and therefore, temporal ordering of data is a natural and
simple way of storing archived sensor data. In addition to 
simplicity, a temporally ordered store is often suitable for many sensor data
processing tasks since they involve time-series data processing. 
Examples include signal processing operations such as FFT, wavelet
transforms, clustering, similarity matching, and target detection.
Consequently, the local archival store is a collection of records,
designed as an append-only circular buffer, where new records are
appended to the tail of the buffer. The format of each data record is
shown in Figure 6. Each record has a metadata field which includes
a timestamp, sensor settings, calibration parameters, etc. Raw 
sensor data is stored in the data field of the record. The data field
is opaque and application-specific-the storage system does not
know or care about interpreting this field. A camera-based sensor,
for instance, may store binary images in this data field. In order
to support a variety of applications, TSAR supports variable-length
data fields; as a result, record sizes can vary from one record to
another.
Our archival store supports three operations on records: create,
read, and delete. Due to the append-only nature of the store, 
creation of records is simple and efficient. The create operation simply
creates a new record and appends it to the tail of the store. Since
records are always written at the tail, the store need not maintain
a free space list. All fields of the record need to be specified at
creation time; thus, the size of the record is known a priori and the
store simply allocates the the corresponding number of bytes at the
tail to store the record. Since writes are immutable, the size of a
record does not change once it is created.
proxy
proxy
proxy
record
3 record
summary
local archive in
flash memory
data summary
start,end offset
time interval
sensor
summary
sent to proxy
Insert summaries
into interval skip graph
Figure 7: Sensor Summarization
45
The read operation enables stored records to be retrieved in 
order to answer queries. In a traditional database system, efficient
lookups are enabled by maintaining a structure such as a B-tree that
indexes certain keys of the records. However, this can be quite 
complex for a small sensor node with limited resources. Consequently,
TSAR sensors do not maintain any index for the data stored in their
archive. Instead, they rely on the proxies to maintain this metadata
index-sensors periodically send the proxy information 
summarizing the data contained in a contiguous sequence of records, as well
as a handle indicating the location of these records in flash memory.
The mechanism works as follows: In addition to the summary
of sensor data, each node sends metadata to the proxy containing
the time interval corresponding to the summary, as well as the start
and end offsets of the flash memory location where the raw data
corresponding is stored (as shown in Figure 7). Thus, random 
access is enabled at granularity of a summary-the start offset of each
chunk of records represented by a summary is known to the proxy.
Within this collection, records are accessed sequentially. When a
query matches a summary in the index, the sensor uses these offsets
to access the relevant records on its local flash by sequentially 
reading data from the start address until the end address. Any 
queryspecific operation can then be performed on this data. Thus, no
index needs to be maintained at the sensor, in line with our goal
of simplifying sensor state management. The state of the archive
is captured in the metadata associated with the summaries, and is
stored and maintained at the proxy.
While we anticipate local storage capacity to be large, eventually
there might be a need to overwrite older data, especially in high
data rate applications. This may be done via techniques such as
multi-resolution storage of data [9], or just simply by overwriting
older data. When older data is overwritten, a delete operation is
performed, where an index entry is deleted from the interval skip
graph at the proxy and the corresponding storage space in flash
memory at the sensor is freed.
4.2 Adaptive Summarization
The data summaries serve as glue between the storage at the 
remote sensor and the index at the proxy. Each update from a sensor
to the proxy includes three pieces of information: the summary, a
time period corresponding to the summary, and the start and end
offsets for the flash archive. In general, the proxy can index the
time interval representing a summary or the value range reported
in the summary (or both). The former index enables quick lookups
on all records seen during a certain interval, while the latter index
enables quick lookups on all records matching a certain value.
As described in Section 2.4, there is a trade-off between the 
energy used in sending summaries (and thus the frequency and 
resolution of those summaries) and the cost of false hits during queries.
The coarser and less frequent the summary information, the less
energy required, while false query hits in turn waste energy on 
requests for non-existent data.
TSAR employs an adaptive summarization technique that 
balances the cost of sending updates against the cost of false positives.
The key intuition is that each sensor can independently identify the
fraction of false hits and true hits for queries that access its local
archive. If most queries result in true hits, then the sensor 
determines that the summary can be coarsened further to reduce update
costs without adversely impacting the hit ratio. If many queries
result in false hits, then the sensor makes the granularity of each
summary finer to reduce the number and overhead of false hits.
The resolution of the summary depends on two 
parametersthe interval over which summaries of the data are constructed and
transmitted to the proxy, as well as the size of the 
applicationspecific summary. Our focus in this paper is on the interval over
which the summary is constructed. Changing the size of the data
summary can be performed in an application-specific manner (e.g.
using wavelet compression techniques as in [9]) and is beyond the
scope of this paper. Currently, TSAR employs a simple 
summarization scheme that computes the ratio of false and true hits and 
decreases (increases) the interval between summaries whenever this
ratio increases (decreases) beyond a threshold.