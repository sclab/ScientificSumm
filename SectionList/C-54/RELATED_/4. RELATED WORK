There has been a substantial amount of research on 
remote access to spatial data. One specific approach has
been adopted by numerous Web-based mapping services
(MapQuest [5], MapsOnUs [6], etc.). The goal in this 
approach is to enable remote users, typically only equipped
with standard Web browsers, to access the company"s 
spatial database server and retrieve information in the form of
pictorial maps from them. The solution presented by most
of these vendors is based on performing all the calculations
on the server side and transferring only bitmaps that 
represent results of user queries and commands. Although the
advantage of this solution is the minimization of both 
hardware and software resources on the client site, the resulting
product has severe limitations in terms of available 
functionality and response time (each user action results in a new
bitmap being transferred to the client).
Work described in [9] examines a client-server 
architecture for viewing large images that operates over a 
lowbandwidth network connection. It presents a technique
based on wavelet transformations that allows the 
minimization of the amount of data needed to be transferred over
the network between the server and the client. In this case,
while the server holds the full representation of the large 
image, only a limited amount of data needs to be transferred
to the client to enable it to display a currently requested
view into the image. On the client side, the image is 
reconstructed into a pyramid representation to speed up zooming
and panning operations. Both the client and the server keep
a common mask that indicates what parts of the image are
available on the client and what needs to be requested. This
also allows dropping unnecessary parts of the image from the
main memory on the server.
Other related work has been reported in [16] where a
client-server architecture is described that is designed to 
provide end users with access to a server. It is assumed that
this data server manages vast databases that are impractical
to be stored on individual clients. This work blends raster
data management (stored in pyramids [22]) with vector data
stored in quadtrees [19, 20].
For our peer-to-peer transfer approach (APPOINT), 
Napster is the forefather where a directory service is centralized
on a server and users exchange music files that they have
stored on their local disks. Our application domain, where
the data is already freely available to the public, forms a
prime candidate for such a peer-to-peer approach. Gnutella
is a pure (decentralized) peer-to-peer file exchange system.
Unfortunately, it suffers from scalability issues, i.e., floods of
messages between peers in order to map connectivity in the
system are required. Other systems followed these popular
systems, each addressing a different flavor of sharing over
the Internet. Many peer-to-peer storage systems have also
recently emerged. PAST [18], Eternity Service [7], CFS [10],
and OceanStore [15] are some peer-to-peer storage systems.
Some of these systems have focused on anonymity while 
others have focused on persistence of storage. Also, other 
approaches, like SETI@Home [21], made other resources, such
as idle CPUs, work together over the Internet to solve large
scale computational problems. Our goal is different than
these approaches. With APPOINT, we want to improve 
existing client-server systems in terms of performance by using
idle networking resources among active clients. Hence, other
issues like anonymity, decentralization, and persistence of
storage were less important in our decisions. Confirming
the authenticity of the indirectly delivered data sets is not
yet addressed with APPOINT. We want to expand our 
research, in the future, to address this issue.
From our perspective, although APPOINT employs some
of the techniques used in peer-to-peer systems, it is also
closely related to current Web caching architectures. 
Squirrel [13] forms the middle ground. It creates a pure 
peer-topeer collaborative Web cache among the Web browser caches
of the machines in a local-area network. Except for this 
recent peer-to-peer approach, Web caching is mostly a 
wellstudied topic in the realm of server/proxy level caching [8,
11, 14, 17]. Collaborative Web caching systems, the most
relevant of these for our research, focus on creating 
either a hierarchical, hash-based, central directory-based, or
multicast-based caching schemes. We do not compete with
these approaches. In fact, APPOINT can work in 
tandem with collaborative Web caching if they are deployed
together. We try to address the situation where a request
arrives at a server, meaning all the caches report a miss.
Hence, the point where the server is reached can be used to
take a central decision but then the actual service request
can be forwarded to a set of active clients, i.e., the 
down8
load and upload operations. Cache misses are especially
common in the type of large data-based services on which
we are working. Most of the Web caching schemes that are
in use today employ a replacement policy that gives a 
priority to replacing the largest sized items over smaller-sized
ones. Hence, these policies would lead to the immediate 
replacement of our relatively large data files even though they
may be used frequently. In addition, in our case, the user
community that accesses a certain data file may also be very
dispersed from a network point of view and thus cannot take
advantage of any of the caching schemes. Finally, none of
the Web caching methods address the symmetric issue of
large data uploads.
