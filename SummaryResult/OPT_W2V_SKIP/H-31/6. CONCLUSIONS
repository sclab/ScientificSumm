We exploit this property to develop a new per-term smoothing algorithm for Poisson language models, which is shown to outperform term-independent smoothing for both Poisson and multinomial models.
We compare the new models with the popular multinomial retrieval models both analytically and experimentally.
Further exploring the flexibilities over multinomial language models, such as length normalization and pseudo-feedback could be good future work.
Furthermore, we show that a  mixture background model for Poisson can be used to improve the performance and robustness over the standard Poisson background model.
Our analysis shows that while our new models and multinomial models are equivalent 