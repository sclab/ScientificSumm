W.l.g we may assume the error is bounded (since we are looking at what is essentially a finite set) therefore the probability that erσ(h, fb0 ) > ε cannot be too small, hence fb0 is not  PAClearnable with a sample of size n ✷ The following theorem gives an upper bound on the  sample complexity required for learning a set of functions with finite fat shattering dimension.
In the case of discrete learning , we would like to obtain a function h that with high probability agrees with f. We would then take the probability Pσ ( f ( x 