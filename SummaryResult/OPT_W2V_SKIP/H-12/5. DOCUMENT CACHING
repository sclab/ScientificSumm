Similar results hold for the wt100g collection, where a model of about 10 Mb  offers substantial space and time savings over no model at all, but returns diminish as the model size increases.
This is more than compensated for by the reduced size of each  document, allowing more documents into the document cache.
In addition to the document cache, the RAM of the  Snippet Engine must also hold the CTS decoding table that maps integers to strings, which is capped by a parameter at compression time (1 Gb in our experiments here).
The reason for the large impact of the 