page similarity detection algorithms may be useful for this purpose [9, 13]. 
in this case the hidden-web crawler should be able to detect that the site does not have much more new content and stop downloading pages from the site.
a calendar with links for every day).
finally, some hidden web sites may contain an infinite number of hidden web pages which do not contribute much  significant content (e.g.
in this case, a fully  automatic hidden-web crawler should know that the first result index page contains only a partial result and press the next button  automatically.
in addition, some hidden-web sites return their  results in batches of, say, 20 pages, so the user has to click on a next button in order to see more results.
the method proposed in [15] may be a good starting point.
but how can the crawler discover the query interfaces?
for example, in this paper we assumed that the crawler already knows all query  interfaces for hidden-web sites.
other practical issues in addition to the automatic query  generation problem, there are many practical issues to be addressed to build a fully automatic hidden-web crawler.
since many web sites follow limited formatting styles in presenting multiple attributes - for example, most book titles are preceded by the label title: - we believe we may learn page-segmentation rules automatically from a small set of training examples.
the main challenge is to automatically segment the returned pages so that we can  identify the sections of the pages that present the values corresponding to each attribute.
rowling", etc), we can apply the same idea that we used for the textual database: estimate the frequency of each  attribute value and pick the most promising one.
thus, if we can analyze the returned pages and extract the values for each field (e.g, title = ‘harry potter", author = ‘j.k.
for example, when an online bookstore supports queries on title, author and isbn, the pages returned from a query typically contain the title, author and isbn of corresponding books.
while generating queries for multi-attribute databases is clearly a more difficult problem, we may exploit the following observation to  address this problem: when a site supports multi-attribute queries, the site often returns pages that contain values for each of the query attributes.
multi-attribute databases we are currently investigating how to extend our ideas to structured multi-attribute databases.
6.1 future work we briefly discuss some future-research avenues.
given these results, we believe that our work provides a potential  mechanism to improve the search-engine coverage of the web and the user experience of web search.
in particular, in certain cases the adaptive policy can download more than 90% of a hidden web site after issuing approximately 100 queries.
experimental evaluation on 4 real hidden web sites shows that our policies have a great potential.
we proposed three different query generation policies for the hidden web: a policy that picks queries at random from a list of keywords, a policy that picks queries based on their frequency in a generic text collection, and a policy which adaptively picks a good query based on the content of the pages downloaded from the hidden web site.
in this paper, we studied how we can build a hidden web crawler that can automatically query a hidden web site and download pages from it.
therefore they cannot get to the hidden web pages which are only accessible through query interfaces.
traditional crawlers normally follow links on the web to  discover and download pages.
