a method to learn  better ranking functions for retrieval was recently proposed by radlinski and joachims [19] and has been applied to various local domains, including cornell university"s website [8]. 
a scientist trying to research the  habitat and evolutionary history of a jaguar may have better success using a finely tuned zoology-specific search engine than querying google with multiple keyword searches and wading through irrelevant results.
consider the canonical ambiguous search query, ‘jaguar", which can refer to either the car  manufacturer or the animal.
by restricting itself to only a specific domain of the  internet, a localized search engine can provide more precise search results.
this prohibits external search engines" crawlers from downloading content from the domain, and an external search engine must instead rely on outside links and anchor text to index these restricted pages.
second, site-specific domains can enable the robots  exclusion policy.
first, crawling dynamically generated pages-pages in the ‘hidden web"-has been the subject of research [20] and is a non-trivial task for an external crawler.
this is in  contrast to their large-scale counterparts, which suffer from  several shortcomings.
this obviates the need for crawling or spidering, and a complete and up-to-date  index of the domain can thus be guaranteed.
for site-specific domains, the local domain is readily  available on their own web server.
we note that, for topic-specific search engines, the relevant community can be efficiently identified and downloaded by using a focused crawler [21, 4].
the computational burden required to support search queries over a database this size is more manageable as well.
however, building a  localized search engine over a web community of a hundred thousand pages would only require a few gigabytes of  storage.
for a researcher who wishes to build a search engine with access to a couple of workstations or a small server, storage of this magnitude is simply not available.
to download a crawl of this size, approximately 167 terabytes of space is needed.
[13] found that the ‘surface web" (publicly available static sites) consists of 8.9 billion pages, and that the average size of these pages is approximately 18.7 kilobytes.
a 2003 study by lyman et al.
the resources needed to build a global search engine are enormous.
localized search engines enjoy three major advantages over their large-scale counterparts: they are relatively inexpensive to build, they can offer more  precise search capability over their local domain, and they can provide a more complete index.
localized search engines index a single community of the web, typically either a site-specific community, or a  topicspecific community.
