we use this method because of its simplicity and its success in previous work [15]. 
we adjust scores to have zero mean and unit variance.
we place these scores in the length n vector, y, where yi refers to the score of the ith-ranked document.
scores are often only computed for the top n documents from the collection.
it is also  different from ranking systems by the average performance on a set of queries.
this is different from ranking queries by the average performance on each query.
we would like this ranking to approximate the ranking of retrievals by the evaluation measure.
in this paper, we present results for ranking retrievals from arbitrary systems.
we would like to predict the performance of this retrieval with respect to some evaluation measure (eg, mean average precision).
we refer to the set of scores for a particular query-system combination as a  retrieval.
given a query, an information retrieval system produces a ranking of documents in the collection encoded as a set of scores associated with documents.
