categories and subject descriptors h.3.3 [information search and retrieval]: retrieval models; h.3.4 [systems and software]: performance  evaluation (efficiency and effectiveness) general terms performance, design, reliability, experimentation 
we also describe the first large-scale experiment to evaluate zero-judgment performance prediction for a massive number of retrieval systems over a variety of collections in several languages.
these new predictors can also be incorporated with classic  predictors to improve performance further.
when compared to a state of the art baseline, we demonstrate that the spatial analysis of retrieval scores provides significantly better prediction performance.
we find that the low correlation between scores of  topically close documents often implies a poor retrieval  performance.
if documents are embedded in a high-dimensional space (as they often are), we can apply techniques from spatial data analysis to detect correlations between document scores.
one common shortcoming of previous techniques is the  assumption of uncorrelated document scores and judgments.
our focus is on zero-judgment  performance prediction of individual retrievals.
approaches exist for the case of few and even no relevance judgments.
previous work addresses the evaluation of systems, the ranking of queries by difficulty, and the ranking of individual retrievals by  performance.
problems include the  inability to exhaustively label all documents for a topic,  nongeneralizability from a small number of topics, and  incorporating the variability of retrieval systems.
evaluation of information retrieval systems is one of the core tasks in information retrieval.
