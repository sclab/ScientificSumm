it is important before applying this measure to confirm that, given the semantics for some link between two retrieved items, we should expect a correlation between scores. 
similarly, answer candidates in a question-answering task may or may not exhibit  autocorrelation; in this case, the semantics of links is questionable too.
for example, ranked lists which incorporate document novelty should not exhibit  spatial autocorrelation; if anything autocorrelation should be negative for this task.
there are certainly cases where there is no reason to believe that retrieval scores will have topical autocorrelation.
we should make it clear that we have selected tasks where topical autocorrelation is appropriate.
once computed, a system rarely compares the score of a to the score of a topically-related document b. with some exceptions, the correlation of document scores has largely been ignored.
the score of document a may be computed by  examining query term or phrase matches, the document length, and perhaps global collection statistics.
query-based  information retrieval systems often score documents  independently.
why might systems fail to conform to the cluster hypothesis?
our experiments, then, demonstrate that a failure to respect the clustering  hypothesis correlates with poor performance.
recall that we regard autocorrelation as the degree to which a retrieval satisfies the clustering hypothesis.
the success of autocorrelation as a predictor may also have roots in the clustering hypothesis.
nevertheless, in situations where this information is lacking, autocorrelation provides substantial information.
our results demonstrate that this approximation is not as powerful as information from multiple retrievals.
therefore, if score diffusion tends to, in general, improve performance, then diffused scores will, in general, provide a good surrogate for relevance.
we know that diffusion of scores on the web graph and language model graphs improves performance [14, 16].
why is Ëœy a reasonable surrogate?
we believe that autocorrelation is, like multiple-retrieval algorithms, approximating a good ranking; in this case by diffusing scores.
assuming the presence of multiple retrievals is unrealistic in this case.
for example, a system may need to, at retrieval time, assess its performance before deciding to conduct more intensive processing such as pseudo-relevance feedback or interaction.
this situation arises often in system design.
the most important result from our experiments involves prediction when no information is available from multiple runs (tables 1 and 2a).
