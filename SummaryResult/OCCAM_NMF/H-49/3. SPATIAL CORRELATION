since ˜y encodes the spatial  dependencies in y and yµ encodes the spatial properties of the multiple runs, we can compute a third correlation between these two vectors, ρ(˜y, yµ) = ˜yt yµ ˜y 2 yµ 2 (5) we can interpret equation 5 as measuring the correlation between a high quality ranking (yµ) and a spatially smoothed version of the retrieval (˜y). 
an  alternative means of combination is suggested by the mathematical form of our predictors.
one way to accomplish this is to combine these predictors as independent variables in a linear regression.
we use pearson"s product-moment correlation to measure the similarity between these vectors, ρ(y, yµ) = yt yµ y 2 yµ 2 (4) we will comment on the similarity between equation 3 and 4 in section 7. of course, we can combine ρ(y, ˜y) and ρ(y, yµ) if we  assume that they capture different factors in the prediction.
because yµ represents a very good retrieval, we hypothesize that a strong similarity between yµ and y will correlate positively with system  performance.
since we use zero mean and unit variance normalization, work in metasearch suggests that this assumption is justified [15].
we use the mean vector as an  approximation to relevance.
we will represent the mean of these vectors as yµ = pm i=1 yi.
assume that we are given m score functions, yi, for the same n documents.
we can treat the correlation between the retrieval we are interested in and the combined scores as a predictor of performance.
in this situation, we can use combined information from these scores to construct a surrogate for a high-quality ranking [17].
the same query.
notice that, as the moran coefficient increases, neighboring cells tend to have similar values.
binary retrieval scores would define a pattern on this grid.
from the perspective of information retrieval, each of these grid spaces would represent a document and  documents would be organized so that they lay next to topically-related documents.
the moran coefficient is a local measure of function consistency.
3.3 correlation with other retrievals sometimes we are interested in the performance of a single retrieval but have access to scores from multiple systems for (a) im = 0.006 (b) im = 0.241 (c) im = 0.487 figure 1: the moran coefficient, im for a several binary functions on a grid.
we present some examples of autocorrelations of functions on a grid in figure 1.
unfortunately, the bound for equation 2 is not consistent for different w and y. therefore, we use the cauchy-schwartz inequality to establish a bound, ˜im ≤ n etwe s ytwtwy yty and we define the normalized spatial autocorrelation as im = yt wy p yty × ytwtwy notice that if we let ˜y = wy, then we can write this formula as, im = yt ˜y y 2 ˜y 2 (3) which can be interpreted as the correlation between the  original retrieval scores and a set of retrieval scores diffused in the space.
we would like to compare autocorrelation values for  different retrievals.
assuming the function y over n locations, this is defined as ˜im = n etwe p i,j wijyiyj p i y2 i = n etwe yt wy yty (2) where et we = p ij wij.
one such suitable measure is the moran coefficient of spatial  autocorrelation.
3.2 spatial autocorrelation of a retrieval recall that we are interested in measuring the similarity between the scores of spatially-close documents.
we also row normalize the matrix so that pn j=1 wij = 1 for all i.
in all of our experiments, we have fixed k to 5. we leave exploration of parameter sensitivity to future work.
recall that we are given the top n documents retrieved in y. we can compute an n × n  similarity matrix, w. an element of this matrix, wij represents the similarity between documents ranked i and j. in  practice, we only include the affinities for a document"s k-nearest neighbors.
given documents and some similarity measure, we can construct a matrix which encodes the similarity between pairs of documents.
assuming vectors are scaled by their l2 norm, we use the inner product, ˜di, ˜dj , to define similarity.
we use this weighting scheme due to its success for topical link detection in the context of topic detection and tracking (tdt) evaluations [6].
specifically, we adopt tf.idf document vectors, ˜di = di log „ (n + 0.5) − ci 0.5 + ci « (1) where d is a vector of term frequencies, c is the length-|v| document frequency vector.
instead, we choose an inner product known to be effective at detecting  interdocument topical relationships.
3.1 spatial representation of documents our work does not focus on improving a specific similarity measure or defining a novel vector space.
we conclude by extending this model to include information from multiple retrievals from multiple systems for a single query.
high spatial autocorrelation suggests that knowing the value of a function at location a will tell us a great deal about the value at a neighboring location b. there is a high spatial autocorrelation for a function representing the temperature of a location since knowing the temperature at a location a will tell us a lot about the temperature at a neighboring location b. low spatial  autocorrelation suggests that knowing the value of a function at location a tells us little about the value at a neighboring location b. there is low spatial autocorrelation in a function measuring the outcome of a coin toss at a and b. in this section, we will begin by describing what we mean by spatial proximity for documents and then define a  measure of spatial autocorrelation.
if we want to quantify the spatial dependencies of a  function, we would employ a measure referred to as the spatial autocorrelation [5, 10].
the function would map the location of a neighborhood to an infection rate.
for example, we might be interested in the prevalence of a disease in the neighborhood of some city.
we use the term function here to mean a mapping from a location to a real value.
whereas in information retrieval, we are concerned with the score at a point in a space, in spatial data analysis, we are concerned with the value of a function at a point or location in a space.
because of the prevalence and success of spatial models of information retrieval, we believe that the application of spatial data analysis techniques are appropriate.
an embedding space is often  selected to respect topical proximity; if two documents are near, they are more likely to share a topic.
for example, given a vocabulary, v, this vector space may be an arbitrary |v|-dimensional space with cosine inner-product or a multinomial simplex with a  distributionbased distance measure.
in information retrieval, we often assume that the  representations of documents exist in some high-dimensional  vector space.
