some recent work on experimental design can be found in [6], [16]. 
since the computation of the determinant and eigenvalues of a matrix is much more expensive than the computation of matrix trace, a-optimal design is more efficient than the other two.
• e-optimal design: maximum eigenvalue of hsse.
• a-optimal design: trace of hsse.
the three most common scalar measures of the size of the parameter covariance matrix in optimal experimental design are: • d-optimal design: determinant of hsse.
thus, the maximum likelihood estimate for the weight vector, ˆw, is that which minimizes the sum squared error jsse(w) = k i=1 wt zi − yi 2 (2) the estimate ˆw gives us an estimate of the output at a novel input: ˆy = ˆwt x. by gauss-markov theorem, we know that ˆw − w has a zero mean and a covariance matrix given by σ2 h−1 sse, where hsse is the hessian of jsse(w) hsse = ∂2 jsse ∂w2 = k i=1 zizt i = zzt where z = (z1, z2, · · · , zk).
we define f(x) = wt x to be the learner"s output given input x and the weight vector w. suppose we have a set of labeled sample points (z1, y1), · · · , (zk, yk), where yi is the label of zi.
different observations have errors that are  independent, but with equal variances σ2 .
2.2 optimal experimental design we consider a linear regression model y = wt x + (1) where y is the observation, x is the independent variable, w is the weight vector and is an unknown error with zero mean.
in other words, the points zi(i = 1, · · · , k) can improve the classifier the most if they are labeled and used as training points.
given a set of points a = {x1, x2, · · · , xm} in rd , find a subset b = {z1, z2, · · · , zk} ⊂ a which contains the most  informative points.
2.1 the active learning problem the generic problem of active learning is the following.
in this section, we give a brief description of these approaches.
the most related work is optimal experimental design [1], including a-optimal design, d-optimal design, and  eoptimal design.
since our proposed algorithm is based on regression  framework.
