further details that cannot be included here for the sake of space are available in an on-line technical report [16]. 
section 5 concludes.
section 4 reports results from experiments with the system.
section 3  describes the architecture of bee.
section 2 reviews relevant previous work.
the current version of bee characterizes and predicts the behavior of agents  representing soldiers engaged in urban combat [8].
in bee, it is a set of parameters governing the behavior of software agents  representing the individuals being analyzed.
for nonlinear dynamical systems, the  representation is a closed-form mathematical equation.
it is inspired by  techniques used to predict the behavior of nonlinear dynamical  systems, in which a representation of the system is continually fit to its recent past behavior.
bee (behavioral evolution and extrapolation) is a novel  approach to recognizing the rational and emotional state of multiple interacting agents based solely on their behavior, without recourse to intentional communications from them.
domains that exhibit these constraints can often be  characterized as adversarial, and include military combat, competitive business tactics, and multi-player computer games.
an agent"s emotional state may be at least as important as its rational state in determining its behavior.
the agents often are trying to hide their intentions (and even their presence), rather than intentionally sharing information.
environmental dynamics can frustrate agent intentions.
increasing the number of agents leads to a combinatorial  explosion that can swamp conventional analysis.
many realistic problems deviate from these conditions.
work to date  focuses almost entirely on recognizing the rational state (as opposed to the emotional state) of a single agent (as opposed to an  interacting community), and frequently takes advantage of explicit  communications between agents (as in managing conversational  protocols).
this problem has traditionally been addressed under the  rubric of plan recognition or plan inference.
such  reasoning is motivated by a desire to predict the agent"s behavior.
a central challenge in many application domains is reasoning from external observations of agent behavior to an estimate of their internal state.
however, this behavior is driven by the agent"s internal state, which (in the case of a human) may involve high-level psychological and cognitive concepts such as intentions and emotions.
our observations are often limited to the agent"s external behavior, which can frequently be  summarized numerically as a trajectory in space-time (perhaps  punctuated by actions from a fairly limited vocabulary).
reasoning about agents that we observe in the world must integrate two disparate levels.
