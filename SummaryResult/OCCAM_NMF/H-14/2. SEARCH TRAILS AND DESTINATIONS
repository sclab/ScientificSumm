, : where query and destination term weights, an computed using standard tf.idf weighting and que session-normalized smoothed tf.idf weighting, respec exploring alternative algorithms for the destination p remains an interesting challenge for future work, resu study described in subsequent sections demonstrate th approach provides robust, effective results. 
visual inspection of those that did not lie in this range revealed that many were either relevant but had no judgments, or were related but had indirect query association (e.g., petfooddirect.com for query [dogs]).
the average rating of most of the destinations lay between good and excellent.
then, given a new query q consisting of k terms t1â€¦tk, we identify highest-scoring destinations using the following similarity function: 1 independent measures t-test: t(~60m) = 3.89, p < .001 2 the topical relevance of the destinations was tested for a subset of around ten thousand queries for which we had human judgments.
for both destination types, we obtain a corpus of query-destination pairs and use it to construct term-vector representation of destinations that is analogous to the classic tf.idf document representation in traditional ir [14].
as discussed above, we extract two types of destinations: query destinations and session destinations.
to overcome this problem, we utilize a simple term-based prediction model.
therefore, a lookup-based approach would prevent us from reliably suggesting destinations for a large fraction of searches.
however, we have found that over the six-month period covered by our dataset, 56.9% of queries are unique, and 97% queries occur 10 or fewer times, accounting for 19.8% and 66.3% of all searches respectively (these numbers are comparable to those reported in previous studies of search engine query logs [15,17]).
for query trails, users also visit more pages, and spend significantly longer, on the last domain in the trail compared to all previous domains combined.1 these distinctions of the last domains in the trails may indicate user interest, page utility, or page relevance.2 2.3 destination prediction for frequent queries, most popular destinations identified from web activity logs could be simply stored for future lookup at search time.
this suggests that users often do not find all the information they seek on the first domain they visit.
on average, users visit 2 unique (non search-engine) domains per query trail, and just over 4 unique domains per session trail.
measure query trails session trails number of unique domains 2.0 4.3 total page views all domains 4.8 16.2 domains 1 to (n - 1) 1.4 10.1 domain n (destination) 3.4 6.2 total time spent (secs) all domains 172.6 621.8 domains 1 to (n - 1) 70.4 397.6 domain n (destination) 102.3 224.1 the statistics suggest that users generally browse far from the search results page (i.e., around 5 steps), and visit a range of domains during the course of their search.
table 1. summary statistics (mean averages) for search trails.
statistics are averages for all trails with two or more steps (i.e., those trails where at least one search result was clicked).
differences in user interaction between the last domain on the trail (domain n) and all domains visited earlier (domains 1 to (n - 1)) are particularly important, because they highlight the wealth of user behavior data not captured by logs of search engine interactions.
2.2 trail and destination analysis table 1 presents summary statistics for the query and session trails.
we now describe some trail features.
approximately 14 million query trails and 4 million session trails were extracted from the logs.
query trails use the same termination criteria as session trails, but also terminate upon submission of a new query to a search engine.
session trails transcend multiple queries and terminate only when one of the three termination criteria above are satisfied.
there are two types of search trails we consider: session trails and query trails.
if a page (at step i) meets any of these criteria, the trail is assumed to terminate on the previous page (i.e., step i - 1).
to reduce the amount of noise from pages unrelated to the active search task that may pollute our data, search trails are terminated when one of the following events occurs: (1) a user returns to their homepage, checks e-mail, logs in to an online service (e.g., myspace or del.ico.us), types a url or visits a bookmarked page; (2) a page is viewed for more than 30 minutes with no activity; (3) the user closes the active browser window.
since users may open a new browser window (or tab) for each task [18], each task has its own browser trail, and a corresponding distinct search trail.
extracting search trails using this methodology also goes some way toward handling multi-tasking, where users run multiple searches concurrently.
trails must contain pages that are either: search result pages, search engine homepages, or pages connected to a search result page via a sequence of clicked hyperlinks.
after originating with a query submission to a search engine, trails proceed until a point of termination where it is assumed that the user has completed their information-seeking activity.
it is these search trails that we use to identify popular destinations.
located within some of these trails were search trails that originated with a query submission to a commercial search engine such as google, yahoo!, windows live search, and ask.
within each browser instance, participant navigation was summarized as a path known as a browser trail, from the first to the last web page visited in that browser.
2.1 trail extraction for each user, interaction logs were grouped based on browser identifier information.
in-depth description and analysis of trail extraction are presented in [20].
in this section, we summarize the extraction of trails, their features, and destinations (trail end-points).
this information was sufficient to reconstruct temporally ordered sequences of viewed pages that we refer to as trails.
we used web activity logs containing searching and browsing activity collected with permission from hundreds of thousands of users over a five-month period between december 2005 and april 2006. each log entry included an anonymous user identifier, a timestamp, a unique browser window identifier, and the url of a visited web page.
