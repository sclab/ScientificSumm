[21]; however, the features that we use for definining our expansions-including topical structure and  organizational structure-have not been used in this way before. 
the models we  consider in our second round of experiments are mixture models  similar to contextual language models [1] and to the expanded  documents of tao et al.
in the first step, we consider three baseline models, two taken from [4] (the models 1 and 2 mentioned above), and one a refined version of a model introduced in [3] (which we refer to as model 3 below); this third model is also similar to the model described by petkova and croft [19].
our modeling proceeds in two steps.
in our modeling of expert finding and profiling we collect evidence for expertise from multiple sources, in a heterogeneous collection, and integrate it with the co-occurrence of candidates" names and query terms-the language modeling  setting allows us to do this in a transparent manner.
we use generative language modeling to find associations  between topics and people.
as an aside, creating a textual summary of a person shows some similarities to biography finding, which has received a  considerable amount of attention recently; see e.g., [13].
balog and de rijke [2] study the related task of  finding experts that are similar to a small set of experts given as input.
while their methods proved to be efficient on the w3c corpus, they require an amount of data that may not be available in the typical knowledge-intensive organization.
turning to other expert retrieval tasks that can also be addressed using topic-people associations, balog and de rijke [3] addressed the task of determining topical expert profiles.
petkova and croft [19] propose yet another approach, based on a combination of the above model 1 and 2, explicitly modeling topics.
macdonald and ounis [16] propose a different approach for ranking candidate expertise with respect to a topic based on data fusion techniques, without using  collectionspecific heuristics; they find that applying field-based weighting models improves the ranking of candidates.
most systems that took part in the 2005 and 2006 editions of the expert finding task at trec implemented (variations on) one of these two models; see [10, 20].
in the reported experiments the second method performs significantly better when there are sufficiently many associated documents per candidate.
"s model 1 directly models the knowledge of an expert from associated documents, while their model 2 first locates documents on the topic and then finds the associated experts.
balog et al.
[4], who formalize and compare two methods.
one such published approach is the p@noptic system [9], which builds a representation of each person by concatenating all documents associated with that person-this is similar to model 1 of balog et al.
in contrast with focusing on particular document types, there is also an increased interest in the development of systems that index and mine published intranet documents as sources of evidence for  expertise.
more recent approaches use specific document sets (such as email [6] or software [18]) to find expertise.
for updating  profiles in these systems in an automatic fashion there is a need for intelligent technologies [5].
most of these tools (usually called  yellow pages or people-finding systems) rely on people to self-assess their skills against a predefined set of keywords.
initial approaches to expertise finding often employed databases containing information on the skills and knowledge of each  individual in the organization [11].
