30 times more than in the w3c setting. 
the cause for the latter differences seems to reside in the number of knowledge areas considered here-approx.
for expert profiling, the  differences are far more dramatic: the map scores for model 2  reported here are around 50% below the scores in [3], while the (best) mrr scores are about the same as those in [3].
for expert finding the map scores for model 2 reported here are about 50% higher than the corresponding figures in [4], while our mrr scores are slightly below those in [4].
while it is hard to compare scores across collections, we  conclude with a brief comparison of the absolute scores in table 1 to those reported in [3, 4] on the w3c test set (2005 edition).
however, the  performance is substantially better for the english topics.
for the profiling task the coverage of the  candidates (%ca) is very similar for both languages.
apart from that, the scores fall in the same range for both languages.
when we compare the results across languages, we find that the coverage of english topics (%q) is higher than of the dutch ones for expert finding.
adding the homepages does not prove to be particularly useful.
expert profiling benefits much from the clean data present in the rd and cd document types, while the publications contribute the most to the expert finding task.
the various document types differ in their characteristics and how they improve the finding and profiling tasks.
model 1 has the best coverage of candidates (%ca) and topics (%q).
however, when the data is clean and very focused (rd), model 3 outperforms it in a number of cases.
looking at table 1 we see that model 2 performs the best across the board.
rd+cd+pub+hp is equivalent to the full  collection and will be referred as the baseline of our experiments.
the rows of the table correspond to the various document types (rd, cd, pub, and hp) and to their combinations.
6.3 results table 1 shows the performance of model 1, 2, and 3 on the  expert finding and profiling tasks.
we also report the percentage of topics (%q) and candidates (%ca)  covered, for the expert finding and profiling tasks, respectively.
we use standard information retrieval measures, such as mean average precision (map) and mean reciprocal rank (mrr).
the results were averaged for the queries in the intersection of relevance judgements and results; missing queries do not contribute a value of 0 to the scores.
for english we only used topics for which the dutch translation was available; for dutch all topics were considered.
results were evaluated separately for english and dutch.
6.2 experimental setup the output of our models was evaluated against the self-assigned topic labels, which were treated as relevance judgements.
what about performance differences between the two languages in our test collection?
and how does model 3 compare to model 1?
how do they compare on our data set?
in [4], model 2 outperformed model 1 on the w3c collection.
the question is how the models compare on the different tasks, and in the setting of the uvt expert collection.
both expert finding and profiling rely on the estimations of p(q|ca).
6.1 research questions we address the following research questions.
we detail our research  questions and experimental setup, and then present our results.
below, we evaluate section 4"s models for expert finding and profiling onthe uvt expert collection.
