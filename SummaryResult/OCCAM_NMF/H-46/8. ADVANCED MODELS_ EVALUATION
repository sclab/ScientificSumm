these scores demonstrate that despite its simplicity, our method for combining results over multiple languages achieves substantial improvements over the baseline. 
the relative improvement of the precision scores ranges from 10% to 80%.
the coverage of topics and candidates for the expert finding and profiling tasks,  respectively, is close to 100% in all cases.
proved over all measures for both tasks.
best scores are in boldface.
method model 1 model 2 model 3 map mrr map mrr map mrr expert finding uk bl 0.296 0.454 0.339 0.509 0.221 0.333 uk ct 0.330 0.491 0.342 0.500 0.228 0.342 nl bl 0.240 0.350 0.271 0.403 0.227 0.389 nl ct 0.251 0.382 0.267 0.410 0.246 0.404 expert profiling uk bl 0.485 0.546 0.499 0.548 0.381 0.416 uk ct 0.562 0.620 0.508 0.558 0.440 0.486 nl bl 0.263 0.313 0.294 0.358 0.262 0.315 nl ct 0.330 0.384 0.317 0.387 0.294 0.345 table 6: performance of the context models (ct) compared to the baseline (bl).
all three models significantly  imlang.
table 3 reports on the multilingual results, where performance is evaluated on the full topic set.
we performed experiments with various λ settings, but did not observe significant differences in performance.
the weights on these languages were set to be identical (λuk = λnl = 0.5).
8.5 multilingual models in this subsection we evaluate the method for combining  results across multiple languages that we described in section 7.3. in our setting the set of languages consists of english and dutch: l = {uk, nl}.
the poor performance on expert profiling may be due to the fact that context models alone did not perform very well on the profiling task to begin with.
noticably, for expert finding (and model 1), it improves over 50% (for  english) and over 70% (for dutch) on map.
we find a positive impact of the context models only for expert finding.
table 6 reports on the results.
second, we performed another evaluation, where we combined the contextual models with the candidate models (to score  candidates again).
the  explanation may be that general concepts (i.e., our main topics) may belong to several organizational units.
however, the expert profiling task shows a different picture: the scores are low, and the task seems hard.
as far as expert finding goes, given a topic, the corresponding organizational unit can be identified with high precision.
table 5 reports on the results.
an organizational unit is considered to be relevant for a given topic (or vice versa) if at least one member of the unit selected the given topic as an expertise area.
first, we evaluated the models of the organizational units, using all topics (all) and only the main topics (main).
the unit a person belongs to is used as a context for that person.
8.4 contextual information a two level hierarchy of organizational units (faculties and  institutes) is available in the uvt expert collection.
the content-based approaches performed consistently better than hdist.
for both tasks, the ll method performed best.
we managed to improve upon the baseline in all cases, but the improvement is more noticeable for the profiling task.
wise mutual information (pmi), log-likelihood (ll), and distance within topic hierarchy (hdist).
topics model 1 model 2 model 3 map mrr map mrr map mrr expert finding uk all 0.423 0.545 0.654 0.799 0.494 0.629 uk main 0.500 0.621 0.704 0.834 0.587 0.699 nl all 0.439 0.560 0.672 0.826 0.480 0.630 nl main 0.440 0.584 0.645 0.816 0.515 0.655 expert profiling uk all 0.240 0.640 0.306 0.778 0.223 0.616 uk main 0.523 0.677 0.519 0.648 0.461 0.587 nl all 0.203 0.716 0.254 0.770 0.183 0.627 nl main 0.332 0.576 0.380 0.624 0.332 0.549 table 5: evaluating the context models on organizational units.
the four methods used for  estimating knowledge-area similarity are kl divergence (kldiv),  pointlang.
8.3 exploiting knowledge area similarity table 4 presents the results.
the relevance judgements were restricted to the main topic set, but were not expanded with subtopics.
this main set consists of 132 dutch and 119 english topics.
(a main topic has subtopics, but is not a subtopic of any other topic.)
this set is referred as main topics, and consists of topics that are located at the top level of the topical hierarchy.
8.2 experimental setup given that the self-assessments are also sparse in our collection, in order to be able to measure differences between the various  models, we selected a subset of topics, and evaluated (some of the) runs only on this subset.
and finally, is our simple way of combining the monolingual scores sufficient for obtaining significant improvements?
for which tasks?
furthermore, is our way of bringing in contextual information useful?
which of the various methods for capturing word  relationships is most effective?
8.1 research questions our questions follow the refinements presented in the preceding section: does exploiting the knowledge area similarity improve  effectiveness?
runs were evaluated on the main topics set.
method model 1 model 2 model 3 map mrr map mrr map mrr english baseline 0.296 0.454 0.339 0.509 0.221 0.333 kldiv 0.291 0.453 0.327 0.503 0.219 0.330 pmi 0.291 0.453 0.337 0.509 0.219 0.331 ll 0.319 0.490 0.360 0.524 0.233 0.368 hdist 0.299 0.465 0.346 0.537 0.219 0.332 dutch baseline 0.240 0.350 0.271 0.403 0.227 0.389 kldiv 0.239 0.347 0.253 0.386 0.224 0.385 pmi 0.239 0.350 0.260 0.392 0.227 0.389 ll 0.255 0.372 0.281 0.425 0.231 0.389 hdist 0.253 0.365 0.271 0.407 0.236 0.402 method model 1 model 2 model 3 map mrr map mrr map mrr english baseline 0.485 0.546 0.499 0.548 0.381 0.416 kldiv 0.510 0.564 0.513 0.558 0.381 0.416 pmi 0.486 0.546 0.495 0.542 0.407 0.451 ll 0.558 0.589 0.586 0.617 0.408 0.453 hdist 0.507 0.567 0.512 0.563 0.386 0.420 dutch baseline 0.263 0.313 0.294 0.358 0.262 0.315 kldiv 0.284 0.336 0.271 0.321 0.261 0.314 pmi 0.265 0.317 0.265 0.316 0.273 0.330 ll 0.312 0.351 0.330 0.377 0.284 0.331 hdist 0.280 0.327 0.288 0.341 0.266 0.321 table 4: performance on the expert finding (top) and profiling (bottom) tasks, using knowledge area similarities.
best scores for each model are in italic, absolute best scores for the expert finding and profiling tasks are in boldface.
expert finding expert profiling language model 1 model 2 model 3 model 1 model 2 model 3 %q map mrr %q map mrr %q map mrr %ca map mrr %ca map mrr %ca map mrr english only 97.8 0.237 0.372 98.6 0.280 0.441 98.5 0.166 0.293 100 0.199 0.387 88.7 0.281 0.525 90.9 0.169 0.329 dutch only 61.3 0.249 0.401 62.6 0.265 0.436 62.6 0.195 0.344 91.9 0.164 0.426 90.1 0.195 0.488 91.9 0.125 0.328 combination 99.4 0.297 0.444 99.7 0.324 0.491 99.7 0.223 0.388 100 0.241 0.445 92.1 0.313 0.564 93.2 0.224 0.411 table 3: performance of the combination of languages on the expert finding and profiling tasks (on candidates).
in this section we present an experimental evaluation of our  advanced models.
