while the inex-2003  metric exhibits undesirable anomalies [11], and was not used in 2004, values are reported in the evaluation section to provide an additional instrument for investigating overlap. 
two versions were created, one that considered only component size and another that considered both size and overlap.
this inex-2003 metric extends the definitions of precision and recall to consider both the size of reported components and the overlap between them.
at inex 2003, yet another metric was introduced to  ameliorate the perceived limitations of the inex-2002 metric.
while xcg was not used officially at inex 2004, a version of it is likely to be used in the future.
thus, the xcg metric rewards retrieval runs that avoid overlap.
this ideal gain vector is constructed from the relevance judgments by  eliminating overlap and retaining only best element along a given path.
[11] propose an xml cumulated gain metric, which compares the cumulated gain [9] of a ranked list to an ideal gain vector.
to address this problem, kazai et al.
in  particular, both the generalized and sog quantization functions give partial credit to a near miss even when a (3,3)  element overlapping it is reported at a higher rank.
the inex-2002 metric does not penalize overlap.
since this metric was first introduced at inex 2002, it is generally referred as the inex-2002 metric.
this paper reports  results of this metric using all three of these quantization  functions.
both the generalized quantization function and the specificity-oriented generalization (sog) function credit elements according to their degree of relevance [11], with the second function  placing greater emphasis on specificity.
other quantization functions are designed to give partial credit to elements which are near misses, due to a lack or exhaustivity and/or specificity.
this variant is essentially the familiar map value, with (3,3) elements treated as relevant and all other elements treated as not relevant.
one variant, the strict quantization function gives a weight of 1 to (3,3) elements and a weight of 0 to all others.
3.3 evaluation metrics the principle evaluation metric used at inex 2004 is a version of mean average precision (map), adjusted by  various quantization functions to give different weights to  different elements, depending on their exhaustivity and specificity values.
additional information on the assessment methodology may be found in piwowarski and lalmas [19], who provide a detailed rationale.
thus, a (3,3) element is highly exhaustive and highly specific, a (1,3) element is marginally exhaustive and highly specific, and a (0,0) element is not relevant.
a four-point scale is used in both dimensions.
instead, the inex organizers adopted two dimensions for relevance  assessment: the exhaustivity dimension reflects the degree to which an element covers the topic, and the specificity  dimension reflects the degree to which an element is focused on the topic.
3.2 relevance assessment since xml ir is concerned with locating those elements that provide complete coverage of a topic while containing as little extraneous information as possible, simple relevant vs. not relevant judgments are not sufficient.
each group could submit up to three experimental runs consisting of the top m = 1500 elements for each topic.
each topic includes a short keyword query, which is executed over the collection by each participating group on their own xml ir system.
in 2004, 40 new co topics were selected by the conference organizers from contributions provided by the conference participants.
since the work described in this paper is directed at the content-only task, where the ir system receives no guidance regarding the elements to return, the cas task is ignored in the remainder of our description.
for the other task, the  contentand-structure or cas task, the topics are written in an xml query language [22] and contain explicit references to document structure, which the ir system must attempt to satisfy.
for this task, the ir system is required to select the elements to be returned.
for one task, the content-only or co task, the topics consist of short natural language statements with no direct reference to the structure of the documents in the  collection.
the two tasks differed in the types of topics they used.
at inex 2004, the two main experimental tasks were both adhoc retrieval tasks, investigating the performance of  systems searching a static collection using previously unseen topics.
3.1 tasks for the main experimental tasks, inex 2004 participants were provided with a collection of 12,107 articles taken from the ieee computer societies magazines and journals  between 1995 and 2002. each document is encoded in xml using a common dtd, with the document of figures 1 and 2 providing one example.
for detailed information, the proceedings of the  conference itself should be consulted [8].
space limitations prevent the inclusion of more than a brief summary of inex 2004 tasks and evaluation  methodology.
