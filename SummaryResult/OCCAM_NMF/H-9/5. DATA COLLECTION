on average, we have 6.23 and 5.89 clicks for each test case in the two test sets respectively. 
finally, we obtain 172 and 177 test cases in the first and second test sets respectively.
since we want to test whether our algorithm can learn from the past queries, we also filter out those test cases whose queries can not retrieve at least 100 pseudo-documents from our history collection.
for each test case, i.e., each session, we count the number of different clicks and filter out those test cases with fewer than 4 clicks under the assumption that a query with more clicks is more likely to be an informational query.
it thus makes sense to focus on the informational queries in our evaluation.
organizing search results into different aspects is expected to help informational queries.
using clicks as judgments, we can then compare different algorithms for  organizing search results to see how well these algorithms can help users reach the clicked urls.
since it is infeasible to ask the original user who submitted a query to judge the results for the query, we follow the work [11] and opt to use the clicks associated with the query in a session to approximate relevant documents.
different test cases may have the same queries but possibly different clicks.)
(note that we do not aggregate sessions for test cases.
each session  contains a single query and several clicks.
for each test set, we use every session as a test case.
according to the time, we separate this data into two test sets equally for cross-validation to set parameters.
we construct our test data from the last 1/3 data.
we build the pseudo-documents for all these queries as described in section 3. the average length of these pseudo-documents is 68 words and the total data size of our history collection is 129mb.
on average, each query has 3.5 distinct clicks.
after cleaning, we get 169,057 unique queries in our history data collection totally.
in the history collection, we clean the data by only  keeping those frequent, well-formatted, english queries (queries which only contain characters ‘a", ‘b", ..., ‘z", and space, and appear more than 5 times).
to test our algorithm, we separate the whole data set into two parts according to the time: the first 2/3 data is used to simulate the historical data that a search engine  accumulated, and we use the last 1/3 to simulate future queries.
there are 8,144,000 queries, 3,441,000 distinct queries, and 4,649,000 distinct urls in the raw data.
in total, this log data spans 31 days from 05/01/2006 to 05/31/2006.
we construct our data set based on the msn search log data set released by the microsoft live labs in 2006 [14].
