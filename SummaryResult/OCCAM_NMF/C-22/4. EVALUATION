0 1000 2000 3000 4000 5000 1 2 3 4 5 6 0 20 40 60 80 100 120 mmco (%) et (milliseconds) n (interval) mmco (%) figure 3. performance characteristics with simple criteria 
the graph also demonstrates that even using modest criteria to reduce the metrics measurement to more realistic levels, has a rapid improvement on collection overhead at 20% for 500ms of et.
furthermore, the measurement time includes delivering the results synchronously via a remote call to the adaptation engine on a different host, which would normally be done asynchronously, thus further reducing the impact on method execution performance.
these results are encouraging since even for the worst case scenario of n=1 the metric collection overhead is an acceptable 20% for a method of 1500ms duration (which is relatively short for a component or service level object in a distributed enterprise class application) with previous work on adaptation showing that such an overhead could easily be recovered by the efficiency gains made by adaptation [5].
furthermore, the value of n=5 was also applied to the service push criterion so that metrics were only pushed to the adaptation engine after five such notifications, that is for example five different mobjects had updated their metrics.
this means that for a frequency value of 5 (n=5), metrics are only measured on every fifth method call, which then results in a notification through the model entity hierarchy to the service, on this same fifth invocation.
in addition, to demonstrate the effect of using simple frequency based criteria, the mmco results as a percentage of method execution time were plotted as a 3-dimensional graph in figure 3 with the z-axis representing the frequency used in both the measure metrics criterion and the service to adaptation engine push criterion.
clearly, it is not practical to measure metrics and perform adaptation on objects with short execution times that cannot benefit from remote execution on hosts with greater processing power, thereby offsetting it overhead of remote compared with local execution as well as the cost of object migration and the metrics collection process itself.
50 150 250 350 450 550 1 5 10 15 20 25 number of mobjects/methods mobjectmetricscollectionoverheadmmco(ms) methods mobjects both figure 2. worst case performance characteristics the next step was to determine the percentage metrics collection overhead compared with execution time in order to provide information about the execution characteristics of objects that would be suitable for adaptation using this metric collection approach.
note that in contrast, the proxy metrics collection overhead (pmco) was relatively small and constant at < 5ms, since in the absence of a proxy push criterion (this was only implemented on the service) the response time (rt) data for a single method is pushed during every invocation.
although combining these together provides an exponential growth that is approximately n-squared, the initial results are not discouraging since delivering all of the metrics associated with 20 mobjects, each having 20 methods (which constitutes quite a large application given that mobjects typically represent coarse grained object clusters) is approximately 400ms, which could reasonably be expected to be offset with adaptation gains.
it can be seen that the independent factors of increasing the number of mobjects and methods independently are linear.
figure 2 shows the metric collection overhead in the mobject (mmco), for different numbers of mobjects and methods when all criteria are set to always to provide the maximum measurement and propagation of metrics and thus an absolute worst case performance scenario.
all other (update and push) criteria were set to always meaning that they always evaluated to true and thus a notification was posted.
in addition, the push criterion was also implemented on the service, in order to provide an evaluation of controlling the frequency of metrics delivery to the adaptation engine.
the measure metrics criterion was chosen, since this represents the starting point of the measurement process and can control under what circumstances and how frequently metrics are measured.
in addition, since exhaustive testing of criteria is beyond the scope of this paper, two different types of criteria were used in the tests.
since only a limited number of tests could be executed, this evaluation chose to measure the worst case scenario in which all metrics collection was initiated in mobjects, wherein the propagation cost is higher than for any other metrics collected in the system.
the two machines were connected to a router with a third computer acting as a file server and hosting the external adaptation engine implemented within the mobjex system controller, thereby simulating a global adaptation scenario.
all tests were executed using two pentium 4 3.0 ghz pcs with 1,024 mb of ram, running java 1.4.2_08.
the preliminary tests presented in this section aim to analyse the performance and scalability of the solution and evaluate the impact on application execution in terms of metrics collection overhead.
