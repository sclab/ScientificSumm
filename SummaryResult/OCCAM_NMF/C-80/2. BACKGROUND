no re-hashing of permanent storage is required since ganesh only uses hashing on volatile data. 
sha256) only add a few extra bytes and the generated hashes are still orders of magnitude smaller than the data items they represent.
there would be no impact on performance as stronger hash functions (e.g.
while we currently use sha-1, replacing it with a different hash function would be simple.
further, ganesh does not depend critically on any specific hash function.
all communication is between trusted parts of the system and an adversary has no way to force ganesh to  accept invalid data.
however, as  explained by black [5], we believe that these issues do not form a concern for ganesh.
the assumption that collisions are so rare as to be effectively non-existent has recently come under fire [17].
a collection of data items  effectively becomes content-addressable, allowing a small hash to serve as a codeword for a much larger data item in permanent storage or network transmission.
hence, they are able to treat the hash of a data item as its unique identifier.
the above assumptions allow hash-based systems to assume that collisions do not occur.
[23] provide more details about these assumptions.
menezes et al.
the functions are also assumed to be one-way; that is, finding an  input that results in a specific output is computationally infeasible.
in other words, it is  computationally intractable to find two inputs that hash to the same output.
cryptographic hash functions are  assumed to be collision-resistant.
the use of hash-based techniques to reduce the volume of data transmitted has emerged as a common theme of many recent  storage systems, as discussed in section 8.2. these techniques rely on some basic assumptions.
as sql queries can generate large results, hash-based techniques lend themselves well to the problem of efficiently transferring these large results across bandwidth constrained links.
2.2 hash-based systems ganesh"s focus is on efficient transmission of results by  discovering similarities with the results of previous queries.
in the absence of both caching and replication, wan bandwidth can easily become a limiting factor in the performance and  scalability of data-intensive applications.
thus, the back-end database is usually located far from many sets of first and second-tier nodes [2].
as a result, databases tend to be centralized to meet the strong consistency  requirements of many ebusiness applications such as banking,  finance, and online retailing [38].
while databases can be  easily replicated in a lan, this is infeasible in a wan because of the difficult task of simultaneously providing strong consistency, availability, and tolerance to network partitions [7].
content that is generated dynamically from the back-end database cannot be cached in the first two tiers.
it can also increase the availability and scalability of web services.
this improves user experience by lowering end-to-end latency and reducing exposure www 2007 / track: performance and scalability session: scalable systems for dynamic content 311 back-end database server front-end web and application servers figure 1: multi-tier architecture to backbone traffic congestion.
the first two tiers can be replicated close to a  concentration of clients at the edge of the internet.
figure 1 illustrates this architecture.
today, ebusiness systems often use a three-tiered architecture consisting of a front-end web server, an application server, and a back-end database server.
2.1 dynamic content generation as the world wide web has grown, many web sites have  decentralized their data and functionality by pushing them to the edges of the internet.
