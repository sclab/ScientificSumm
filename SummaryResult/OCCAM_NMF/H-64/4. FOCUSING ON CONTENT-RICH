this metadata informed our learning and evaluation, as described in section 6.1. 
additionally, a subset of the documents (1112) contained a subject heading.
each of the 1279 documents  contains a list of one or more keywords.
finally, the editor"s desk column provided an ideal learning environment because it is well-supplied with topical metadata.
each  document is highly topical, further aiding the discovery of  termtopic relations.
), the editor"s desk  documents are all written in clear, journalistic prose.
tables, lists, etc.
while the entire bls collection contains a great deal of  nonprose text (i.e.
it also allowed the clustering process to learn on a relatively clean data set.
limiting attention to these 1279 documents not only  reduced the dimensionality of the problem.
the editor"s desk column has been written daily (five times per week) since 1998. as such, we operated on a set of n = 1279 documents.
the column is published daily, and each entry describes an important current issue in the bls domain.
bls agents suggested that we focus on the  editor"s desk because it is intended to span the intellectual domain of the agency.
these are brief articles, written by bls employees.
in consultation with employees of the bls, we continued our analysis on documents that form a series titled from the editor"s desk6 .
to remedy the high dimensionality of the data, we  resolved to limit the algorithm to a subset of the collection.
these words are certainly related, but they are related at a level of specificity far below what we sought.
for instance, the 80-cluster solution derived a cluster whose most highly associated words (in terms of log-odds ratio [1]) were drug, pharmacy, and chemist.
moreover, the granularity of these clusters was  unsuitably fine.
in future work we will turn to model-based clustering [7] as a more principled method of selecting the number of clusters and of representing clusters.
because we need only a flat clustering there is little to be gained by the more expensive hierarchical algorithms.
chief among these is the computational efficiency enjoyed by the k-means approach.
however, 80 clusters are far too many for application to an interface such as the relation 5 we have focused on k-means as opposed to other clustering algorithms for several reasons.
this paramterization led to  semantically intelligible clusters.
when applying k-means to our 15,000 document collection,  indicators such as the gap statistic [17] and an analysis of the mean-squared distance across values of k suggested that k ≈ 80 was optimal.
however, k-means clustering requires that the  researcher specify k, the number of clusters to define.
[8, 16]).
clustering by k-means is well-studied in the statistical literature, and has shown good results for text analysis (cf.
readers interested in the details of the algorithm are referred to [12] for a  thorough treatment of the subject.
though the algorithms for  accomplishing such an arrangement are legion, our analysis focuses on k-means clustering5 , during which, each  observation oi is assigned to the cluster ck whose centroid is closest to it, in terms of euclidean distance.
as described in [12], the goal of cluster analysis is to assign each of the n observations to one of a small number k groups, each of which is characterized by high intra-cluster correlation and low inter-cluster correlation.
thus aij shows the measurement for the ith observation on the jth variable.
let a be the n×p data matrix with n  observations in p variables.
documents to derive empirically evidenced topics we initially turned to cluster analysis.
