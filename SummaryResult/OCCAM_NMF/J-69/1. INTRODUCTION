we discuss related work in section 5 and conclude in section 6. 
in section 4.4, we show how traitors disrupt cooperation and how short-term  history deals with them.
in section 4.3, we present the problem of zero-cost identities and show how an  adaptive stranger policy promotes persistent identities.
in section 4.2, we describe collusion and  demonstrate how subjective reputation mitigates it.
we describe the model in section 2 and the reciprocative decision function in section 3. we then proceed to the incentive techniques in section 4. in section 4.1, we describe the challenges of large populations and high turnover and show the effectiveness of discriminating server selection and shared history.
we show that short-term  history prevents traitors from disrupting cooperation.
the rest of the paper is organized as follows.
long-term history exacerbates this problem by  allowing peers with many previous transactions to exploit that  history for many new transactions.
the peer could be making a strategic decision or someone may have hijacked her identity (e.g., by compromising her host).
• short-term history: history also creates the possibility that a previously well-behaved peer with a good reputation will turn traitor and use his good reputation to exploit other peers.
the adaptive stranger policy does this without requiring centralized allocation of identities, an entry fee for newcomers, or rate-limiting [13] [9] [25].
we show that if reciprocative peers treat strangers (peers with no history) using a policy that adapts to the  behavior of previous strangers, peers have little incentive to whitewash and whitewashing can be nearly eliminated from the system.
• adaptive stranger policy: zero-cost identities allows  noncooperating peers to escape the consequences of not  cooperating and eventually destroy cooperation in the system if not stopped.
we show that the maxflow-based  algorithm scales better than private history in the presence of colluders without the centralized trust required in previous work [9] [20].
to eliminate this cost, we have developed a constant mean running time variation, which trades effectiveness for  complexity of computation.
the basic idea is that b would only believe c if c had already provided  service to b. the cost of the maxflow algorithm is its o(v 3 ) running time, where v is the number of nodes in the system.
we show that a maxflow-based algorithm that computes reputation subjectively promotes cooperation despite collusion among 1/3 of the population.
in the example in figure 1, c can falsely claim that a served him, thus deceiving b into providing service.
• maxflow-based subjective reputation: shared history  creates the possibility for collusion.
the cost of shared history is a distributed  infrastructure (e.g., distributed hash table-based storage) to store the history.
we show that with shared history, b knows that a served c and consequently will serve a. this results in a higher level of cooperation than with  private history.
however, if everyone keeps only private history, no one will provide service because b does not know that a has served c, etc.
consider the example in figure 1. if everyone provides service, then the system operates optimally.
• shared history: scaling to higher turnover and mitigating asymmetry of interest requires shared history.
we show that by having each peer keep a 102 private history of the actions of other peers toward her, and using discriminating server selection, the reciprocative  decision function can scale to large populations and moderate levels of turnover.
however, the large populations and high turnover of p2p systems makes it less likely that repeat interactions will occur with a familiar entity.
therefore, we propose a family of scalable and robust incentive  techniques, based upon a novel reciprocative decision function, to  address these challenges and provide different tradeoffs: • discriminating server selection: cooperation requires  familiarity between entities either directly or indirectly.
strategies that work well in traditional prisoners" dilemma games such as tit-for-tat [4] will not fare well in the p2p context.
• zero-cost identity: many p2p systems allow peers to  continuously switch identities (i.e., whitewash).
in the example in figure 1, a wants service from b, b wants service from c, and c wants service from a.
• asymmetry of interest: asymmetric transactions of p2p systems create the possibility for asymmetry of interest.
while social dilemmas have been studied extensively, p2p  applications impose a unique set of challenges, including: • large populations and high turnover: a file sharing  system such as gnutella and kazaa can exceed 100, 000  simultaneous users, and nodes can have an average life-time of the order of minutes [33].
in particular, we use a prisoners" dilemma model to capture the  essential tension between individual and social utility, asymmetric payoff matrices to allow asymmetric transactions between peers, and a learning-based [14] population dynamic model to specify the behavior of individual peers, which can be changed continuously.
we adopt a game-theoretic approach in addressing this problem.
avoiding this tragedy of the commons [18] requires incentives for cooperation.
a wants service from b, b wants service form c, and c wants service from a. utility of the system.
as a result, each user"s  attempt to maximize her own utility effectively lowers the overall a bc figure 1: example of asymmetry of interest.
in many of these systems, users have natural disincentives to cooperate because cooperation consumes their own resources and may degrade their own performance.
further examples are file preservation [25], discussion boards [17], online auctions [16], and overlay routing [6].
in a wireless ad-hoc network, overall packet  latency and loss rate increase when nodes refuse to forward packets on behalf of others [26].
for example, in a file-sharing system, overall download latency and failure rate increase when users do not share their resources [3].
many peer-to-peer (p2p) systems rely on cooperation among  selfinterested users.
