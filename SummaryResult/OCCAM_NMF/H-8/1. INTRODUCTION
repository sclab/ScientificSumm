the amount of data available to researchers would grow  exponentially over time. 
research groups would be able to share the relevance judgments they have done in-house for pilot studies, new tasks, or new topics.
small, reusable test collections could have a huge impact on information retrieval research.
any set of judgments, no matter how small, becomes reusable to some degree.
even if confidence is not high, as long as we can trust it, we can identify which systems need more  judgments in order to increase confidence.
the real question is how much confidence we have in our evaluations, and, more importantly, whether we can trust our estimates of confidence.
a malicious  adversary can always produce a new ranked list that has not retrieved any of the judged documents.
specifically, the question of reusability is not how  accurately we can evaluate new systems.
we need a more careful definition of reusability.
while we can say that it was right 75% of the time, or that it had a rank correlation of 0.8, these numbers do not have any  predictive power: they do not tell us which systems are likely to be wrong or how confident we should be in any one.
in previous work,  reusability has been tested by simply assessing the accuracy of a set of relevance judgments at evaluating unseen systems.
first we must formally define what it means to be reusable.
returning to our hypothetical resesarcher, can she reuse her relevance judgments?
as we will see, the judgments these methods produce can significantly bias the evaluation of a new set of systems.
[4], among others.
[8], and aslam et al.
[11], zobel [21], sanderson and joho [17], carterette et al.
as a result, there have been a slew of recent papers on reducing annotator effort in producing test collections: cormack et al.
the pooling method gives thousands of relevance judgments, but it requires many hours of (paid) annotator time.
this solution is not adequate for our hypothetical  researcher.
this method creates large sets of judgments that are reusable for training or evaluating new systems that did not contribute to the pool [21].
the solution used by nist at trec (text retrieval conference) is the pooling method [19, 20]: all competing systems contribute n documents to a pool, and every document in that pool is judged.
evaluation is an important aspect of information retrieval research, but it is only a semi-solved problem: for most retrieval tasks, it is impossible to judge the relevance of every document; there are simply too many of them.
can they evaluate without more relevance judgments?
can they reliably reuse the original judgments?
or another research group decides to implement a system to perform the task?
but what happens when she develops a new system and needs to evaluate it?
she can only judge the documents that seem to be the most  informative and stop when she has a reasonable degree of confidence in her conclusions.
she does not have the time or resources to judge every document, or even every retrieved document.
since the task is new, it is unlikely that there are any extant relevance  judgments.
she has built a system to  perform the task and wants to evaluate it.
consider an information retrieval researcher who has  invented a new retrieval task.
