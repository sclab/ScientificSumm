in our experiments, we will see that this further specification of sub-domain is necessary in some cases, but not in all, especially when feedback model is also used. 
this approach is indeed a combination of domain and feedback models.
these documents are the top-ranked documents retrieved with the original query within the domain.
therefore, we further construct a  sub-domain model more related to the given query, by using a subset of in-domain documents that are related to the query.
using such a large domain model as the background can introduce much noise terms.
this is particularly true for large domains such as science and technology defined in trec queries.
although domain models are more refined than a single user profile, the topics in a single domain can still be very different, making the domain model too large.
the same strategy is used for knowledge model.
therefore, we only retain the top 100 strongest terms.
the score depending on the domain model is then as follows: ∑∈ = vt d dom qdom tptpdqscore )|(log)|(),( θθ (8) although the above equation requires using all the terms in the vocabulary, in practice, only the strongest terms in the domain model are useful and the terms with low probabilities are often noise.
: )|(log)|(minarg 0 dom qt q dom q tptp dom θθθ θ ∑∈ = (7) this classification method is an extension to naïve bayes as shown in [22].
in this study, we use a simple classification method: the selected domain is the one with which the query"s kl-divergence score is the lowest, i.e.
query classification has been investigated in several studies [18][28].
we will compare both approaches.
this can be done manually by the user or automatically by the system using query classification.
table 1. term probabilities before/after em term initial final change term initial final change air 0.00358 0.00558 + 56% year 0.00357 0.00052 - 86% environment 0.00213 0.00340 + 60% system 0.00212 7.13*e-6 - 99% rain 0.00197 0.00336 + 71% program 0.00189 0.00040 - 79% pollution 0.00177 0.00301 + 70% million 0.00131 5.80*e-6 - 99% storm 0.00176 0.00302 + 72% make 0.00108 5.79*e-5 - 95% flood 0.00164 0.00281 + 71% company 0.00099 8.52*e-8 - 99% tornado 0.00072 0.00125 + 74% president 0.00077 2.71*e-6 - 99% greenhouse 0.00034 0.00058 + 72% month 0.00073 3.88*e-5 - 95% given a set of domain models, the related ones have to be assigned to a new query.
this can be observed in the following table, which shows some words in the domain model of environment before and after em iterations (50 iterations).
it is able to extract the most specific words of the domain from the documents while filtering out the common words of the language.
the em algorithm is used to extract the domain model domθ that maximizes p(dom| θ"dom) (where dom is the set of documents in the domain), that is: ( ) ( ) ( ) ( )[ ] ( ) ∏ ∏∈ ∈ +−= = domd dt dtc cdom domdom tptp domp dom dom ; ' ||1maxarg |maxarg θηθη θθ θ θ (6) this is the same process as the one used to extract feedback model in [35].
then the likelihood of a document in the domain can be formulated as follows: ( ) ( ) ( ) ( )[ ] ( ) ∏∈ +−= dt dtc cdomdom tptpdp ; ||1'| θηθηθ (5) where c(t; d) is the count of t in document d and η is a smoothing parameter (which will be fixed at 0.5 as in [35]).
therefore, we employ an em process to extract the specific part of the domain as follows: we assume that the documents in a domain are generated by a domain-specific model (to be extracted) and general language model (collection model).
if maximum likelihood estimation is used directly on these documents, the resulting domain model will contain both  domain-specific terms and general terms, and the former do not emerge.
whatever the strategy, we will obtain a set of documents for each domain, from which a language model can be extracted.
this will be explained in detail in our experiments.
this choice is also motivated by the possibility to compare between manual and automatic assignment of domain to a new query.
therefore, in this study, we only use the second approach.
it is found that both approaches mentioned above are equally effective and result in comparable performance.
these domains have been mapped to odp categories.
an earlier study [4] has compared the above two strategies using trec queries 51-150, for which a domain has been manually assigned.
the answers could be those that the user have read, browsed through, or judged relevant to an in-domain query, or they can be simply the top-ranked retrieval results.
by assigning a domain to his queries, the system can gather a set of answers to the queries automatically, which are then considered to be in-domain documents.
2) a user can define his own domains.
in that case, a new query should be classified into the same domains either manually or automatically.
these documents can be identified in two different ways: 1) one can take advantages of an existing domain hierarchy and the documents manually classified in them, such as odp.
models as in previous studies, we exploit a set of documents already classified in each domain.
