sarit kraus is also affiliated with umiacs. 
it is also supported by the  defense advanced research projects agency (darpa), through the  department of the interior, nbc, acquisition services division, under contract no.
acknowledgments : this research is supported by the united states department of homeland security through center for risk and economic analysis of terrorism events (create).
however most of this work is focused on either limiting energy consumption involved in patrolling [7] or optimizing on criteria like the length of the path traveled [4, 13], without reasoning about any explicit model of an adversary[14].
finally the patrolling problem which motivated our work has  recently received growing attention from the multiagent community due to its wide range of applications [4, 13].
this contrasts with asap, where our emphasis is on a highly efficient heuristic approach that is not focused on equilibrium solutions.
while that work provides epsilon error-bounds based on the k-uniform strategies, their solution concept is still that of a nash equilibrium, and they do not provide efficient algorithms for obtaining such k-uniform strategies.
our k-uniform strategies are similar to the k-uniform strategies of [12].
we provide an efficient mixed integer linear program (milp) implementation for asap, along with experimental results illustrating significant speedups and higher rewards over other approaches.
third, it operates directly on the compact, bayesian game  representation, without requiring conversion to normal form.
second, it provides  strategies which are simple to understand, represent, and implement.
first, asap searches for the highest reward strategy, rather than a bayes-nash  equilibrium, allowing it to find feasible strategies that exploit the  natural first-mover advantage of the game.
therefore, we present a heuristic called asap, with three key advantages towards addressing this problem.
however, the complexity of this problem was shown to be np-hard in the general case [5], which also provides algorithms for this problem in the non-bayesian case.
less attention has been paid to finding the optimal strategy to commit to in a bayesian game (the stackelberg scenario [15]).
on autonomous agents and multi-agent systems (aamas 07) 317 figure 3: reward for asap using multisets of 10, 30, and 80 elements subclasses of bayesian games, finding single bayes-nash  equilibria for general bayesian games [10] or approximate bayes-nash equilibria [18].
joint conf.
much work has been done on finding optimal bayes-nash equilbria for the sixth intl.
the gala toolkit is one method for defining such games [9] without requiring the game to be represented in normal form via the harsanyi transformation [8]; gala"s guarantees are focused on fully competitive games.
bayesian games have been a popular choice to model such incomplete information games [3].
agents acting in the real world quite frequently have such incomplete information about other agents.
specifically, we deal with situations where the adversaries" actions and payoffs are known but the exact  adversary type is unknown to the security agent.
in these environments, intentional threats are caused by adversaries about whom the security patrolling agents have  incomplete information.
this paper focuses on security for agents patrolling in hostile  environments.
