based on the test results we decided to use the rocchio"s classifier on both the ad and the page side. 
it is an area of current investigation how to improve the classification using a noisy training set.
this is  probably a result of the noise induced by using a search engine to machine generate training pages for the svm and  logregression classifiers.
the tests showed that for both ads and pages the rocchio classifier returned the best results, especially on the page side.
we conducted tests using professional editors to judge the quality of page and ad class assignments.
as the score of the max class is normalized with regard to document length, the scores for different documents are comparable.
these weights are based on the standard tf-idf formula.
the terms ci and di represent the weight of the ith feature in the class centroid and the document respectively.
the score is normalized by the document and class length to produce comparable score.
the classification is based on the cosine of the angle  between the document d and the centroid meta-documents: cmax = arg max cj ∈c cj cj · d d = arg max cj ∈c p i∈|f | ci j· di qp i∈|f |(ci j)2 qp i∈|f |(di)2 where f is the set of features.
each centroid is defined as a sum of the tf-idf values of each term, normalized by the number of queries in the class cj = 1 |cj| x q∈cj q q where cj is the centroid for class cj; q iterates over the queries in a particular class.
we then used the meta document as a centroid for a nearest-neighbor classifier based on rocchio"s  framework [9] with only positive examples and no relevance  feedback.
for each taxonomy node we concatenated all the exemplary queries into a single  metadocument.
however, we obtained the best performance by using the third document classifier, based on the information in the source taxonomy queries only.
this filtering eliminates low content pages, pages deemed unsuitable for advertising, pages that lead to excessive class confusion, etc.)
(the  second method differs from the first in the type of secondary filtering used.
using this training sets we then trained a hierarchical svm [2] (one against all between every group of siblings) and a log-regression [11] classifier.
on the ad side we generated a training set for each class by selecting the ads that have a bid phrase  assigned to this class.
for the first two methods we tried to find exemplary pages and ads for each concept as follows: number of categories by level 0 200 400 600 800 1000 1200 1400 1600 1800 2000 1 2 3 4 5 6 7 8 9 level numberofcategories number of children per nodes 0 50 100 150 200 250 300 350 400 0 2 4 6 8 10 12 14 16 18 20 22 24 29 31 33 35 52 number of children numberofnodes queries per node 0 500 1000 1500 2000 2500 3000 1 50 80 120 160 200 240 280 320 360 400 440 480 number queries (up to 500+) numberofnodes figure 1: taxonomy statistics: categories per level; fanout for non-leaf nodes; and queries per node we generated a page training set by running the queries in the taxonomy over a web search index and using the top 10 results after some filtering as documents labeled with the query"s label.
given the taxonomy of queries (or bid-phrases - we use these terms interchangeably) described in the previous  section, we tried three methods to build corresponding page and ad classifiers.
the detailed description and evaluation of this process is outside the scope of this paper.
in this section we overview the methods we used to build a page and an ad classifier pair.
thus we need to classify pages into the same taxonomy used to classify ads.
4.2 classification method as explained, the semantic phase of the matching relies on ads and pages being topically close.
figure 1 provides some statistics about the taxonomy used in this work.
however, the details and tools are completely different.
a similar-in-spirit process for  building enterprise taxonomies via queries has been presented in [6].
nevertheless, it has been a significant effort to  develop this 6000-nodes taxonomy and it has required several person-years of work.
after  initial seeding with a few queries, using the provided tools a human editor can add several hundreds queries to a given node.
the taxonomy has been populated by human editors  using keyword suggestions tools similar to the ones used by ad networks to suggest keywords to advertisers.
the queries placed in the taxonomy are high  volume queries and queries of high interest to advertisers, as indicated by an unusually high cost-per-click (cpc) price.
each node has on average around 100 queries.
each node in our source taxonomy is represented as a  collection of exemplary bid phrases or queries that correspond to that node concept.
we will explain below how we can use the same taxonomy to classify pages and ads as well.
this taxonomy has been commercially built by yahoo!
to obtain sufficient resolution, we used a taxonomy of around 6000 nodes primarily built for classifying commercial interest queries, rather than pages or ads.
the ads suitable for these two concepts are, however, very  different.
for example, classifying all medical related pages into one node will not result into a good classification since both sore foot and flu pages will end up in the same node.
the  matching process requires that the taxonomy provides sufficient differentiation between the common commercial topics.
4.1 taxonomy choice the semantic match of the pages and the ads is performed by classifying both into a common taxonomy.
