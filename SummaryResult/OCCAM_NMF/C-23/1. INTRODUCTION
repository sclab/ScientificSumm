our research approaches are outlined in section 5, and experimental results and a performance evaluation of our scheme are presented in section 6. section 7 concludes this research paper. 
related background review and studies are presented in section 2 and the co-allocation architecture and related work are introduced in section 3. in section 4, an efficient replica selection service is proposed by us.
the remainder of this paper is organized as follows.
we also discuss combination cost and provide an effective scheme for reducing it.
experimental results show that our approach is superior to previous methods and achieved the best overall performance.
in this paper, we propose a dynamic co-allocation scheme based on co-allocation grid data transfer architecture called  recursiveadjustment co-allocation scheme that reduces the idle time spent waiting for the slowest server and improves data transfer performance [24].
therefore, it is important to reduce the differences in finishing time among replica servers.
an idle-time drawback remains since faster servers must wait for the slowest server to deliver its final block.
several co-allocation strategies were provided in previous work [17].
this can improve the performance compared to the single-server cases and alleviate the internet congestion problem [17].
co-allocation of data transfers enables the clients to download data from multiple locations by establishing multiple connections in parallel.
another way is to use co-allocation technology [17] to download data.
this method selects the best servers to provide optimum transfer rates because bandwidth quality can vary unpredictably due to the sharing nature of the internet.
one way to improve download speeds is to determine the best replica locations using replica selection techniques [19].
bandwidth quality is the most important factor affecting transfers between clients and servers since download speeds are limited by the bandwidth traffic congestion in the links connecting the servers to the clients.
downloading large datasets from several replica locations may result in varied performance rates, because the replica sites may have different architectures, system loadings, and network connectivity.
recently, large-scale, data-sharing scientific communities such as those described in [1, 5] used this technology to replicate their large datasets over several sites.
replicating popular content in distributed servers is widely used in practice [14, 17, 19].
the data grid infrastructure integrates data storage devices and data management services into the grid environment, which consists of scattered computing and storage resources, perhaps located in different countries/regions yet accessible to users [12].
certain data-intensive scientific applications, such as high-energy physics, bioinformatics applications and virtual astrophysical observatories, entail huge amounts of data that require data file management systems to replicate files and manage data transfers and distributed data access.
most data grid applications execute simultaneously and access large numbers of data files in the grid environment.
data grids aggregate distributed resources for solving large-size dataset management problems.
