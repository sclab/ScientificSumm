any smoothing methods for the document language model can be used to estimate p(·|d) such as the gamma smoothing as discussed in section 2.2.1. the empirical study of the smoothing methods is  presented in section 4. 
with no prior knowledge on p(·|u), we could set it to p(·|c).
we may then score each document with the query likelihood computed using the  following two-stage smoothing model: p(c(w, q)|λd, u) = (1 − δ)p(c(w, q)|λd) + δp(c(w, q)|u) (4) where δ is a parameter, roughly indicating the amount of noise in q. this looks similar to the interpolation  smoothing, except that p(·|λd) now should be a smoothed language model, instead of the one estimated with mle.
p(·|u) models the typical term frequencies in the user"s queries.
in order to distinguish the content and non-discriminative words in a query, we follow [29] and assume that a query is generated by sampling from a two-component mixture of poisson language models, with one component being the document model λd and the other being a query background language model p(·|u).
2.2.3 two-stage smoothing as discussed in [29], smoothing plays two roles in retrieval: (1) to improve the estimation of the document language model, and (2) to explain the common terms in the query.
as the second summation is difficult to compute efficiently, we conflate all non-query terms as one pseudo  non-queryterm, denoted as n. using the pseudo-term formulation and a poisson collection model, we can rewrite the retrieval formula as score(d, q) ∝ w∈d∩q log(1 + 1 − δ δ e−λd,w (λd,w|q|)c(w,q) e−λd,c |q| (λd,c )c(w,q) ) + log (1 − δ)e−λd,n |q| + δe−λc,n |q| 1 − δ + δe−λc,n |q| (3) where λd,n = |d|− w∈q c(w,d) |d| and λc,n = |c|− w∈q c(w,c) |c| .
the second summation can be actually treated as a document prior, which penalizes long documents.
in the retrieval formula above, the first summation can be computed efficiently.
· p(c(w, q)|c) ) − log (1 − δ)e−λd,w|q| + δp(c(w, q) = 0|c) 1 − δ + δp(c(w, q) = 0|c) ] + w∈d log (1 − δ)e−λd,w|q| + δp(c(w, q) = 0|c) 1 − δ + δp(c(w, q) = 0|c) we can also use a poisson language model for p(·|c), or use some other frequency-based models.
with this simple  interpolation, we can score a document with score(d, q) = w∈v log((1 − δ)p(c(w, q)|d) + δp(c(w, q)|c)) (2) using the maximum likelihood estimator for p(·|d), we have λd,w = c(w,d) |d| , thus equation 2 becomes score(d, q) ∝ w∈d∩q [log(1 + 1 − δ δ e−λd,w|q| (λd,w|q|)c(w,q) c(w, q)!
one is the document language model estimated with  maximum likelihood estimator, and the other is a model  estimated from the collection background, p(·|c), which assigns non-zero rate to w. for example, we may use an interpolation coefficient  between 0 and 1 (i.e., δ ∈ [0, 1]).
2.2.2 interpolation (jelinek-mercer) smoothing another straightforward method is to decompose the query generation model as a mixture of two component models.
the posterior distribution of λd is given by p(λd|d, c) ∝ w∈v e−λw(|d|+µ) λ c(w,d)+µλc,w−1 w which is a product of |v | gamma distributions with  parameters c(w, d) + µλc,w and |d| + µ for each word w. given that the gamma mean is α β , we have ˆλd,w = λd,w λd,wp(λd,w|d, c)dλd,w = c(w, d) + µλc,w |d| + µ this is precisely the smoothed estimate of multinomial language model with dirichlet prior [28].
we use bayesian estimation with the following gamma prior, which has two parameters, α and β: gamma(λ|α, β) = βα γ(α) λα−1 e−βλ for each term w, the parameters αw and βw are chosen to be αw = µ ∗ λc,w and βw = µ, where µ is a parameter and λc,w is the rate of w estimated from some background language model, usually the collection language model.
the rate of a term is estimated  independently of other terms.
given a particular document d, we want to estimate λd.
a document is assumed to be generated from a  potentially different model.
2.2.1 bayesian smoothing using gamma prior following the risk minimization framework in [11], we  assume that a document is generated by the arrival of terms in a time period of |d| according to the document language model, which essentially consists of a vector of poisson rates for each term, i.e., λd = λd,1, ..., λd,|v | .
instead, we only need to guarantee that k=0,1,2,... p(c(w, d) = k|d) = 1. in this section, we introduce three different strategies to smooth a poisson language model, and show how they lead to different retrieval functions.
thus we do not have to discount the probability of seen words in order to give a non-zero rate to an unseen word.
in poisson  language models, however, we do not have the same constraint as in a multinomial model (i.e., w∈v p(w|d) = 1).
in general, we have to discount the  probabilities of some words seen in the text to leave some extra  probability mass to assign to the unseen words.
2.2 smoothing in poisson retrieval model in general, we want to assign non-zero rates for the query terms that are not seen in document d. many smoothing methods have been proposed for multinomial language  models[2, 28, 29].
as in existing language modeling approaches, the main challenge of constructing a reasonable retrieval model is to find a smoothed language model for p(·|d).
a document can be scored with the likelihood in  equation 1. however, if a query term is unseen in the document, the mle of the poisson distribution would assign zero  probability to the term, causing the probability of the query to be zero.
in our experiments, we adopt this pseudo non-query term strategy.
in other words, we may assume that there is exactly one non-query term in the vocabulary for each query.
as a compromise, we may conflate all the non-query terms as one single pseudo term.
in the other extreme, we may focus on the terms in the query and ignore other terms, but some useful information may be lost by ignoring the  nonquery terms.
however, this may bring in noise and  considerable computational cost.
in one extreme, we can use the vocabulary of the whole collection.
in practice, we have the flexibility to choose the  vocabulary v .
the likelihood that a query q is generated from the document language model λd can be written as p(q|d) = w∈v p(c(w, q)|λd) (1) this representation is clearly different from the multinomial query generation model as (1) the likelihood includes all the terms in the vocabulary v , instead of only those appearing in q, and (2) instead of the appearance of terms, the event space of this model is the frequencies of each term.
given a document d, we may estimate a poisson language model λd using d as a sample.
the  maximum likelihood estimate (mle) of λi is ˆλi = d∈d c(wi, d) d∈d w ∈v c(w , d) note that this mle is different from the mle for the  poisson distribution without considering the document lengths, which appears in [22, 24].
let d = {d1, ..., dm} be an observed set of document  samples generated from the poisson process above.
we refer to these n independent poisson processes with parameter λ as a poisson language model.
where λ = {λ1, ..., λn} and |w| = n i=1 c(wi, w).
with n such independent poisson processes, each  explaining the generation of one term in the vocabulary, the  likelihood of w to be generated from such poisson processes can be written as p(w|λ) = n i=1 p(c(wi, w)|λ) = n i=1 e−λi·|w| (λi · |w|)c(wi,w) c(wi, w)!
without losing generality, we set t to the length of the text w (people write one word in a unit time), i.e., t = |w|.
the probability density function of such a poisson distribution is given by p(c(wi, w) = k|λit) = e−λit (λit)k k!
with a homogeneous poisson process, the frequency count of each event, i.e., the number of  occurrences of wi, follows a poisson distribution with associated parameter λit, where λi is a rate parameter characterizing the expected number of wi in a unit time.
suppose t is the time period during which the author  composed the text.
we consider the frequency counts of the n unique terms in w as n different types of events, sampled from n independent homogeneous poisson processes, respectively.
let w be a piece of text composed by an author and c(w1), ..., c(wn) be a frequency vector representing w, where c(wi, w) is the frequency count of term wi in text w. in retrieval, w could be either a query or a document.
2.1 the generation process let v = {w1, ..., wn} be a vocabulary set.
alternatively, we assume that a query is generated by sampling the frequency of words from a series of independent poisson processes [20].
in most existing work [12, 6, 28, 29], people assume that each query word is sampled independently from a multinomial distribution.
process in the query generation framework, a basic assumption is that a query is generated with a model estimated based on a document.
