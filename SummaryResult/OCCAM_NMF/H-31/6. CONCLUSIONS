it is also appealing to find  robust methods to learn the per-term smoothing coefficients without additional computation cost. 
further exploring the flexibilities over multinomial language models, such as length normalization and pseudo-feedback could be good future work.
our work opens up many interesting  directions for further exploration in this new family of models.
furthermore, we show that a  mixture background model for poisson can be used to improve the performance and robustness over the standard poisson background model.
we exploit this property to develop a new per-term smoothing algorithm for poisson language models, which is shown to outperform term-independent smoothing for both poisson and multinomial models.
in  particular, we show that poisson has an advantage over  multinomial in naturally accommodating per-term smoothing.
our analysis shows that while our new models and multinomial models are equivalent under some assumptions, they are generally different with some important differences.
we compare the new models with the popular multinomial retrieval models both analytically and experimentally.
we  derive several smoothing methods for this family of models, including single-stage smoothing and two-stage smoothing.
we present a new family of query generation language models for retrieval based on poisson distribution.
