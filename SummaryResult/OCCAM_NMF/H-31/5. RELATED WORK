we show that  poisson language model is natural to accommodate the per-term smoothing without heuristic twist of the semantics of a  generative model, and is able to efficiently better model the mixture background, both analytically and empirically. 
[7] analytically shows that term specific smoothing could be useful.
language model smoothing [2, 28, 29] and background structures [15, 10, 25, 27] have been studied with  multinomial language models.
once again, none of this work explores poisson distribution in the query generation framework.
poisson mixtures [3] such as 2-poisson [22], negative multinomial, and katz"s  kmixture [9] has shown to be effective to model and retrieve documents.
[26] introduces a way to empirically search for an exponential model for the documents.
however, the poisson model in their paper is still under the document generation  framework, and also does not account for the document length variation.
[24] studies the parallel derivation of three  different retrieval models which is related to our comparison of poisson and multinomial.
poisson distribution has been previously studied in the document generation models [16, 22, 3, 24], leading to the development of one of the most effective retrieval formula bm25 [23].
we introduce a new family of language models, based on poisson  distribution.
all existing query generation language models are based on either multinomial distribution [19, 6, 28, 13] or  multivariate bernoulli distribution [21, 17, 18].
the most popular and  fundamental one is the query-generation language model [21, 13].
language models have been shown to be effective for many retrieval tasks [21, 28, 14, 4].
to the best of our knowledge, there has been no study of query generation models based on poisson distribution.
