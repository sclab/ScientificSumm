this demonstrates that the proposed technique can efficiently handle large-scale system like  netflix. 
the system finished one modified em  iteration in about 4 hours.
we evaluated the algorithm on the whole netflix data set (480,189 users, 159,836 features, and 100 million ratings) running on a single cpu pc (2gb  memory, p4 3ghz).
our results show that the modified em algorithm converges quickly, and 2 - 3 modified em iterations would result in a reliable estimation.
1 2 3 4 5 0.011 0.0115 0.012 0.0125 0.013 0.0135 0.014 iterations meansquareerror new algorithm traditional em 1 2 3 4 5 0.0102 0.0104 0.0106 0.0108 0.011 0.0112 0.0114 iterations classificationerror new algorithm traditional em em, can it really learn millions of user models quickly?
performances on reuters-c, reuters-m, reuters-g are similar.
1 6 11 16 21 0.5 1 1.5 2 2.5 3 3.5 iterations meansquareerror new algorithm traditional em 1 6 11 16 21 0.35 0.4 0.45 0.5 0.55 0.6 0.65 iterations classificationerror new algorithm traditional em figure 4: performance on a reuters-e subset with 26 profiles.
the new algorithm is statistical significantly better than em algorithm at iteration 2 to 17 (evaluated with mean square error).
0 2 4 6 8 10 1 1.05 1.1 1.15 1.2 1.25 1.3 1.35 1.4 iterations meansquareerror new algorithm traditional em 1 2 3 4 5 6 7 8 9 10 0.33 0.34 0.35 0.36 0.37 0.38 0.39 iterations classificationerror new algorithm traditional em figure 3: performance on a movielens subset with 1,000 users.
the new algorithm is statistical significantly better than em algorithm at iterations 2 - 10. norm-2 regularized linear models are equivalent to the bayesian hierarchical models learned at the first iteration, and are statistical significantly worse than the bayesian hierarchical models.
although the proposed technique is faster than standard figure 2: performance on a netflix subset with 1,000 users.
however, the experiments also show that when the assumption does not hold, the new algorithm does not hurt performance.
the results suggest that only on a corpus where the number of unrelated user-feature pairs is much larger than the number of related pairs, such as on the  netflix data set, the proposed technique will get a significant improvement over standard em.
thus the two learning algorithms  perform similarly.
since the number of unrelated user-feature pairs is not  extremely large, the sparseness is not a serious problem on the reuters data set.
further analysis shows that only 58% of the user-feature pairs are unrelated on this data set.
the general patterns are very similar on other reuters" subsets.
the accuracy of the new  algorithm is similar to that of the standard em algorithm at each iteration.
figure 4 shows that the two algorithms work similarly on the reuters-e data set.
this is not  surprising since the number of related feature-users pairs is much smaller than the number of unrelated feature-user pairs on these two data sets, and thus the proposed new algorithm is expected to work better.
figure 2 and figure 3 show that the proposed new  algorithm works better than the standard em algorithm on the netflix and movielens data sets.
however, the strength of the correlation differs over data sets, and the amount of  training data is not the only characteristics that will influence the final performance.
this suggests that the borrowing information from other users has more significant improvements for users with less  training data, which is as expected.
further analysis shows a negative correlation between the number of training data for a user and the improvement the system gets.
figure 2, figure 3, and figure 4 show that on all data sets, the bayesian hierarchical modeling approach has a  statistical significant improvement over the regularized linear regression model, which is equivalent to the bayesian  hierarchical models learned at the first iteration.
