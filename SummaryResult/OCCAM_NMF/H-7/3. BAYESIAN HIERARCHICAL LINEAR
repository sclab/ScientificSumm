the joint  likelihood for all the variables in the probabilistic model, which includes the data and the parameters, is: p(d, θ) = p(φ) m p(wm|φ) j p(ym,j|xm,j, wm) (1) for simplicity, we assume a, b, c, and σ are provided to the system. 
let θ = (φ, w1, w2, ..., wm ) represent the parameters of this system that needs to be estimated.
2. for each user m, wm is sampled randomly from a  normal distribution: wm ∼ n(µ, σ2 ) 3. for each item xm,j, ym,j is sampled randomly from a normal distribution: ym,j ∼ n(wt mxm,j, σ2 ).
the rating, y, for a  document, x, is conditioned on the document and the user model, wm, associated with the user m. users share information about their models through the prior, φ = (µ, σ).
2 the first dimension of x is a dummy variable that always equals to 1. figure 1: illustration of dependencies of variables in the hierarchical model.
with these settings, we have the following model for the system: 1. µ and σ are sampled from n(0, ai) and iwν (ai),  respectively.
i is the k dimensional identity matrix, and a, b, and c are real numbers.
usually, a normal distribution n(0, ai) and an inverse wishart distribution p(σ) ∝ |σ|− 1 2 b exp(−1 2 ctr(σ−1 )) are used as hyperprior to model the prior distribution of µ and σ respectively.
µ = (µ1, µ2, ..., µk ) is a k dimensional vector that represents the mean of the gaussian distribution, and σ is the covariance matrix of the gaussian.
assume that each user model wm is an independent draw from a population distribution p(w|φ), which is governed by some unknown hyperparameter φ. let the prior distribution of user model w be a gaussian distribution with parameter φ = (µ, σ), which is the commonly used prior for linear models.
now we look at one commonly used model where y = wt x + , where ∼ n(0, σ2 ) is a random noise [25][27].
to reliably estimate the user model wm, the  system can borrow information from other users through the prior φ = (µ, σ).
the model is called generalized bayesian  hierarchical linear model when y = f(wt x) is any generalized linear model such as logistic regression, svm, and linear regression.
the system can predict the user label y of a document x given an estimation of wm (or wm"s distribution) using a function y = f(x, w).
we assume a user model is sampled randomly from a prior distribution p(w|φ).
in this graph, each user model is  represented by a random vector wm.
figure 1 shows the graphical representation of a bayesian hierarchical model.
generalized bayesian hierarchical linear models, one of the simplest bayesian hierarchical models, are commonly used and have achieved good performance on collaborative  filtering [25] and content-based adaptive filtering [27] tasks.
k = 1, 2, ..., k: the dimensional index of input variable x. the bayesian hierarchical modeling approach has been widely used in real-world information retrieval applications.
j = 1, 2, ..., jm: the index for a set of data for user m. jm is the number of training data for user m. dm = {(xm,j, ym,j)}: a set of data associated with user m. xm,j is a k dimensional vector that represents the mth user"s jth training document.2 ym,j is a scalar that represents the label of document xm,j.
wm: the user model parameter associated with user m. wm is a k dimensional vector.
m is the total number of users.
m = 1, 2, ..., m: the index for each individual user.
in the rest of this paper, we will use the following notations to represent the variables in the system.
for each user, the system learns a user model from the user"s history.
the task of the system is to recommend documents that are relevant to each user.
regression assume there are m users in the system.
