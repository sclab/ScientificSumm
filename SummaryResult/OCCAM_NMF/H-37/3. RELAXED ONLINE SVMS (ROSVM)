as we will see in the experimental section, this parameter can be set as low as 1 with little impact on the quality of results, providing computational savings. 
we introduce a parameter t to control the maximum number of iterations we allow.
that is, a close first approximation may be good enough.
however, it is possible that these iterations provide less benefit than their expense justifies.
successive iterations of this loop bring the hyperplane closer to an optimal value.
smo has one main loop, which can alternate between passing over the entire data set, or the smaller active set of current  support vectors [22].
3.3 reducing iterations as an iterative solver, smo makes repeated passes over the data set to optimize the objective function.
in the experimental section, we show that this  version of online svms, which updates only on actual errors, does not significantly degrade performance on content-based spam detection, but does significantly reduce cost.
in the extreme case, we can set m = 0, which creates a mistake driven online svm.
this update procedure is similar to that used by  variants of the perceptron algorithm [18].
the learner may encounter an example that lies within the margins, but farther from the margins than m. such an example means the hypothesis is no longer globally optimal for the data set, but it is considered good enough for continued use without immediate retraining.
an example xi is now considered well classified when yif(xi) > m, for some 0 ≤ m ≤ 1. here, each update still produces an optimal  hyperplane.
the number of re-training updates can be reduced by  relaxing the definition of well classified.
under the kkt conditions, an example xi is considered well-classified when yif(xi) > 1. if we re-train on every example that is not well-classified, our hyperplane will be guaranteed to be optimal at every step.
3.2 reducing number of updates as noted before, the kkt conditions show that a well classified example will not change the hypothesis; thus it is not necessary to re-train when we encounter such an  example.
this set of constraints reduces the number of free variables in the optimization problem, reducing computational cost.
(for completeness, when p ≥ n, define (n − p) = 1.)
, x(n−p)}.
, xn}, subject to the fixed constraints on the hyperplane that were found in previous optimizations over examples {x1, .
, xn)}, but rather one that satisfies a relaxed requirement that the margin be maximized over the examples { x(n−p+1), .
thus, the margin found by an  optimization is not guaranteed to be one that maximizes the margin for the global data set of examples {x1, .
, (n − p)} : αj = cj where cj is a constant, fixed as the last value found for αj while j > (n − p).
, n} : 0 ≤ αi ≤ c and nx i=1 αiyi = 0 to this, we add the additional lookback buffer constraint ∀j ∈ {1, .
in this case, the original  softmargin svm is computed by maximizing at example n: w (α) = nx i=1 αi − 1 2 nx i,j=1 αiαjyiyj < xi, xj >, subject to the previous constraints [23]: ∀i ∈ {1, .
formally, the optimization problem is now defined most clearly in the dual form [23].
this allows the hypothesis to remember rare (but informative) features that were learned further than p examples in the past.
this hypothesis may contain values for features that do not occur anywhere in the p most recent examples, and these will not be changed.
note that this is not equivalent to training a new svm classifier from scratch on the p most recent examples,  because each successive optimization problem is seeded with the previous hypothesis w [8].
we can bound this expense by only considering the p most recent examples for optimization (see figure 4 for pseudo-code).
3.1 reducing problem size in the full online svms, we re-optimize over the full set of seen data on every update, which becomes expensive as the number of seen data points grows.
experimental results reported in the  following section show that they equal or approach the  performance of full online svms on content-based spam detection.
as we describe in the remainder of this subsection, all of these methods trade statistical robustness for reduced  computational cost.
solver by allowing an approximate solution to the  optimization problem.
set (w, b) := (w",b") if size(seendata) > p remove oldest example from seendata add xi to seendata done figure 4: pseudo-code for relaxed online svm.
, (xn, yn), c, m, p: initialize w := 0, b := 0, seendata := { } for each xi ∈ x do: classify xi using f(xi) = sign(< w, xi > +b) if yif(xi) < m find w , b with smo on seendata, using w, b as seed hypothesis.
• reduce the number of iterations in the iterative svm given: dataset x = (x1, y1), .
• reduce the number of training updates by only  training on actual errors.
we propose three ways to relax online svms: • reduce the size of the optimization problem by only optimizing over the last p examples.
the full margin maximization feature that they provide is unnecessary, and relaxing this requirement can reduce computational cost.
thus, while svms do create high performance spam  filters, applying them in practice is overkill.
however, as we saw in the previous section, the task of content-based spam detection is best achieved by svms with a high value of c. setting c to a high value for this domain implies that minimizing  training loss is more important than maximizing the margin (see figure 3).
maximizing the margin is expensive, typically requiring quadratic training time in the number of training examples.
one of the main benefits of svms is that they find a  decision hyperplane that maximizes the margin between classes in the data space.
