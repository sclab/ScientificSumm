the answer might depend on the particular  application, context and culture. 
exactly how different factors affect the decisions of the users is not clear.
significant improvements of quality estimates are likely to be obtained by  incorporating all empirical evidence about rating behavior.
a natural direction for future work is to examine  concrete applications of our results.
our approach to approximate this elusive "objective value" is by no means perfect, but conforms neatly to the idea behind the model.
however, the idea that variation in ratings is not primarily a function of variation in value turns out to be a useful one.
for example, if ef (i) reflects the true value of f at a point in time, the difference in the ratings following high and low expectations can be explained by hotel revenue models that are maximized when the value is modified accordingly.
other  explanations might account for the correlation of ratings with past reports.
one assumption that we make is about the existence of an objective quality value vf for the feature f. this is rarely true, especially over large spans of time.
the proposed model was validated on the empirical data and provides better estimates of the ratings actually submitted.
the gap tends to have a bigger influence when an important fraction of the textual comment is dedicated to discussing a certain feature.
the final rating depends, as expected, on the true observation, and on the gap between the observation and the expectation.
we have captured the empirical evidence in a behavior model that predicts the ratings submitted by the users.
intuitively, this supports the hypothesis that review forums act as discussion groups where users are keen on presenting and motivating their own opinion.
second, we include evidence from the textual comments, and find that when users devote a large fraction of the text to discussing a certain feature, they are likely to motivate a divergent rating (i.e., a rating that does not conform to the prior expectation).
intuitively, the perception of quality (and consequently the rating) depends on how well the actual experience of the user meets her expectation.
we validate two facts about the hotel reviews we collected from tripadvisor: first, the ratings following low expectations (where the expectation is computed as the average of the previous reports) are likely to be higher than the ratings table 10: precision(p), recall(r), s= 2pr p+r while  spotting poor ratings for sydney o r s c v p 0.650 0.463 0.544 0.550 0.580 σ r 0.234 0.378 0.571 0.169 0.592 s 0.343 0.452 0.557 0.259 0.586 p 0.562 0.615 0.600 0.500 0.600 τ r 0.054 0.098 0.101 0.015 0.175 s 0.098 0.168 0.172 0.030 0.271 following high expectations.
previous reports create an expectation of  quality which affects the subjective perception of the user.
second, we emphasize the dependence of ratings on  previous reports.
nevertheless, further evidence is required to support the intuition that ratings corresponding to high weights are expert opinions that deserve to be given higher priority when computing estimates of quality.
this observation allows the construction of feature-by-feature estimators of quality that have a lower variance, and are hopefully less noisy.
specifically, it seems that users who discuss amply a certain feature are likely to agree on a common rating.
using simple natural language processing algorithms, we were able to establish a correlation between the weight of a certain feature in the textual comment accompanying the  review, and the noise present in the numerical rating.
for that we use two additional sources of information besides the vector of numerical ratings: first we look at the textual comments that accompany the reviews, and second we consider the reports that have been previously submitted by other users.
conclusion the goal of this paper is to explore the factors that drive a user to submit a particular rating, rather than the  incentives that encouraged him to submit a report in the first place.
