7 closes the paper. 
6  discusses some issues and sect.
5 presents some experiments on trec 8 data; sect.
4; sect.
3 presents the basic ideas of normalizing average precision and of constructing a  systemstopics graph, whose properties are analyzed in sect.
2 gives some  motivations for the work.
the paper is organized as follows: sect.
in passing, we also provide a small generalization to kleinberg"s hits algorithm, as well as to inlinks and pagerank.
this reveals phenomena previously hidden in trec data.
we then apply  analysis methods originally developed for search applications to the resulting network.
links represent effectiveness measurements on system-topic pairs.
in this paper, we propose and demonstrate a method for constructing a network, specifically a weighted complete bidirectional directed bipartite graph, on a set of trec  topics and participating systems.
[1].
one example is using hits for stemming, as described by agosti et al.
these indicators and algorithms might be quite  general in nature, and can be used for applications which are very different from search result ranking.
several extensions to these algorithms have been and are being  proposed.
some indicators (and the corresponding algorithms that compute them) have been found useful in this respect, and are nowadays well known: inlinks (the number of links to a web page), pagerank [7], and hits (hyperlink-induced topic search) [5].
current search engines use link analysis techniques to help rank the retrieved documents.
network analysis and ir fruitfully meet in web search engine implementation, as is already described in textbooks [3,6].
of  particular importance is social network analysis [16], that studies networks made up by links among humans (friendship,  acquaintance, co-authorship, bibliographic citation, etc.).
network analysis is a discipline that studies features and properties of (usually large) networks, or graphs.
other conferences such as ntcir, inex, clef  provide comparable data.
system effectiveness is then measured by well  established metrics (mean average precision being the most used).
this defines a set of relevant documents for each topic.
after the lists have been submitted and pooled, the trec organizers employ human assessors to provide relevance judgements on the pooled set.
participants use their systems to retrieve, and  submit to trec, a list of documents for each topic.
trec (text retrieval conference) [12, 15] is an annual benchmarking exercise that has become a de facto standard in ir evaluation: before the actual  conference, trec provides to participants a collection of  documents and a set of topics (representations of information needs).
evaluation is a primary concern in the information  retrieval (ir) field.
