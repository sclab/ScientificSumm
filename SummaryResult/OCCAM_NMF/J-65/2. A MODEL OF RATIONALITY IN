nonanonymous email systems may promise comparably higher reliability and (possibly) reduced costs of operations. 
mix-net systems may decrease the expected losses from privacy intrusions.
the magnitudes of the parameters in equation 1 will change with the chosen technology.
for example, the agent may consider the costs and benefits of sending an email through an anonymous mix-net system [8] and compare those to the costs and benefits of sending that email through a conventional, non-anonymous  channel.
maybe she would not complete the transaction at all (d = 0).
maybe she would complete the transaction without protection.
a rational agent would, in theory, choose the technology d that maximizes her expected payoff in equation 1. maybe she would choose to complete the transaction under the  protection of a privacy enhancing technology.
imagine that the probability of keeping your financial transactions private is very high when you use a bank in bermuda: still, the expected value from  keeping your financial information confidential will depend on a number of other factors.
similarly, the expected  benefit from keeping information private will also be a  collection over time of probability distributions dependent on  several parameters.
the probability of keeping  information private will also depend on the environment in which the transaction is taking place.
these efforts may be function, among other things, of the expected value of that  information to those parties.
over time, the probability of keeping certain information private, for instance, will not only depend on the chosen technology d but also on the efforts by other parties to  appropriate that information.
ve() and pd (), in other words, can be read as multi-variate parameters inside which are hidden several other variables, expectations, and functions because of the complexity of the privacy network described above.
the payoffs are themselves only expected because, regardless of the probability that the transaction is completed or the  information remains private, they may depend on other sets of events and their associated probabilities.
note that ve and p could refer to sets of payoffs and the associated probabilities of occurrence.
the functional parameters δ and γ embody the variable weights and attitudes an individual may have towards  keeping her information private (for example, her privacy  sensitivity, or her belief that privacy is a right whose respect should be enforced by the government) and completing  certain transactions.
22 make one"s on-line shopping experience more cumbersome, and therefore more expensive.
but it may 1 see also [1].
protecting one"s financial privacy by not  divulging credit card information on-line may protect against future losses and hassles related to identity theft.
viceversa, it may also cost a larger bill, because of price discrimination.
for instance, revealing one"s identity to an on-line bookstore may earn a discount.
since the payoffs in equation 1 can be either positive or  negative, equation 1 embodies the duality implicit in privacy issues: there are both costs and benefits gained from  revealing or from protecting personal information, and the costs and benefits from completing a transaction, ve (t), might be distinct from the costs and benefits from keeping the  associated information private, ve (a).
max d ut = δ ve (a) , pd (a) + γ ve (t) , pd (t) − cd t (1) in equation 1, δ and γ are unspecified functional forms that describe weighted relations between expected payoffs from a set of events v and the associated probabilities of  occurrence of those events p. more precisely, the utility u of completing a transaction t (the transaction being any action - not necessarily a monetary operation - possibly involving exposure of personal information) is equal to some function of the expected payoff ve (a) from maintaining (or not)  certain information private during that transaction, and the probability of maintaining [or not maintaining] that  information private when using technology d, pd (a) [1 − pd (a)]; plus some function of the expected payoff ve (t) from  completing (or non completing) the transaction (possibly  revealing personal information), and the probability of completing [or not completing] that transaction with a certain  technology d, pd (t) [1 − pd (t)]; minus the cost of using the  technology t: cd t .1 the technology d may or may not be privacy enhancing.
to understand how a rational agent could navigate through those complex relations, in equation 1 we abstract the  decision process of an idealized rational economic agent who is facing privacy trade-offs when completing a certain  transaction.
this most often requires the study of a network of relations between a subject, certain information (related to the subject), other parties (that may have various linkages of interest or association with that information or that  subject), and the context in which such linkages take place.
hence its value may be discussed (if not ascertained) only once its context has also been  specified.
privacy has therefore become more a class of multifaceted interests than a single, unambiguous concept.
the boundaries between private and public become blurred.
in an information society the self is expressed, defined, and affected through and by  information and information technology.
the concept of privacy, once intended as the right to be left alone [41], has transformed as our society has become more information oriented.
we want to see if individuals can be  economically rational (forward-lookers, bayesian updaters, utility maximizers, and so on) when it comes to protect their own personal information.
while these studies focus on market  interactions between one agent and other parties, here we are interested in formalizing the decision process of the single individual.
since [28, 37, 29] economists have been interested in  privacy, but only recently formal models have started  appearing [3, 7, 39, 40].
here we want to investigate what underlying assumptions about personal behavior would support the hypothesis of full rationality in privacy decision making.
under this view, individuals may accept small rewards for giving away information because they expect future damages to be even smaller (when  discounted over time and with their probability of occurrence).
privacy decision making some have used the dichotomy between privacy attitudes and behavior to claim that individuals are acting rationally when it comes to privacy.
