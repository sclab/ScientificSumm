in this situation, any further  constraints, e.g., smoothness of motion, speed limits in different  locations, or speed reductions during sharp turns, are explicitly and uniformly expressed by the tactical target, and can result in faster and more effective action selection by a dbc algorithm. 
on the other hand, the tactical target expression of variations and correlations between position and speed of motion is now grounded in the state space representation.
now, even if the reward  function would encode more, and finer, details of the properties of  motion, the pomdp solution will have to search in a much larger space of policies, while still being guided by the implicit concept of the reward accumulation procedure.
for pomdps, this would mean that the same concept is encoded twice, in some sense: directly in the state space, and  indirectly within reward accumulation.
alternatively, the state space could directly include the notion of speed.
pomdps would encode terrain as state space points, while speed would be ensured by negative reward for every step taken without reaching the  goalaccumulating higher reward can be reached only by faster motion.
for example, consider the task of moving through rough terrain towards a goal and reaching it as fast as possible.
as a result, dbc has a greater ability to express state sequencing properties, which are grounded in the environment model and its state space definition.
dbc"s target dynamics specification can use the environment"s state space as a means to describe, discern, and preserve changes that occur within the system.
dbc concentrates on the underlying principle of state  sequencing, the system dynamics.
this can be viewed as if the sequencing of states and the attributes of that sequencing are only introduced by and for the controlling mechanism-the pomdp policy.
pomdps regard state as a stationary snap-shot of the environment;  whatever attributes of state sequencing one seeks are reached through properties of the control process, in this case reward accumulation.
3.2 dbc view of the state space it is important to note the complementary view that dbc and pomdps take on the state space of the environment.
• agent level is then faced with a problem of selecting a  control signal function a∗ to satisfy a minimization problem as follows: a∗ = arg min a pr(d(τa, q) > θ) where d(τa, q) is a random variable describing deviation of the dynamics estimate τa, created by l under control signal a, from the ideal dynamics q. implicit in this minimization problem is that l is manipulated via the environment, based on the environment model produced by the environment  design level.
for instance,  kullbackleibler divergence is an important tool of information  theory [3] that allows one to measure the price of encoding an information source governed by q, while assuming that it is governed by p. the user level also defines the threshold of dynamics  deviation probability θ.
there are many possible variations available at the user level to define divergence between system dynamics; several of them are: - trace distance or l1 distance between two  distributions p and q defined by d(p(·), q(·)) = 1 2 x |p(x) − q(x)| - fidelity measure of distance f(p(·), q(·)) = x p(x)q(x) - kullback-leibler divergence dkl(p(·) q(·)) = x p(x) log p(x) q(x) notice that the latter two are not actually metrics over the space of possible distributions, but nevertheless have  meaningful and important interpretations.
thus specification of target dynamics can be expressed by q ∈ f, and the learning or tracking algorithm can be represented as a function l : o×(a×o)∗ → f; that is, it maps sequences of observations and actions performed so far into an estimate τ ∈ f of system dynamics.
• user level, in the case of markovian environment, operates on the set of system dynamics described by a family of  conditional probabilities f = {τ : s × a → π(s)}.
that is, ω(o|s , a, s) is the probability that one will observe o given that the environment has moved from state s to state s under action a.
this is what the sensor input would look like for an outside observer; - ω is the observation probability function: ω : s × a × s → π(o).
that is, t(s |a, s) is the  probability that the environment will move from state s to state s under action a; - o is the set of all possible observations.
on autonomous agents and multi-agent systems (aamas 07) 791 - a is the set of all possible actions applicable in the  environment; - t is the environment"s probabilistic transition function: t : s ×a → π(s).
notice how dbc discards rewards, and replaces it by another optimality criterion (structural differences are summarized in table 1): • environment design level is to specify a tuple < s, a, t, o, ω, s0 >, where: - s is the set of all possible environment states; - s0 is the initial state of the environment (which can also be viewed as a probability distribution over s); the sixth intl.
joint conf.
3.1 dbc for markovian environments for a partially observable markovian environment, dbc can be specified in a more rigorous manner.
in fact, the  combination of feasibility and response data can provide a basis for the application of strong learning algorithms such as em [2, 9].
in this case, feasibility and system response data from the agent level would provide key information for an environment model update.
for example, the user level can operate as an exploratory device for a learning algorithm, inferring an ideal dynamics target from the environment model at hand that would expose and verify most critical features of system behavior.
design agent model ideal dynamics estimator estimator dynamics feasibility system response data figure 1: data flow of the dbc framework extending upon the idea of actor-critic algorithms [5], dbc data flow can provide a good basis for the design of a learning  algorithm.
data would also be available about the system response to different actions performed; combined with a dynamics estimator defined by the user level, this can provide an important tool for the environment model calibration at the environment design level.
for instance, the agent level could provide data about target dynamics feasibility, allowing the user level to modify the requirement, perhaps focusing on attainable features of system  behavior.
dbc levels can also have a back-flow of information (see  figure 1).
thus, for strong enough  estimator algorithms, successful manipulation would mean a successful simulation of the specified target dynamics (i.e., beyond discerning via the available sensory input).
notice that this manipulation is not direct, but via the environment.
the other alternative would be to rely on the  estimation procedure provided by the user level-to utilize the  environment design level model of the environment to choose actions, so as to manipulate the dynamics estimator into believing that a  certain dynamics has been achieved.
one possibility would be to create a mixture of available system trends, much like that which happens in behavior-based robotic architectures [1].
specific action selection then depends on system formalization.
to this end, we use a probability threshold-that is, we would like the agent level to maximize the probability that the deviation measure will remain below a certain threshold.
as we are interested in the continual development of a stochastic system, such as happens in classical control theory [16] and  continual planning [4], as well as in our example of museum sweeps, the question becomes how the agent level is to treat the  deviation measurements over time.
• agent level in turn combines the environment model from the environment design level, the dynamics estimation  procedure, the deviation measure and the target dynamics  specification from user level, to produce a sequence of actions that create system dynamics as close as possible to the  targeted specification.
in the museum guard scenario above, these would correspond to a stochastic sweep schedule, and a measure of relative surprise between the specified and actual sweeping.
the user level also specifies the  estimation or learning procedure for system dynamics, and the measure of deviation.
• user level in turn relies on the environment model produced by environment design to specify the target system  dynamics it wishes to observe.
for  example, this level would specify the laws of physics within the system, and set its parameters, such as the gravitation  constant.
• environment design level is concerned with the formal specification and modeling of the environment.
the specification of dynamics based control (dbc) can be  broken into three interacting levels: environment design level, user level, and agent level.
