790 978-81-904262-7-5 (rps) c 2007 ifaamas 
experimental settings and results are then presented in section 5. section 6 provides a short discussion of the overall approach, and section 7 gives some concluding remarks and directions for future work.
this is followed by a review of the extended markov tracking (emt) approach as a dbc-structured control regimen in section 4. that section also discusses the limitations of emt-based control relative to the  general dbc framework.
section 3 introduces the dynamics based control (dbc) structure, and its specialization to markovian environments.
in section 2 we motivate dbc using area-sweeping problems, and discuss related work.
the paper is organized as follows.
experimental data demonstrates that even with a simple model of the environment and a simple design of target dynamics, high success rates can be produced both in catching the quarry, and in surprising the quarry (as expressed by the observed entropy of the controlled agent"s position).
to evaluate dbc, we carried out a set of experiments applying multi-target emt to the tag game [11]; this is a variant on the area sweeping problem, where an agent is trying to tag a moving target (quarry) whose position is not known with certainty.
emt is an efficient instantiation of dbc.
we show that the recently developed extended markov  tracking (emt) approach [13, 14, 15] is subsumed by dbc, with emt employing greedy action selection, which is a specific  parameterization among the options possible within dbc.
in this paper, we present the structure of dynamics based  control.
here, optimality is measured in terms of probability of deviation magnitudes.
as actual system behavior may deviate from that which is specified by target dynamics (due to the stochastic nature of the system), planning in such  environments needs to be continual [4], in a manner similar to classical closed-loop controllers [16].
in dbc, the goal of a planning (or control) process becomes to ensure that the system will change in accordance with specific  (potentially stochastic) target dynamics.
we call this general planning framework  dynamics based control (dbc).
the idea here is to view  planexecution as a process that compels a (stochastic) system to change, and a plan as a dynamic process that shapes that change according to desired criteria.
we do not use a (state-based) reward function, but instead optimize over a different criterion, a transition-based specification of the desired system dynamics.
we take an alternative view of planning in stochastic  environments.
while theoretically attractive, the complexity of optimally solving a pomdp is prohibitive [8, 7].
in this framework, the planning and control problem is often addressed by imposing a reward function, and computing a policy (of choosing actions) that is optimal, in the sense that it will result in the highest expected utility.
in recent years, partially observable markov decision processes (pomdps) [12] have  become a popular formal basis for planning in stochastic  environments.
planning and control constitutes a central research area in  multiagent systems and artificial intelligence.
