this behavior did not occur in our second observation model, since it significantly reduced the set of eligible courses of action-essentially contributing to tie-breaking among them. 
it is essentially attempting to  simultaneously follow several courses of action, all of which are  consistent with the target dynamics.
since the emt algorithm has greedy action selection, and the state space does not encode any form of commitment (not even speed or acceleration), the agent is locked within a small portion of cells.
further analysis of the data showed that under certain circumstances, a fluctuating  behavior occurs in which the agent faces equally viable versions of quarry-following behavior.
near the maximum limit of trial length (100 steps), entropy suddenly dropped.
one characteristic of the entropy graph for the open arena  scenario particularly caught our attention in the case of the  omniposition quarry observation model.
as the agent actively seeks the quarry, the entropy never reaches its maximum.
for runs where the quarry was not captured immediately, the entropy reaches  between 0.85 and 0.952 for different runs and scenarios.
on autonomous agents and multi-agent systems (aamas 07) a q q a 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 a q figure 3: these configurations of the tag game space were used: a) multiple dead-end, b) irregular open arena, c) circular corridor table 2: performance of the emt-based solution in three tag game domains and two observability models: i) omniposition quarry, ii) quarry is not at hunter"s position model domain capture% e(steps) time/step i dead-ends 100 14.8 72(msec) arena 80.2 42.4 500(msec) circle 91.4 34.6 187(msec) ii dead-ends 100 13.2 91(msec) arena 96.8 28.67 396(msec) circle 94.4 31.63 204(msec) ical entropy grows with the length of interaction.
joint conf.
as figure 4 and figure 5 show,  empir794 the sixth intl.
we also tested the behavior cell frequency entropy, empirical measures from trial data.
the significance of this  differential is made even more prominent by the fact that, should the environment model parameters change, the online nature of emt would allow it to maintain its performance time, while the pomdp policy would need to be recomputed, requiring yet again a large overhead of computation.
that is, over 4 × 104 online steps took an order of magnitude less time than the offline computation of pomdp policy in [11].
for the  irregular open arena domain, which proved to be the most difficult, 1000 experiment runs bounded by 100 steps each, a total of 42411 steps, were completed in slightly under 6 hours.
in spite of a simple and inefficient matlab implementation of the emt algorithm, the decision time for any given step  averaged significantly below 1 second in all experiments.
although direct comparisons are difficult to make, the emt  performance displayed notable efficiency vis-`a-vis the pomdp  approach.
the results of these experiments are shown in table 2.  balancing [15] the catch, move, and stalk target dynamics described in the previous section by the weight vector [0.8, 0.1, 0.1], emt  produced stable performance in all three domains.
the second version in fact utilized and  expressed the fact that once hunter and quarry occupy the same cell the game ends.
in the second, only those joint positions in which hunter and quarry occupied different locations were mapped into the hunter"s location.
in the first version, observability function mapped all joint positions of hunter and quarry into the position of the hunter as an observation.
we also used two variations of the environment observability function.
in every run, the initial position of both the agent and its quarry was selected at random; this means that as far as the agent was concerned, the quarry"s initial position was uniformly distributed over the entire domain cell space.
in each setting, a set of 1000 runs was performed with a time limit of 100 steps.
three configurations of the domain shown in figure 3 were used, each posing a different challenge to the agent due to  partial observability.
we ran several experiments to evaluate emt performance in the tag game.
this results in the following three target dynamics: tcatch(at+1 = si|qt = sj, at = sa) ∝ 1 si = sj 0 otherwise tmobile(at+1 = si|qt = so, at = sj) ∝ 0 si = sj 1 otherwise tstalk(at+1 = si|qt = so, at = sj) ∝ 1 dist(si,so) note that none of the above targets are directly achievable; for instance, if qt = s9 and at = s11, there is no action that can move the agent to at+1 = s9 as required by the tcatch target dynamics.
for the game of tag, we can easily formulate three major trends: catching the quarry, staying mobile, and stalking the quarry.
we use emt and directly specify the target dynamics.
the only sensory information available to the agent is its own location.
so for the instance shown in figure 2 and q0 = 0.1: p(q = s9|q = s9, a = s11) = 0.1 p(q = s2|q = s9, a = s11) = 0.3 p(q = s8|q = s9, a = s11) = 0.3 p(q = s14|q = s9, a = s11) = 0.3 although the evasive behavior of the quarry is known to the agent, the quarry"s position is not.
with probability q0 1 it stays put, and with  probability 1 − q0 it moves to an adjacent cell further away from the 1 in our experiments this was taken to be q0 = 0.2. agent.
the effects of an action taken by the agent are deterministic, but the environment in general has a stochastic response due to the  motion of the quarry.
for  figure 2, it would be s = {s0, ..., s23} × {s0, ..., s23}.
the state space of the formal markovian environment is described by the cross-product of the agent and quarry"s positions.
these form a formal space of actions within a markovian environment.
as a result, both the agent and its quarry have the same motion capability, which allows them to move in four directions, north, south, east, and west.
we slightly modify this setting: the moment that both agents occupy the same cell, the game ends.
in the classical version of the game, co-location leads to a special observation, and the ‘tag" action can be performed.
0 51 2 3 4 6 7 8 10 12 13 161514 17 18 19 2221 23 9 11q a 20 figure 2: tag domain; an agent (a) attempts to seek and  capture a quarry (q) the game of tag extremely limits the agent"s perception, so that the agent is able to detect the quarry only if they are co-located in the same cell of the grid world.
an example domain is shown in figure 2.
it is a single agent problem of capturing a quarry, and belongs to the class of area sweeping problems.
the game of tag was first introduced in [11].
