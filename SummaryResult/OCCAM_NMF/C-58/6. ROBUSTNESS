if reconfigurations are only transient (like a node temporarily not responding due to a burst of load), the replicated aggregate closely or correctly resembles the current state. 
an aggregated value in file location system is valid as long as the node hosting the file is active, irrespective of the status of other nodes in the system; whereas an application that counts the number of machines in a system may receive incorrect results irrespective of the replication.
however, note that this technique is not appropriate in some cases.
probes for the file location can then be answered without accessing the root; hence they are not affected by the failure of the root.
with different up and down values in a network of 4096 nodes for different  readwrite ratios.
0.1 1 10 100 1000 10000 0.0001 0.01 1 100 10000 avg.numberofmessagesperoperation read to write ratio update-all up=all, down=9 up=all, down=6 update-up update-local up=2, down=0 up=5, down=0 figure 8: flexibility of our approach.
for example, in a file location application, using a non-zero positive down parameter ensures that a file"s global aggregate is replicated on nodes other than the root.
by reducing the number of nodes that have to be accessed to answer a probe, applications can reduce the  probability of incorrect results occurring due to the failure of nodes that do not contribute to the aggregate.
in our system, applications control replication in space using up and down knobs in the install api; with large up and down values, aggregates at the intermediate virtual nodes are propagated to more nodes in the system.
in our system, however, all internal nodes have to be replicated along with the root.
replication in space: replication in space is more  challenging in our system than in a dht file location application because replication in space can be achieved easily in the latter by just  replicating the root node"s contents.
note that this strategy will be useful only after the dht adaptation is completed (point 6 on the time line in figure 7).
in particular, if an application  detects or suspects an answer as stale, then it can re-issue the probe increasing the up and down parameters to force the refreshing of the cached data.
additionally, using up and down knobs in the probe api, applications can force on-demand fast re-aggregation of updates to avoid staleness in the face of reconfigurations.
on-demand re-aggregation: the default lazy aggregation scheme lazily propagates the old updates in the system.
probes that complete or start between points 2 and 8 may return stale answers.
probes initiated between points 1 and 2 and that are affected by reconfigurations are reevaluated by aml upon detecting the reconfiguration.
figure 7 shows the time line for the default lazy re-aggregation upon reconfiguration.
on newchild(child, prefix), the aml layer creates space in its data structures for this child.
on failedchild(child,  prefix), the aml layer marks the child as inactive and any outstanding probes that are waiting for data from this child are re-evaluated.
any new updates, installs, and probes for this prefix are sent to the parent immediately.
if parent is not null, then  aggregation functions and already existing data are lazily transferred in the background.
on  newparent(parent, prefix), all probes in the outstanding-probes table  corresponding to prefix are re-evaluated.
lazy re-aggregation: the dht layer informs the aml layer about reconfigurations in the network using the following three function calls - newparent, failedchild, and newchild.
second, applications can reduce the probability of probe response staleness during such repairs through our flexible api with appropriate setting of the down parameter.
first, lazy re-aggregation propagates already received updates to new children or new parents in a lazy fashion over time.
we provide two  mechanisms for replication in time.
6.2 aml adaptation broadly, we use two types of strategies for aml adaptation in the face of reconfigurations: (1) replication in time as a  fundamental baseline strategy, and (2) replication in space as an  additional performance optimization that falls back on replication in time when the system runs out of replicas.
385 reconfig reconfig notices dht partial dht complete dht ends lazy time data 3 7 81 2 4 5 6starts lazy data starts lazy data starts lazy data repairrepair reaggr reaggr reaggr reaggr happens figure 7: default lazy data re-aggregation time line also note that the administrative isolation property satisfied by our adht algorithm ensures that the reconfigurations in a level i  domain do not affect the probes for level i in a sibling domain.
due to redundancy in the leaf sets and the routing table, updates can be routed towards their root nodes successfully even during failures.
note that maintaining extra leaf sets does not degrade the fault-tolerance property of the original pastry; indeed, it enhances the resilience of adhts to failures by providing additional routing links.
6.1 adht adaptation our adht layer adaptation algorithm is same as pastry"s  adaptation algorithm [32] - the leaf sets are repaired as soon as a  reconfiguration is detected and the routing table is repaired lazily.
our system handles reconfigurations at two levels - adaptation at the adht layer to ensure connectivity and adaptation at the aml layer to ensure access to the data in sdims.
our  system also provides two hooks that applications can use for improved end-to-end robustness in the presence of reconfigurations: (1)  ondemand re-aggregation and (2) application controlled replication.
second, the nodes needed for aggregation to answer the probe become unreachable.
first, reconfigurations lead to incorrectness in the previous aggregate values.
during reconfigurations, a probe might return a stale value for two reasons.
our two main principles for robustness are to guarantee (i) read availability - probes complete in finite time, and (ii) eventual consistency -  updates by a live node will be visible to probes by connected nodes in finite time.
in large scale systems, reconfigurations are common.
