also our current  prototype does not restrict the resource consumption in executing the aggregation functions; but, ‘techniques from research on resource management in server systems and operating systems [2, 3] can be applied here. 
our current prototype does not implement access control on  install, update, and probe operations but we plan to implement  astrolabe"s [38] certificate-based restrictions.
we are currently designing different policies to decide when to issue a lease and when to revoke a lease and are also evaluating them with the above mechanism.
now a probe for level-k aggregate can be answered by level-l virtual node if it has a valid lease, irrespective of the up and down values.
a virtual node at level l can issue the lease for level-k aggregate for k > l to a child only if down≥ k −l or if it has the lease for that aggregate from its parent.
a level-l virtual node for an attribute can issue the lease for  levell aggregate to a parent or a child only if up is greater than l or it has leases from all its children.
we implement a lease-based mechanism for dynamic adaptation.
thus each update for an attribute triggers re-evaluation of  continuous probes for that attribute.
at a virtual node, continuous probes are handled similarly as one-shot probes except that such probes are stored in the  outstanding probe table for a time period of exptime specified in the probe.
the last case accounts for nodes that participate in the adht protocol but that fail at the aml level.
a node stops waiting for its children when one of three conditions occurs: (1) all children have responded, (2) the adht layer signals one or more reconfiguration events that mark all  children that have not yet responded as unreachable, or (3) a watchdog timer for the request fires.
for probes that include phases 2 (probe scatter) and 3 (probe aggregation), an issue is how to decide when a node should stop waiting for its children to respond and send up its current  aggregate value.
note that in the extreme case of a function installed with up = down = 0, a level-i probe can touch all nodes in a level-i subtree while in the opposite extreme case of a  function installed with up = down = all, probe is a completely local operation at a leaf.
finally, in phase 4, the aggregate routing phase the aggregate value is routed down to the node that requested it.
in phase 3, the probe aggregation phase, when a node receives values for the specified key from each of its children, it executes the aggregate function on these values and  either (a) forwards the result to its parent (if its level is less than i) or (b) initiates phase 4 (if it is at level i).
in the former case, the system proceeds to phase 2 and in the latter it skips to phase 4. in phase 2, the probe scatter phase, each node that receives a probe request sends it to all of its children unless the node"s reduction mib already has a value that matches the probe"s attribute key, in which case the node initiates phase 3 on behalf of its subtree.
in phase 1, the route probe phase, the system routes the probe up the attribute key"s tree to either the root of the level-i subtree or to a node that stores the requested value in its  ancestor mib.
as figure 1 illustrates, the system satisfies a probe for a level-i aggregate value using a four-phase protocol that may be short-circuited when  updates have previously propagated either results or partial results up or down the tree.
probe a probe collects and returns the aggregate value for a specified attribute key for a specified level of the tree.
upon receipt of a level-i aggregate from a parent, a level k virtual node stores the value in its ancestor mib and, if k ≥ i−down, forwards this aggregate to its children.
also, the level-i (i ≥ 1) virtual node sends the updated level-i aggregate to all its children if the function"s down parameter exceeds zero.
update when a level i virtual node receives an update for an attribute from a child below: it first recomputes the level-i  aggregate value for the specified key, stores that value in its reduction mib and then, subject to the function"s up and domain parameters, passes the updated value to the appropriate parent based on the  attribute key.
then, the request is flooded down the tree and installed on all intermediate and leaf nodes.
execution of an install operation for function aggrfunc on attribute type attrtype proceeds in two phases: first the install request is passed up the adht tree with the attribute key (attrtype, null) until it reaches the root for that key within the specified domain.
given these data structures, it is simple to support the three api functions described in section 3.1. install the install operation (see table 1) installs on a domain an aggregation function that acts on a specified attribute type.
the outstanding probes table maintains temporary information  regarding in-progress probes.
each aggregate  function is installed on all nodes in a domain"s subtree, so the aggregate function table can be thought of as a special case of the ancestor mib with domain functions always installed up to a root within a specified domain and down to all nodes within the domain.
an aggregation function table contains the aggregation  function and installation arguments (see table 1) associated with an  attribute type or an attribute type and name.
along with these mibs, a virtual node maintains two other  tables: an aggregation function table and an outstanding probes  table.
for example, one program might monitor local  configuration and perform updates with information such as total memory, free memory, etc., a distributed file system might perform update for each file stored on the local node.
we  envision various sensor programs and applications insert data into local mib.
this local mib stores information about the local node"s state inserted by local applications via update() calls.
each level-0 leaf node maintains a local mib rather than maintaining child mibs and a reduction mib.
level-0 differs slightly from other levels.
so, the  aggregate values are tagged not only with level information, but are also tagged with id of the node that performed the aggregation.
note that the 384 list for a key might contain multiple aggregate values for a same level but aggregated at different nodes (see figure 4).
a virtual node ni at level i also maintains an ancestor mib to store the tuples containing attribute key and a list of aggregate  values at different levels scattered down from ancestors.
virtual node ni at level i maintains a reduction mib of tuples with a tuple for each key present in any child mib containing the attribute type, attribute name, and output of the attribute type"s  aggregate functions applied to the children"s tuples.
nodes maintain their child mibs in stable storage and use a simplified version of the bayou log exchange protocol (sans conflict detection and resolution) for synchronization after disconnections [26].
for a given virtual node ni at level i, each child mib contains the subset of a child"s reduction mib that contains tuples that match ni"s node id in i bits and whose up aggregation function attribute is at least i. these local copies make it easy for a node to recompute a level-i aggregate value when one child"s input changes.
our system, built upon pastry, handles multi-bit correction (b = 4) and is a simple extension to the scheme described here.
note that in the discussion below, for ease of explanation, we assume that the routing protocol is correcting single bit at a time (b = 1).
this basic strategy of maintaining child, reduction, and ancestor mibs is based on astrolabe [38], but our structured propagation strategy channels information that flows up according to its attribute key and our flexible propagation strategy only sends child updates up and ancestor aggregate results down as far as specified by the attribute key"s aggregation  function.
to support hierarchical aggregation, each virtual node at the root of a level-i subtree maintains several mibs that store (1) child mibs containing raw aggregate values gathered from children, (2) a  reduction mib containing locally aggregated values across this raw information, and (3) an ancestor mib containing aggregate values scattered down from ancestors.
as figure 6 illustrates, each physical node in the system acts as several virtual nodes in the aml: a node acts as leaf for all attribute keys, as a level-1 subtree root for keys whose hash matches the node"s id in b prefix bits (where b is the number of bits corrected in each step of the adht"s routing scheme), as a level-i subtree root for attribute keys whose hash matches the node"s id in the initial i ∗ b bits, and as the system"s global root for attribute keys whose hash matches the node"s id in more prefix bits than any other node (in case of a tie, the first non-matching bit is ignored and the comparison is continued [46]).
we refer an (attribute type, attribute name) tuple as an attribute key.
we refer to a store of (attribute type, attribute name, value) tuples as a management information base or mib, following the  terminology from astrolabe [38] and snmp [34].
local mib mibs ancestor reduction mib (level 1)mibs ancestor mib from child 0x... mib from child 0x... level 2 level 1 level 3 level 0 1xxx... 10xx... 100x... from parents0x.. to parent 0x... −− aggregation functions from parents to parent 10xx... 1x.. 1x.. 1x.. to parent 11xx... node id: (1001xxx) 1001x.. 100x.. 10x.. 1x.. virtual node figure 6: example illustrating the data structures and the  organization of them at a node.
given the adht construction described in section 4.2, each node implements an aggregation management layer (aml) to support the flexible api described in section 3. in this section, we describe the internal state and  operation of the aml layer of a node in the system.
the internal design of our sdims prototype comprises of two layers: the autonomous dht (adht) layer manages the overlay topology of the system and the aggregation management layer (aml) maintains attribute tuples, performs aggregations, stores and propagates aggregate values.
