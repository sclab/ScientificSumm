using the approach described here, we achieve a f3 score of 0.3081. this score is equivalent to the initial human interest model score of 0.3031 but fails to outperform the optimized human interest model model. 
score(s) = max(shim, ssp)1âˆ’min(shim,ssp) (5) in order to maintain a diverse set of answers as well as to  ensure that similar sentences are not given similar ranking, we  further re-rank our combined list of answers using maximal marginal relevance or mmr [2].
when both systems agree agree that the sentence is definitional, the sentence"s score is boosted by the degree of agreement between between both systems.
we first normalize the top 1,000 ranked sentences from each system, to obtain the normalized human interest model score, him(s), and the normalized soft pattern bigram model score, sp(s), for every unique sentence, s. for each sentence, the two  separate scores for are then unified into a single score using equation 5. when only one system believes that the sentence is definitional, we simply retain that system"s normalized score as the unified score.
currently, the best approach we found for combining both answer sets is to merge and re-rank both answer sets with boosting agreements.
however, the differences between the two systems also cause issues when we attempt to combine both answer sets.
there are thus rational reasons and practical motivation in  unifying answers from both the pattern based and corpus based  approaches.
it is also indication that in general, interesting and informative nuggets are quite different in nature.
we view this as further indication that definitions are indeed made up of a mixture of informative and  interesting nuggets.
while the nugget agreement rate is higher than the  sentence agreement rate, both systems are generally still picking up different answer nuggets.
when we compared the answer nuggets found by both system for this subset of topics, we found that the nugget agreement rate between both systems was 16.6%.
to verify if both systems are selecting the same answer nuggets, we randomly selected a subset of 10 topics from the trec 2005 question set and manually identified correct answer nuggets (as  defined by trec accessors) from both systems.
unfortunately, we are unable to automatically verify this as the evaluation software we are using does not report correctly identified answer nuggets.
this could result in both systems having the same nuggets marked as correct even though the source answer sentences are structurally different.
there is a distinct possibility that each system may be selecting different sentences with different syntactic structures but actually have the same or similar semantic content.
yet, despite the low agreement rate between both systems, each individual system is still able to attain a relatively high f3 score.
even when we examined the top 500 sentences generated by both systems, the agreement rate was still an extremely low 5.3%.
the remaining answers were completely  different.
in the top 10  sentences from both systems, only 4.4% of these sentences appeared in both answer sets.
in essence, our two experts are disagreeing on which sentences are definitional.
the reason is that both systems are picking up very different sentences as definitional answers.
however, none of the ensemble  learning methods we attempted could outperform our human interest model.
we had initially hoped to unify the two separate definitional question answering systems by applying an ensemble learning method [5] such as voting or  boosting in order to attain a good mixture of informative and interesting nuggets in our answer set.
the human  interest model we have described in this paper on the other hand is an expert in finding interesting nuggets.
is an expert in identifying informative nuggets.
the soft pattern bigram model proposed by cui et al.
rank person org thing event baseline unigram weighting scheme, n+w+s+m 0.3279 0.3630 0.2551 0.2644 1 n+s+m w+s w+m n+m 0.3584 0.3709 0.2688 0.2905 2 n+s n+w+s w+s+m n+s+m 0.3469 0.3702 0.2665 0.2745 3 n+m n+w+s+m w+s n+s 0.3431 0.3680 0.2616 0.2690 table 2: top 3 runs using different web resources for each  entity class we now have two very different experts at identifying  definitions.
we believe that a good definitional question  answering system should provide the reader with a combined mixture of both nugget types as a definitional answer set.
informative nuggets present a general overview of the topic while interesting nuggets give  readers added depth and insight by providing novel and unique aspects about the topic.
however from the perspective of a human reader, both informative and interesting nuggets are useful and definitional.
interestingness we have thus far been comparing the human interest model against the soft-pattern model in order to understand the  differences between interesting and informative nuggets.
