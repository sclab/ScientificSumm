second, how to compute an updated query model based on this term feedback evidence, so that it captures the user"s information need and translates into good retrieval performance. 
there are two key subtasks here: first, how to choose the best terms to present to the user for judgment, in order to gather maximal  evidence about the user"s information need.
we adopt this view, and cast our task as updating the query model from user term feedback.
[25] treats relevance feedback as a query model re-estimation problem, i.e., computing an updated query model θq given the original query text and the extra evidence carried by the judged relevant documents.
with this model, the retrieval task involves estimating a query language model θq from a given query, a document language model θd from each document, and calculating their kl-divergence d(θq||θd), which is then used to score the documents.
we follow the language modeling approach, and base our method on the kl-divergence retrieval model proposed in [25].
