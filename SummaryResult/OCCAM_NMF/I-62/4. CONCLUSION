frtdp and brtdp could also prune the action space in these circumstances to  further reduce their planning time. 
finally, the bounded-rtdp function prunes the action space when qu (a, s) â‰¤ l(s), as singh and cohn [10] suggested.
in particular, both these approaches proposed an efficient state trajectory updates, when given upper and lower bounds.
our tight bounds would enable, for both frtdp and brtdp, to reduce the number of backup to perform before convergence.
for  instance, frtdp [11], and brtdp [6] are both efficient  heuristic search algorithms.
an interesting research avenue would be to experiment our bounds with other heuristic search algorithms.
the only condition for the use our bounds is that each task possesses consumable and/or non-consumable limited resources.
furthermore, the marginal revenue bound proposed in this paper compares favorably to the singh and cohn [10] approach.
boundedrtdp with our proposed bounds may apply to a wide range of stochastic environments.
in this case, the  planning time of bounded-rtdp, which prunes the action space, is significantly lower than for lrtdp.
on the other hand, when the available resource are shared, no q-decomposition is possible and we proposed tight bounds for heuristic search.
the experiments have shown that q-decomposition seems a very efficient approach when a group of agents have to allocate resources which are only available to themselves, but the actions made by an agent may influence the reward obtained by at least another agent.
