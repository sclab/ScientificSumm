the problem is now modelled. 
the only  condition on the use of our bounds is to be in the context of stochastic constrained resource allocation.
also, even if the algorithm used to obtain the optimal policy is rtdp, our bounds can be used with any other algorithm to solve an mdp.
our marginal revenue bounds are  compared theoretically and empirically to the bounds proposed by singh and cohn.
this paper proposes new algorithms to define upper and lower bounds in the context of a rtdp heuristic search approach.
on the other hand, the approach by singh and cohn is suitable to our case, and extended in this paper using, in particular, the concept of marginal revenue [7] to elaborate tight bounds.
this efficient trajectory of state updates can be combined to the approach proposed here since this paper focusses on the definition of tight bounds, and efficient state update for a constrained resource allocation problem.
[6] and smith and simmons [11] suggested, in particular, an efficient trajectory of state  updates to further speed up the convergence, when given upper and lower bounds.
mcmahan et al.
[6], smith and simmons [11], and singh and cohn [10] proposed solving a stochastic problem using a rtdp type heuristic search with upper and lower bounds on the value of states.
to do this, mcmahan et 1212 978-81-904262-7-5 (rps) c 2007 ifaamas al.
rtdp is much more effective if the action space can be pruned of sub-optimal actions.
rtdp is used in this paper to solve efficiently a constrained resource allocation problem.
[1] which  updates states in trajectories from an initial state s0 to a goal state sg.
because of its anytime quality, an interesting approach is rtdp introduced by barto et al.
for instance real-time dynamic  programming (rtdp) [1], lrtdp [4], hdp [3], and lao [5] are all state-of-the-art heuristic search approaches in a stochastic environment.
a common way of addressing this large stochastic problem is by  using markov decision processes (mdps), and in particular real-time search where many algorithms have been  developed recently.
on the other hand, when the resources are available to all agents, no q-decomposition is possible.
when implemented with heuristic search, since the number of states and actions to consider when computing the optimal policy is exponentially reduced compared to other known approaches, q-decomposition  allows to formulate the first optimal decomposed heuristic search algorithm in a stochastic environments.
then, the  planning agents are coordinated through an arbitrator to find the highest global q-value by adding the respective possible q-values of each agents.
in s0, each agent computes their respective q-value.
the planning process starts with the initial state s0.
in our q-decomposition approach, a planning agent manages each task and all agents have to share the limited resources.
to solve this problem efficiently, we adapt  qdecomposition proposed by russell and zimdars [9].
a second type of resource allocation problem is where the resources are already shared among the agents, but the actions made by an agent may influence the reward obtained by at least another agent.
firstly, if the resources are already shared among the agents, and the actions made by an agent does not influence the state of another agent, the globally optimal policy can be computed by planning separately for each agent.
there can be many types of resource allocation problems.
the very high number of states and actions in this type of problem makes it very complex.
in this case, the number of  possible actions in a state is the combination of each individual possible resource assignment to the tasks.
in general, the number of states is the combination of all possible specific states of each task and available resources.
when executing an action to realize a set of tasks, the stochastic nature of the problem induces probabilities on the next visited state.
resources to allocate) to undertake to accomplish certain tasks, according to the perfectly observable state of the  environment.
in such problems, a scheduling process suggests the action (i.e.
in general, resource  allocation problems are known to be np-complete [12].
this paper aims to contribute to solve complex stochastic resource allocation problems.
