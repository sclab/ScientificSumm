indeed, an optimal allocation is one where the resources are distributed in the best way to all tasks, and our lower bound heuristically does that. 
task by using a heuristic to distribute the resources.
0.01 0.1 1 10 100 1000 10000 1 2 3 4 5 6 7 8 timeinseconds number of tasks lrtdp lrtdp-up singh-rtdp mr-rtdp figure 2: efficiency of mr-rtdp compared to singh-rtdp.
on autonomous agents and multi-agent systems (aamas 07) 0.01 0.1 1 10 100 1000 10000 100000 1 2 3 4 5 6 7 8 9 10 11 12 13 timeinseconds number of tasks lrtdp qdec-lrtdp figure 1: efficiency of q-decomposition lrtdp and lrtdp.
on the other hand, the lower bound of mr-rtdp takes into account the value of all 1218 the sixth intl.
joint conf.
indeed, when the number of task to execute is high, the lower bounds by singh-rtdp takes the values of a single task.
from these results, we conclude that the  difference of efficiency between mr-rtdp and singh-rtdp is more attributable to the marginal-revenue lower bound that to the maxu upper bound.
we also implemented mr-rtdp with the singhl bound, and this was slightly more efficient than singh-rtdp.
furthermore, we implemented mr-rtdp with the singhu bound, and this was slightly less efficient than with the maxu bound.
indeed, the time reduction is quite significant compared to lrtdp-up, which demonstrates the efficiency of using bounds to prune the action space.
in particular, singh-rtdp needed 231 seconds in average to solve problem with six tasks and mr-rtdp required 76 seconds.
mr-rtdp  further reduce the planning time by providing very tight  initial bounds.
the singh-rtdp approach diminished the planning time by using a lower and upper bound to prune the action space.
for instance, it took an average of 1512 seconds to plan for the lrtdp-up approach with six tasks (see figure 1).
in terms of experiments, notice that the lrtdp lrtdp-up and approaches for resource allocation, which doe not prune the action space, are much more complex.
for our problem, we allocated each resource of each type in the order of of its specialization like we said when describing the revenue-bound function.
to compute the lower bound of revenue-bound, all available resources have to be separated in many types or parts to be allocated.
these  constraints are managed by the arbitrator as described in  section 2.2. q-decomposition permits to diminish the planning time significantly in our problem settings, and seems a very efficient approach when a group of agents have to allocate resources which are only available to themselves, but the  actions made by an agent may influence the reward obtained by at least another agent.
there are constraint between the group of resource resi and resi such that some assignments are not possible.
when the number of tasks is odd, one more task was assigned to t ai .
resi had one consumable resource type and one non-consumable resource type, while resi had two consumable resource types and one non-consumable resource type.
the set of task t ai, managed by agent i, can be accomplished with the set of resources resi, while the second set of task t ai , managed by agent agi , can be accomplished with the set of resources resi .
to implement qdec-lrtdp, we divided the set of tasks in two equal parts.
• mr-rtdp: the revenue-bound and maxu bounds are used for bounded-rtdp.
• singh-rtdp: the singhl and singhu bounds are used for bounded-rtdp.
• lrtdp-up: the upper bound of maxu is used for lrtdp.
in particular the updates made in the  checksolved function are also made using the the  qdecbackup function.
the approaches described in this paper are compared in figures 1 and 2. lets summarize these approaches here: • qdec-lrtdp: the backups are computed using the qdec-backup function (algorithm 1), but in a lrtdp context.
nevertheless, even if the lrtdp approach uses a simple heuristic, still a huge part of the state space is not visited when computing the optimal policy.
since this heuristic is pretty straightforward, the advantages of using better heuristics are more evident.
this way, the value of each unvisited state is assured to overestimate its real value since the value of achieving a task ta is the highest the planner may get for ta.
a simple heuristic has been used where the value of an unvisited state is assigned as the value of a goal state such that all tasks are achieved.
for this problem a standard lrtdp approach has been implemented.
the number of resource type has been fixed to 5, where there are 3  consumable resource types and 2 non-consumable resources types.
the global constraint is generated randomly at the start of a scenario for each consumable resource type.
furthermore, for consumable resources, the total amount of available  consumable resource is between 1 and 2 for each type.
this constraint is also present on a real naval platform because of sensor and launcher constraints and engagement policies.
for the local constraints, at most 1 resource of each type can be  allocated to execute tasks in a specific state.
there are also local and global resource  constraints on the amount that may be used.
the effectiveness of each resource is modified randomly by ±15% at the start of a scenario.
when a missile is not countered, it transits to another state, which may be  preferred or not to the current state, where the most preferred state for a task is when it is countered.
in particular, each resource type has a probability to counter a missile between 45% and 65% depending on the state of the task.
the state transitions are all stochastic  because when a missile is in a given state, it may always transit in many possible states.
there are two types of states; firstly, states where actions modify the transition probabilities; and then, there are goal states.
in our problem, |sta| = 4, thus each task can be in four distinct states.
for the experiments, 100 randomly resource allocation problems were generated for each approach, and possible number of tasks.
weapons, movements).
tasks) by using its  resources (i.e.
the domain of the experiments is a naval platform which must counter incoming missiles (i.e.
