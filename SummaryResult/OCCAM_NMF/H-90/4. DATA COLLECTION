this data set is publicly available 1 . 
among 91 clicked documents, 29 documents are judged relevant according to trec judgment file.
is 34.4 words.
3.8% -2.9% 10.6% -2.9% -31.1% -36.3% 9.6% -4.3% q3 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 0.0421 0.1483 q3 + hq + hc 0.0726 0.1967 0.0816 0.2067 0.0706 0.1783 0.0810 0.2067 improve 72.4% 32.6% 93.8% 39.4% 67.7% 20.2% 92.4% 39.4% q4 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 0.0536 0.1933 q4 + hq + hc 0.0891 0.2233 0.0955 0.2317 0.0792 0.2067 0.0950 0.2250 improve 66.2% 15.5% 78.2% 19.9% 47.8% 6.9% 77.2% 16.4% table 1: effect of using query history and clickthrough data for document ranking.
the average length of clicked summary fixint bayesint onlineup batchup query (α = 0.1, β = 1.0) (µ = 0.2, ν = 5.0) (µ = 5.0, ν = 15.0) (µ = 2.0, ν = 15.0) map pr@20docs map pr@20docs map pr@20docs map pr@20docs q1 0.0095 0.0317 0.0095 0.0317 0.0095 0.0317 0.0095 0.0317 q2 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 0.0312 0.1150 q2 + hq + hc 0.0324 0.1117 0.0345 0.1117 0.0215 0.0733 0.0342 0.1100 improve.
so on average, there are around 3 clicks per topic.
altogether there are 91 documents clicked to view.
among 120 (4 for each of 30 topics) queries which we study in the experiment, the average query length is 3.71 words.
the summary of the article is query  dependent and is computed online using fixed-length passage retrieval (kl divergence model with bayesian prior smoothing).
and for each clicked document, we store the summary as shown on the search result page.
for each query, we store the query terms and the associated result pages.
we use a relational database to store user interactions, including the submitted queries and clicked documents.
the user needs to select the topic number from a  selection menu before submitting the query to the search engine so that we can easily detect the session boundary, which is not the focus of our study.
in our experiment, only the first 4 queries for each topic are used.
for each topic, the subject composes at least 4 queries.
the subject may also modify the query to do another search.
the subject will browse the results and maybe click one or more results to browse the full text of article(s).
after the subject submits the query, the search engine will do retrieval and return a ranked list of search results to the subject.
for each topic, the first query is the title of the topic given in the original trec topic description.
each subject is assigned 10 topics and given the topic descriptions provided by trec.
we use 3 subjects to do experiments to collect query history and clickthrough history data.
we index the trec ap data set and set up a search engine and web interface for trec ap news articles.
in real applications, we may also expect our models to be most useful for such difficult topics, so our data collection strategy reflects the real world applications well.
the reason why we select difficult topics is that the user then would have to have several interactions with the retrieval system in order to get satisfactory results so that we can expect to collect a  relatively richer query history and clickthrough history data from the user.
we select 30 relatively difficult topics from trec topics 1-150. these 30 topics have the worst average precision performance among trec topics 1-150 according to some baseline experiments using the kl-divergence model with bayesian prior smoothing [20].
for the preprocessing, we only do case folding and do not do stopword removal or stemming.
if not, we select the first sentence of the text as the  title.
most articles have titles.
there are altogether 242918 news  articles and the average document length is 416 words.
we select trec ap88, ap89 and ap90 data as our text database, because ap data has been used in several trec tasks and has  relatively complete judgments.
we decide to augment a trec data set by collecting query history and clickthrough history data.
unfortunately, there are no query  history and clickthrough history data.
the other choice is to use a trec data set, which has a text database, topic description and relevance judgment file.
but the problem is that we have no relevance judgments on such data.
one is to extract topics and any associated query history and clickthrough history for each topic from the log of a retrieval system (e.g., search engine).
there are two choices.
since there is no such data set available to us, we have to create one.
in order to quantitatively evaluate our models, we need a data set which includes not only a text database and testing topics, but also query history and clickthrough history for each topic.
