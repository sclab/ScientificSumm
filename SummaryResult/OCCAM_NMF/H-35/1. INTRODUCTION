after a summary of related work in section 2, we describe the proposed adarank algorithm in details in section 3. experimental results and  discussions are given in section 4. section 5 concludes this paper and gives future work. 
the rest of the paper is organized as follows.
adarank can be viewed as a machine learning method for direct optimization of performance measures, based on a different approach.
several methods for  classification [17] and ranking [5, 19] have been proposed.
recently, direct optimization of performance measures in  learning has become a hot research topic.
from the viewpoint of ir, adarank can be viewed as a machine learning method for ranking model tuning.
as the number of features in the ranking model gets larger and the amount of  training data gets larger, the tuning becomes harder.
tuning ranking models using certain training data and a  performance measure is a common practice in ir [1].
experimental results indicate that adarank can outperform the  baseline methods of bm25, ranking svm, and rankboost, on four benchmark datasets including ohsumed, wsj, ap, and .gov.
adarank offers several advantages: ease in implementation,  theoretical soundness, efficiency in training, and high accuracy in ranking.
a lower bound of the performance on training data is given, which indicates that the ranking accuracy in terms of the performance measure can be continuously improved during the training process.
we show that adarank algorithm can iteratively optimize an  exponential loss function based on any of ir performance measures.
in learning, it repeats the process of re-weighting the training sample, creating a weak ranker, and calculating a weight for the ranker.
adarank utilizes a linear  combination of ‘weak rankers" as its model.
inspired by the work of adaboost for classification [9], we propose to develop a boosting algorithm for information  retrieval, referred to as adarank.
in this paper, we aim to develop a new learning algorithm that can directly optimize any performance measure used in document retrieval.
for example, ranking svm and rankboost train ranking models by minimizing  classification errors on instance pairs.
all the existing methods used for  document retrieval [2, 3, 8, 13, 16, 20] are designed to optimize loss functions loosely related to the ir performance measures, not loss functions directly based on the measures.
[8] take a similar approach and perform the learning by using boosting,  referred to as rankboost.
freund et al.
[13] propose a learning algorithm for ranking on the basis of support vector machines, called ranking svm.
for example, herbrich et al.
several methods for learning to rank have been developed and applied to document retrieval.
ideally, the ranking function is created so that the accuracy of ranking in terms of one of the measures with respect to the training data is maximized.
in document retrieval, usually ranking results are evaluated in terms of performance measures such as map (mean average precision) [1] and ndcg (normalized discounted  cumulative gain) [15].
in ranking, given a new query, the corresponding retrieved documents are sorted by using the trained ranking model.
in training, a ranking model is constructed with data  consisting of queries, their corresponding retrieved documents, and  relevance levels given by humans.
when applied to document retrieval, learning to rank becomes a task as follows.
recently ‘learning to rank" has gained increasing attention in both the fields of information retrieval and machine learning.
