thanks to nick lester and justin zobel for valuable discussions. 
acknowledgments this work was supported in part by arc discovery project dp0558916 (at).
we will explore alternate compression schemes in future work.
as seek time dominates the snippet generation process, we have not focused on this portion of the snippet generation in  detail in this paper.
a variation on the semi-static compression approach we have adopted in this work has been used successfully in  previous search engine design [24], but there are alternate  compression schemes that allow direct matching in compressed text (see navarro and m¨akinen [15] for a recent survey.)
the  document compaction techniques using sentence re-ordering, however, remove the spatial relationship between sentences, and so if a scoring technique relies on the position of a  sentence within a document, the aggressive compaction  techniques reported here cannot be used.
the cts method can incorporate any new metrics that may arise in the future that are calculated on whole words.
rather, we have reported accuracy results in terms of the four components that have been previously shown to be important in determining useful snippets [20].
in this work we have deliberately avoided committing to one particular scoring method for sentences in documents.
these machines would also need access to 37 tb of disk to store the  compressed document representations that were not in cache.
of course in practice, more machines may be required to manage the distributed system, to provide backup services for failed machines, and other networking services.
assuming that each machine has 8 gb of ram, and that there are 20 billion pages to index on the web, a total of m = 62  machines would be required for the snippet engine.
the total amount of ram required by a single machine, therefore, would be n/m(8.192 + 10.24 + 8) bytes.
• the offset array that gives the start position of each document in the single, compressed file: 8 bytes per n/m documents.
each document requires 2 kb when compressed with cts (table 1), and only half of each document is required using st sentence reordering, requiring a total of n/m ×0.01× 1024 bytes.
• a cache of 1% of all n/m documents.
assuming an average uncompressed document size of 8 kb [11], this would require n/m × 8.192 bytes of memory.
• the cts model, which should be 1/1000 of the size of the uncompressed collection (using results in  figure 5 and williams et al.
[23]).
each machine, therefore, requires ram to hold the following.
we also assume a balanced load for each machine, so each  machine serves about n/m documents, which is easily achieved in practice.
we will assume that the snippet engine is distributed across m machines, and that there are n web pages in the collection to be indexed and served by the search engine.
in particular, we can scale our results to examine what  resources would be required, using our scheme, to provide a snippet engine for the entire world wide web.
we are assuming, as there is no evidence to the contrary, that this collection and log is representative of search engine input in other domains.
note that these results are generated using a 100gb  subset of the web, and the excite query log gathered from the same period as that subset was created.
both our document compression and compaction schemes dramatically reduce the time taken to generate snippets.
our second major result is that keeping only half of each  document in ram, effectively doubling the cache size, has little affect on the quality of the final snippets generated from those half-documents, provided that the sentences that are kept in memory are chosen using the significant term  algorithm of luhn [12].
our caching simulations show that if as little as 1% of the documents can be cached in ram as part of the snippet engine, possibly distributed over many machines, then around 75% of seeks can be avoided.
primarily, caching documents avoids seek costs to secondary memory for each document that is to be summarized, and is vital for fast snippet generation.
our  experiments not only show that our scheme is over 50% faster than the obvious baseline, but also reveal some very  important aspects of the snippet generation problem.
in this paper we have described the algorithms and  compression scheme that would make a good snippet engine sub-system for generating text snippets of the type shown on the results pages of well known web search engines.
