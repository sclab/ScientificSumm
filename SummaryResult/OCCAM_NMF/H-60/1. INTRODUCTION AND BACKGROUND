finally, in section 6, we base the definition of the probability of  being informative on the results of the previous sections and compare frequency-based and poisson-based definitions. 
in section 5, we emphasise that the theoretical framework of this paper is applicable for both idf and tf .
we show the steps from  possible worlds to binomial distribution and poisson distribution.
in section 4, we link the results of the previous sections to probability theory.
the value e−1 is related to the  logarithm and we investigate in section 3.3 the link to  information theory.
in section 3.2 we show that by assuming independence of documents, we obtain 1 − e−1 ≈ 1 − 0.37 as the upper bound of the noise probability of a term.
these  observations are reported in section 3. we show in section 3.1 that the frequency-based noise probability n(t) n used in the classic idf -definition can be explained by three assumptions: binary term occurrence, constant document containment and disjointness of document containment events.
when investigating the probabilistic interpretation of the 227 normalised idf , we made several observations related to  disjointness and independence of document events.
can we interpret pfreq , the normalised idf , as the probability that the term is informative?
pfreq (t is informative) := idf(t) maxidf maxidf := max({idf(t)|t ∈ t}), maxidf <= − log(1/n) minidf := min({idf(t)|t ∈ t}), minidf >= 0 minidf maxidf ≤ pfreq (t is informative) ≤ 1.0 this frequency-based probability function covers the interval [0; 1] if the minimal idf is equal to zero, which is the case if we have at least one term that occurs in all documents.
let t be the set of terms occurring in a collection.
for example, consider a normalisation based on the maximal idf -value.
an intuitive solution is a normalisation of idf such that we obtain values in the  interval [0; 1].
such that a good estimate of the  probability of relevance is achieved.
: informativeness of terms, structure of documents, quality of documents, age of documents, etc.)
the missing probabilistic interpretation of idf is a problem in probabilistic retrieval models where we combine uncertain knowledge of different dimensions (e.g.
from a probabilistic point of view, tf is a value with a frequency-based probabilistic interpretation whereas idf has an informative rather than a probabilistic interpretation.
in a tf-idf approach, the removal of  stopwords is conceptually obsolete, if stopwords are just words with a low idf .
therefore, the removal of noisy terms (known as stopword removal) is essential when applying tf .
the tf of a noisy term might be high in a document, but noisy terms are not good  candidates for representing a document.
we  associate noise with the document frequency of a term in a collection, and we associate occurrence with the  withindocument frequency of a term.
we use the notion of noisy terms rather than  frequent terms since frequent terms leaves open whether we refer to the document frequency of a term in a collection or to the so-called term frequency (also referred to as  withindocument frequency) of a term in a document.
an  explanation might be the problem of tf with terms that occur in many documents; let us refer to those terms as noisy terms.
the idf alone works better than the tf alone does.
this approach is known as tf-idf , where tf(t, d) (0 <= tf(t, d) <= 1) is the so-called term frequency of term t in document d. the idf reflects the discriminating power (informativeness) of a term, whereas the tf reflects the occurrence of a term.
also, it is well known that the combination of a document-specific term weight and idf works better than idf alone.
with n being the total number of  documents, and n(t) being the number of documents in which term t occurs, the idf is defined as follows: idf(t) := − log n(t) n , 0 <= idf(t) < ∞ ranking based on the sum of the idf -values of the query terms that occur in the retrieved documents works well, this has been shown in numerous applications.
the inverse document frequency (idf ) is one of the most successful parameters for a relevance-based ranking of  retrieved objects.
