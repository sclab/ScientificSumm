this motivation matches the motivation of our paper: we investigate theoretically the assumptions of classical idf and poisson for a better understanding of parameter estimation and combination. 
[1] establishes a theoretical link between tf-idf and  information theory and the theoretical research on the meaning of tf-idf clarifies the statistical model on which the different measures are commonly based.
we restrict in this paper to the discussion of poisson,  however, our results show that indeed a smoother distribution than poisson promises to be a good candidate for improving the estimation of probabilities in information retrieval.
this makes pareto interesting since poisson is felt to be too radical on frequent events.
the pareto distribution compares to the poisson  distribution in the sense that pareto is fat-tailed, i. e. pareto  assigns larger probabilities to large numbers of events than poisson distributions do.
the pareto distribution is used by [2] for term frequency normalisation.
the pareto distribution is the continuous case of the lotka and lotka and zipf show equivalences.
entropy is maximal if all events are equiprobable and the  frequency-based lotka law (n/iλ is the number of scientists that have written i publications, where n and λ are  distribution parameters), zipf and the pareto distribution are  related.
[8] address entropy and bibliometric distributions.
the probability of being  informative defined in our paper can be viewed as the probability of the disjoint terms in the term space of [12].
a key component is the use of a space of disjoint events, where the framework mainly uses terms as disjoint events.
[12] establishes a framework in which information retrieval models are formalised based on probabilistic inference.
[11], [12], [8] and [1] link frequencies and probability  estimation to information theory.
we use in our paper a different notion of noise: we consider a frequency-based noise that corresponds to the document frequency, and we consider a term noise that is based on the independence of document events.
different definitions of idf are put into  context and a notion of noise is defined, where noise is viewed as the complement of idf .
[3], section 3.3, illustrates and summarises  comprehensively the relationships between frequencies, probabilities and poisson.
the results proved again experimentally that a  onedimensional poisson does not work for rare terms, therefore poisson mixtures and additional parameters are proposed.
[7] and [6] address the deviation of idf and poisson, and apply poisson mixtures to achieve better poisson-based  estimates.
our discussion of the poisson  distribution focuses on the document frequency in a collection rather than on the term frequency in a document.
we will generalise the discussion by pointing out that document frequency and term frequency are dual parameters in the collection space and the  document space, respectively.
the poisson model was here applied to the term frequency of a term in a document.
the non-linear scaling of the poisson function showed significant  improvement compared to a linear frequency-based probability.
[10] uses a 2-poisson model for including term frequency-based  probabilities in the probabilistic retrieval model.
[9] shows experimentally that most of the terms (words) in a collection are distributed according to a low dimension n-poisson model.
[4] proposes a 2-poisson model that takes into account the different nature of relevant and non-relevant documents, rare terms (content words) and frequent terms (noisy terms, function words, stopwords).
in this background section, we focus on work that investigates the application of the poisson distribution in ir since a main part of the work presented in this paper addresses the underlying assumptions of poisson.
the relationship between frequencies, probabilities and information theory (entropy) has been the focus of many researchers.
