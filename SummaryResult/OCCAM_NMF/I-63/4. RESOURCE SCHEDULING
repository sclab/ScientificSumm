1 strictly speaking, solving milps to optimality is  npcomplete in the number of integer variables. 
we empirically analyze the performance of this method in section 5.
despite the fact that the complexity of the milp is, in the worst case, exponential1 in the number of binary variables, the complexity of this milp is significantly (exponentially) lower than that of the milp with flat utility functions,  described in section 2.2. this result echos the efficiency gains reported in [6] for single-shot resource-allocation problems, but is much more pronounced, because of the explosion of the flat utility representation due to the temporal aspect of the problem (recall the prohibitive complexity of the  combinatorial optimization in section 2.2).
the number of constraints (not including the degenerate constraints in (7)) in the milp is: tm + tm|ω| + bτ|ω| + bτ|m||ω|.
however, notice that all but tm|m| of the θ are set to zero by constraint (7), which also  immediately forces all but tm|m||ω| of the δ to be zero via the constraints (8).
then, the number of optimization variables is: tm + bτ|m||ω| + bτ|m|, tm of which are continuous (xm), and bτ|m||ω| + bτ|m| are binary (δ and θ).
let tm = p tm = p m(τa m − τd m + 1) be the sum of the lengths of the arrival-departure windows across all agents.
as a rough measure of the complexity of this milp, let us consider the number of optimization variables and  constraints.
to summarize, table 1 together with the  conservationof-flow constraints from (12) defines the milp that  simultaneously computes an optimal resource assignment for all agents across time as well as optimal finite-horizon mdp policies that are valid under that resource assignment.
this is accomplished via the linear constraint (11), where z ≥ 2 is a constant that is used to turn off the constraints when θm(τ) = 0 or θm(τ + 1) = 0. this constraint is not used for the dynamic problem formulation, where resources can be reallocated between agents at every time step.
this condition is also trivially expressed as a linear inequality (10) in table 1. finally, for the problem formulation where resource  assignments are static during a lifetime of an agent, we add a constraint that ensures that the resource-usage variables δ do not change their value while the agent is active (θ = 1).
after implementing the above constraint, which enforces the meaning of δ, we add a constraint that ensures that the agents" resource usage never exceeds the amounts of  available resources.
this is done via constraint (9) in table 1, which is nearly identical to the analogous constraint from [6].
in a similar fashion, we have to make sure that the resource-usage variables δ are also synchronized with the occupation measure xm.
constraint (6) sets the value of θ to match the policy defined by the occupation measure xm.
this constraint can also be enforced via a linear inequality on θ and δ, as shown in (8).
this is accomplished by constraint (7) in table 1. furthermore, agents should not be using resources while they are inactive.
on autonomous agents and multi-agent systems (aamas 07) indicators θ, as shown in (6) in table 1. another constraint we have to add-because the activity indicators θ are defined on the global timeline τ-is to  enforce the fact that the agent is inactive outside of its  arrivaldeparture window.
joint conf.
this meaning of θ can be enforced with a linear constraint that synchronizes the  values of the agents" occupation measures xm and the activity 1224 the sixth intl.
• θm = {0, 1}, ∀m ∈ m, τ ∈ [0, bτ] are indicator variables that specify whether agent m is active (i.e., executing its mdp) at time τ. the meaning of resource-usage variables δ is illustrated in figure 1: δm(τ, ω) = 1 only if resource ω is allocated to agent m at time τ. the meaning of the activity  indicators θ is illustrated in figure 2b: when agent m is in either the start state sb or the finish state sf , the corresponding θm = 0, but once the agent becomes active and enters one of the other states, we set θm = 1 .
to  formulate these resource constraints, we use the following binary variables: • δm(τ, ω) = {0, 1}, ∀m ∈ m, τ ∈ [0, bτ], ω ∈ ω, which serve as indicator variables that define whether agent m possesses resource ω at time τ. these are analogous to the static indicator variables used in the one-shot static resource-allocation problem in [6].
using this lp as a basis, we augment it with constraints that ensure that the resource usage implied by the agents" occupation measures {xm} does not violate the global  resource requirements bϕ at any time step τ ∈ [0, bτ].
tm = τd m − τa m + 1 is the time horizon for the agent"s mdp.
θm(τ) = 1 and θm(τ + 1) = 1 =⇒ δm(τ, ω) = δm(τ + 1, ω) δm(τ, ω) − z(1 − θm(τ + 1)) ≤ δm(τ + 1, ω) + z(1 − θm(τ)) δm(τ, ω) + z(1 − θm(τ + 1)) ≥ δm(τ + 1, ω) − z(1 − θm(τ)) ∀m ∈ m, ω ∈ ω, τ ∈ [0, bτ] (11) table 1: milp for globally optimal resource scheduling.
only enabled for scheduling with static assignments.
δm(τ, ω) = 0, ϕm(a, ω) = 1 =⇒ xm(s, a, τ − τa m + 1) = 0 ∀s /∈ {sb , sf } 1/|a| x a ϕm(a, ω) x s/∈{sb,sf } xm(s, a, t) ≤ δm(t + τa m − 1, ω) ∀m ∈ m, ω ∈ ω, t ∈ [1, tm] (9) resource bounds x m δm(τ, ω) ≤ bϕ(ω) ∀ω ∈ ω, τ ∈ [0, bτ] (10) agent cannot change resources while  active.
θm(τ) = 0 =⇒ xm(s, a, τ −τa m+1) = 0 ∀s /∈ {sb , sf }, a ∈ a x s/∈{sb,sf } x a xm(s, a, t) ≤ θm(τa m + t − 1) ∀m ∈ m, ∀t ∈ [1, tm] (6) agent can only be active in τ ∈ (τa m, τd m) θm(τ) = 0 ∀m ∈ m, τ /∈ (τa m, τd m) (7) cannot use resources when not active θm(τ) = 0 =⇒ δm(τ, ω) = 0 ∀τ ∈ [0, bτ], ω ∈ ω δm(τ, ω) ≤ θm(τ) ∀m ∈ m, τ ∈ [0, bτ], ω ∈ ω (8) tie x to δ (nonzero x forces corresponding δ to be nonzero.)
objective function (sum of expected rewards over all agents) max x m x s x a rm(s, a) x t xm(s, a, t) (5) meaning implication linear constraints tie x to θ. agent is only active when  occupation measure is nonzero in original mdp states.
on autonomous agents and multi-agent systems (aamas 07) 1223 (a) (b) figure 2: illustration of augmenting an mdp to allow for variable starting and stopping times: a) (left) the original two-state mdp with a single action; (right) the augmented mdp with new states sb and sf and the new action a∗ (note that the origianl transitions are not changed in the augmentation process); b) the augmented mdp displayed as a trajectory through time (grey lines indicate all transitions, while black lines indicate a given trajectory.
in the absence of resource constraints, the agents"  finitehorizon mdps are completely independent, and the globally optimal solution can be trivially obtained via the following lp, which is simply an aggregation of single-agent  finitehorizon lps: max x m x s x a rm(s, a) x t xm(s, a, t) subject to: x a xm(σ, a, t + 1) = x s,a pm(σ|s, a)xm(s, a, t), ∀m ∈ m, σ ∈ s, t ∈ [1, tm − 1]; x a xm(s, a, 1) = αm(s), ∀m ∈ m, s ∈ s; (12) where xm(s, a, t) is the occupation measure of agent m, and the sixth intl.
in the rest of this section, we incrementally develop a mixed integer program (milp) that achieves this.
the resulting optimization problem then  simultaneously solves the agents" mdps and resource-scheduling  problems.
in this section and below, all mdps are assumed to be the  augmented mdps as defined in section 4.1. our approach is similar to the idea used in [6]: we  begin with the linear-program formulation of agents" mdps (1) and augment it with constraints that ensure that the corresponding resource allocation across agents and time is valid.
4.2 milp for resource scheduling given a set of augmented mdps, as defined above, the goal of this section is to formulate a global optimization  program that solves the resource-scheduling problem.
note that if we wanted to model a problem where agents could pause their mdps at arbitrary time steps (which might be useful for domains where dynamic reallocation is  possible), we could easily accomplish this by including an extra action that transitions from each state to itself with zero reward.
for example, the augmented mdps shown in figure 2b (which starts in state sb at time τ = 2) would be constructed from an mdp with original arrival time τ = 3. figure 2b also shows a sample trajectory through the state space: the agent starts in state sb , transitions into the state space s of the original mdp, and finally exists into the sink state sf .
further, in order to account for the new starting state, we begin the mdp one time-step earlier, setting τa m ← τa m − 1. this will not affect the resource allocation due to the resource constraints only being enforced for the original mdp states, as will be discussed in the next section.
for example, in figure 1a, for agent m2 this would happen at time τ = 4. once the agent gets to the end of its activity window (time τ = 6 for agent m2 in figure 1a), it performs the start/stop action, which takes it into the sink finish state sf at time τ = 7. more precisely, given an mdp s, a, pm, rm, αm , we  define an augmented mdp s , a , pm, rm, αm as follows: s = s ∪ sb ∪ sf ; a = a ∪ a∗ ; p (s|sb , a∗ ) = α(s), ∀s ∈ s; p (sb |sb , a) = 1.0, ∀a ∈ a; p (sf |s, a∗ ) = 1.0, ∀s ∈ s; p (σ|s, a) = p(σ|s, a), ∀s, σ ∈ s, a ∈ a; r (sb , a) = r (sf , a) = 0, ∀a ∈ a ; r (s, a) = r(s, a), ∀s ∈ s, a ∈ a; α (sb ) = 1; α (s) = 0, ∀s ∈ s; where all non-specified transition probabilities are assumed to be zero.
to accomplish this, we augment each agent"s mdp with two new states (start and finish states sb , sf ,  respectively) and a new start/stop action a∗ , as illustrated in figure 2. the idea is that an agent stays in the start state sb until it is ready to execute its mdp, at which point it performs the start/stop action a∗ and transitions into the state space of the original mdp with the transition  probability that corresponds to the original initial distribution α(s).
therefore, an important part of the global decision-making problem is to decide the window of time during which each of the agents is active (i.e., executing its mdp).
similarly, agents m2 and m3 only execute their mdps in the intervals τ ∈ [4, 6] and τ ∈ [4, 10], respectively.
for example, in the problem shown in figure 1a, agent m1 releases all resources after time τ = 3, at which point the execution of its mdp is halted.
in other words, the mdps cannot be paused and resumed.
4.1 augmenting agents" mdps in the model described in the previous section, we assume that if an agent does not possess the necessary resources to perform actions in its mdp, its execution is halted and the agent leaves the system.
first, we perform a preprocessing step that augments the agent mdps; this process is described in section 4.1.  second, using these augmented mdps we construct a global optimization problem, which is described in section 4.2.
our resource-scheduling algorithm proceeds in two stages.
