this straightforward approach to solving both of these scheduling problems requires an enumeration and solution of either 2|ω| tm (static allocation) or p t∈[1,tm] 2|ω|t  (dynamic reallocation) mdps for each agent, which very quickly becomes intractable with the growth of the number of  resources |ω| or the time horizon tm. 
in this case, the number of ψ values is exponential in each agent"s planning horizon tm, resulting in a much larger program.
the same set of equations (3) can be used to solve this dynamic scheduling problem, but the integer program is  different because of the difference in how ψ is defined.
therefore, in this case there are p t∈[1,tm](2|ω| )t ∼ 2|ω|tm possibilities of resource bundles assigned to different time slots, for the tm different time horizons.
let the variable ψ ∈ ψm in this case enumerate all possible resource bundles for which at most one bundle may be assigned to agent m at each time step.
for the scheduling problem where the agents are able to dynamically reallocate resources, each agent must specify a value for every combination of bundles and time steps within its time horizon.
this allocation problem is np-complete, even when considering only a single time step, and its difficulty increases significantly with multiple time steps because of the increasing number of values of ψ. the problem of finding an optimal allocation that satisfies the global constraint that the amount of each resource ω allocated to all agents does not exceed the available amount bϕ(ω) can be expressed as the following integer program: max x m∈m x ψ∈ψm zψ mvψ m subject to: x ψ∈ψm zψ m ≤ 1, ∀m ∈ m; x m∈m x ψ∈ψm zψ mnm(ψ, τ, ω) ≤ bϕ(ω), ∀τ ∈ [1, bτ], ∀ω ∈ ω; (3) the first constraint in equation 3 says that no agent can receive more than one bundle, and the second constraint ensures that the total assignment of resource ω does not, at any time, exceed the resource bound.
this allocation is expressed as an indicator variable zψ m ∈ {0, 1} that shows whether ψ is assigned to agent m. for time τ and resource ω, the function nm(ψ, τ, ω) ∈ {0, 1} indicates whether the bundle in ψ uses resource ω at time τ (we make the assumption that agents have binary resource requirements).
the agent m must provide a value vψ m for each ψ, and the coordinator will allocate at most one ψ (resource, time horizon) pair to each agent.
let the variable ψ ∈ ψm enumerate all  possible pairs of resource bundles and time horizons for agent m, so there are 2|ω| × tm values for ψ (the space of bundles is exponential in the number of resource types |ω|).
an agent will get at most one resource bundle for one of the time horizons.
let ω be the set of resources to be allocated among the agents.
on autonomous agents and multi-agent systems (aamas 07) 1221 for the scheduling problem where agents have static  resource requirements within their finite-horizon mdps, the agents provide a valuation for each resource bundle for each possible time horizon (from [1, tm]) that they may use.
joint conf.
we assume τd m < bτ, ∀m ∈ m. the sixth intl.
let bτ be the global time horizon for the problem, before which all of the agents" mdps must finish.
hence, agent m will execute an mdp with time horizon no greater than tm = τd m−τa m+1.
let each agent m ∈ m execute its mdp within the arrival-departure time interval τ ∈ [τa m, τd m].
when resources can be allocated at different times to  different agents, each agent must submit valuations for  every combination of possible time horizons.
then, each agent would provide valuations for each possible resource bundle over time to a centralized coordinator, who would compute the optimal resource assignments across time based on these valuations.
2.2 combinatorial resource scheduling a straightforward approach to resource scheduling for a set of agents m, whose values for the resources are induced by stochastic planning problems (in our case, finite-horizon mdps) would be to have each agent enumerate all possible resource assignments over time and, for each one, compute its value by solving the corresponding mdp.
in such cases, α becomes a part of the  problem input, and a resulting policy is only optimal for that particular α. this result is well known for infinite-horizon mdps with various types of constraints [1, 6], and it also holds for our finite-horizon model, which can be easily  established via a line of reasoning completely analogous to the arguments in [6].
however, for mdps with resource constraints (as defined below in section 3), uniformly-optimal policies do not in general exist.
therefore, an optimal policy can be obtained by using an arbitrary constant α(s) > 0 (in particular, α(s) = 1 will result in x(s, a, t) = π(s, a, t)).
(2) note that the standard unconstrained finite-horizon mdp, as described above, always has a uniformly-optimal  solution (optimal for any initial distribution α(s)).
an optimal (non-stationary) policy is obtained from the occupation measure as follows: π(s, a, t) = x(s, a, t)/ x a x(s, a, t) ∀s ∈ s, t ∈ [1, t].
however, we can also formulate the problem as the following linear program (similarly to the dual lp for infinite-horizon discounted mdps [13, 6, 7]): max x s x a r(s, a) x t x(s, a, t) subject to: x a x(σ, a, t + 1) = x s,a p(σ|s, a)x(s, a, t) ∀σ, t ∈ [1, t − 1]; x a x(s, a, 1) = α(s), ∀s ∈ s; (1) where α(s) is the initial distribution over the state space, and x is the (non-stationary) occupation measure (x(s, a, t) ∈ [0, 1] is the total expected number of times action a is  executed in state s at time t).
the above is the most common way of computing the optimal value function (and therefore an optimal policy) for a finite-horizon mdp.
this optimal value function can be easily computed using dynamic programming, leading to the following optimal  policy π, where π(s, a, t) is the probability of executing action a in state s at time t: π(s, a, t) = ( 1, a = argmaxa r(s, a) + p σ p(σ|s, a)v(σ, t + 1), 0, otherwise.
an optimal policy for such a problem is to act greedily with respect to the optimal value function, defined recursively by the following system of finite-time bellman equations [2]: v(s, t) = max a r(s, a) + x σ p(σ|s, a)v(σ, t + 1), ∀s ∈ s, t ∈ [1, t − 1]; v(s, t) = 0, ∀s ∈ s; where v(s, t) is the optimal value of being in state s at time t ∈ [1, t].
the agent"s optimal  policy is then a function of current state s and the time until the horizon.
2.1 markov decision processes a stationary, finite-domain, discrete-time mdp (see, for example, [13] for a thorough and detailed development) can be described as s, a, p, r , where: s is a finite set of  system states; a is a finite set of actions that are available to the agent; p is a stationary stochastic transition function, where p(σ|s, a) is the probability of transitioning to state σ upon executing action a in state s; r is a stationary reward function, where r(s, a) specifies the reward obtained upon executing action a in state s. given such an mdp, a decision problem under a finite horizon t is to choose an optimal action at every time step to maximize the expected value of the total reward accrued during the agent"s (finite) lifetime.
in the rest of this section, we first introduce some  necessary background on finite-horizon mdps and present a linear-programming formulation that serves as the basis for our solution algorithm developed in section 4. we also  outline the standard methods for combinatorial resource  scheduling with flat resource values, which serve as a comparison benchmark for the new model developed here.
however, since the focus of our work is on scheduling  problems, and a large part of the optimization problem is to decide how resources are allocated in time among agents with finite arrival and departure times, we model the agents" planning problems as finite-horizon mdps, in contrast to previous work that used infinite-horizon discounted mdps.
similarly to the model used in previous work on  resourceallocation with mdp-induced preferences [6, 7], we define the value of a set of resources to an agent as the value of the best mdp policy that is realizable, given those resources.
