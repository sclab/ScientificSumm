categories and subject descriptors: h.3.3 [information retrieval]: retrieval models general terms: algorithms, experimentation 
the  result is a meta-feedback algorithm that is both more robust and more precise than the original strong baseline method.
we find that  resampling documents helps increase individual feedback model precision by removing noise terms, while sampling from the query improves robustness (worst-case performance) by  emphasizing terms related to multiple query aspects.
we then perform model combination over several enhanced models, each based on a slightly modified query sampled from the original query.
treating the baseline feedback method as a black box, and the output feedback model as a random variable, we estimate a posterior distribution for the  feedback model by resampling a given query"s top-retrieved  documents, using the posterior mean or mode as the enhanced feedback model.
existing pseudo-relevance feedback methods typically  perform averaging over the top-retrieved documents, but  ignore an important statistical dimension: the risk or variance associated with either the individual document models, or their combination.
