figure 1: estimating the uncertainty of the feedback model for a single query. 
1 for example, an expected parameter vector conditioned on the query observation is formed from top-retrieved  documents, which are treated as training strings (see [10], p. 62).
a model"s weight combines two complementary factors: the model"s probability of generating the query, and the variance of the model, with high-variance models getting lower weight.
we also perform model combination using several improved feedback language models obtained by a small number of new queries sampled from the original query.
this process is shown in figure 1. as we show later, the mean and mode may vary significantly from the single feedback model proposed by the baseline method.
we use the posterior mean or mode as the improved feedback model estimate.
to do this, we systematically vary the inputs to the baseline feedback method and fit a dirichlet distribution to the output.
therefore, we propose a method for estimating uncertainty associated with an individual feedback model in terms of a posterior distribution over language models.
our main hypothesis in this paper is that estimating the uncertainty in feedback is useful and leads to better  individual feedback models and more robust combined models.
the use of an expectation is reasonable for practical and  theoretical reasons, but by itself ignores potentially valuable information about the risk of the feedback model.
for example, the document vectors may be combined with equal weighting, as in rocchio, or by query likelihood, as may be done using the relevance model1 .
next, a single feedback model vector is computed according to some sort of average,  centroid, or expectation over the set of possibly-relevant  document models.
first, a set of top-retrieved documents is obtained from an initial query and assumed to approximate a set of relevant documents.
current algorithms for pseudo-relevance feedback (prf) tend to follow the same basic method whether we use  vector space-based algorithms such as rocchio"s formula [16], or more recent language modeling approaches such as  relevance models [10].
in this way, retrieval algorithms may attempt to quantify the risk or uncertainty associated with their  output rankings, or improve the stability or precision of their internal calculations.
with this in mind, we wish to treat many important quantities calculated by the  retrieval system, whether a relevance score for a document, or a weight for a query expansion term, as random  variables whose true value is uncertain but where the  uncertainty about the true value may be quantified by replacing the fixed value with a probability distribution over possible values.
even if the query were perfectly specified, language in the collection documents is inherently complex and ambiguous and matching such language effectively is a formidable problem by itself.
not only do we not know the queries that will be presented to our retrieval algorithm ahead of time, but the user"s  information need may be vague or incompletely specified by these queries.
uncertainty is an inherent feature of information retrieval.
