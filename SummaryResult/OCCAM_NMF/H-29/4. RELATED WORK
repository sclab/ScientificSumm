they  propose tailoring the stopping parameter η based on a function of some quality measure of feedback documents. 
a roughly equal balance of query and expansion model is implied by their em stopping condition.
finally, in language modeling approaches to feedback, tao and zhai [18] describe a method for more robust feedback that allows each document to have a different feedback α. the feedback weights are derived automatically using  regularized em.
it would be interesting to study the connections between this approach and our own  modelfitting method.
the use of variance as a feedback model quality measure occurs indirectly through the application of pca.
(7) in the case h = 1, we have a single left singular vector φ: qexp = qorig + (φt qorig)φ so that the dot product φt qorig is a type of dynamic weight on the expanded query that is based on the similarity of the original query to the expanded query.
principal component analysis (pca) is then applied to the ˆwi to obtain the matrix φ of h left singular vectors φh that are used to obtain the new, expanded query qexp = qorig + φt φqorig.
for each si, the normalized centroid vector ˆwi of the documents is  calculated.
they used r query variations to obtain r different sets sr of top-ranked  documents that have been intersected with the top-ranked  documents obtained from the original query qorig.
ando and zhang proposed a method that they call  structural feedback [3] and showed how to apply it to query  expansion for the trec genomics track.
instead of using sampling, they were able to derive closed-form expressions for classifier variance by assuming base classifiers using simple types of inference networks.
[11] used  queryspecific variance estimates of classifier outputs to perform improved model combination.
lee et al.
estimates of output variance have recently been used for improved text classification.
this seems in accord with our own findings for individual feedback models.
downweighting terms with high variance resulted in improved average precision.
in a series of simulations that simplified the problem to 2-feature documents, they found that average precision degrades as term frequency variance - high  noiseincreases.
greiff, morgan and ponte [8] explored the role of variance in term weighting.
their study did not find significant improvements in either robustness (ri) or map on their corpora.
the essence of their method is that they allow skipping of some top-ranked documents, based on a clustering criterion, in order to select a more  varied and novel set of documents later in the ranking for use by a traditional pseudo-feedback method.
[17] proposed an approach to improving the robustness of pseudo-relevance feedback using a method they call selective sampling.
sakai et al.
this is related to our use of document sampling to estimate the risk of the feedback model built from the different sets of top-retrieved  documents.
on the document side, recent work by zhou & croft [21] explored the idea of adding noise to documents, re-scoring them, and using the stability of the resulting rankings as an estimate of query difficulty.
xu and croft"s method of local context  analysis (lca) [19] includes a factor in the empirically-derived weighting formula that causes expansion terms to be  preferred that have connections to multiple query terms.
the idea of examining the overlap between lists of  suggested terms has also been used in early query expansion approaches.
their combination method gave modest positive  improvements in average precision.
in a set of trec topics they found wide average variation in the rank-distance of terms from different expansion  methods.
investigated combining terms from individual distributional methods using a term-reranking combination heuristic.
and carpineto et al.
in particular, the studies of amati et al.
model combination is performed using heuristics.
these studies use the idea of creating multiple  subqueries and then examining the nature of the overlap in the documents and/or expansion terms that result from each subquery.
[2], among others.
[5], and amati et al.
[20], carpineto et al.
our use of query variation was inspired by the work of yomtov et al.
our approach is related to previous work from several  areas of information retrieval and machine learning.
