any opinions, findings, and conclusions or recommendations expressed in this material are the authors.
and do not necessarily reflect those of the sponsors. 
of education grant #r305g03123.
acknowledgements we thank paul bennett for valuable discussions related to this work, which was supported by nsf grants #iis-0534345 and #cns-0454018, and u.s. dept.
in future work, we envision an investigation into how varying the set of sampling methods used and the number of samples controls the trade-off between  robustness, accuracy, and efficiency.
it also gives small but consistent gains in  top10 precision.
our results on standard trec collections show that our framework  improves the robustness of a strong baseline feedback method across a variety of collections, without sacrificing average precision.
we believe it is likely that any reasonably effective baseline feedback algorithm would benefit from our approach.
while our study uses the language modeling approach as a framework for experiments, we make few assumptions about the actual workings of the feedback algorithm.
applications such as selective expansion may then be  implemented in a principled way.
such variance estimates, for example, may be naturally used in a bayesian framework for improved model estimation and combination.
the use of sampling is a very flexible and powerful device and is  motivated by our general desire to extend current models of  retrieval by estimating the risk or variance associated with the parameters or output of retrieval processes.
we have presented a new approach to pseudo-relevance feedback based on document and query sampling.
