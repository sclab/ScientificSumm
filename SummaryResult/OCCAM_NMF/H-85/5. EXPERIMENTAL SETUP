to make sure we are not favoring userbehavior, we also tested all other strategies on the same hold-out test sets, resulting in the same accuracy results as testing over the complete datasets. 
hence, all strategies except for userbehavior were tested on the full set of queries and associated relevance preferences, while userbehavior was tested on a randomly chosen hold-out subset of the queries as described above.
it is worth noting that both the ad-hoc sa and sa+n, as well as the distribution-based strategies (cd, cdiff, and cd+cdiff), do not require a separate training and test set, since they are based on heuristics for detecting anomalous click frequencies for results.
the sampling was done per query (i.e., all results for a chosen query were included in the respective dataset, and there was no overlap in queries between training and test sets).
to evaluate userbehavior we train and validate on 75% of labeled queries, and test on the remaining 25%.
training/test split: the only strategy for which splitting the datasets into training and test was required was the userbehavior method.
note that the current ranker implementation was trained over a superset of the rated query/url pairs in our datasets, but using the same truth labels as we do for our evaluation.
this heuristic allows us to do graceful recall-precision tradeoff using the score of the highest ranked result to threshold the queries (section 4.3) • current: current search engine ranking (section 4.1).
in our preliminary experiments we observed that higher ranker scores indicate higher confidence in the predictions.
• userbehavior: we order predictions based on decreasing highest score of any page.
• cd+cdiff: the strategy combining cd and cdiff as the union of predicted preferences from both (section 4.2).
• cdiff: our generalization of the cd strategy that explicitly uses the relevance component of clickthrough probabilities to induce preferences between search results (section 4.2).
• cd: our refinement of sa+n that takes advantage of our mixture model of clickthrough distribution to select trusted clicks for interpretation (section 4.2).
specifically, we compare the following strategies: • sa: the skip above clickthrough strategy (section 4.2) • sa+n: a more comprehensive extension of sa that takes better advantage of current search engine ranking.
we compared our userbehavior model (section 4.3) to previously published implicit feedback interpretation techniques and some variants of these approaches (section 4.2), and to the current search engine ranking based on query and page features alone (section 4.1).
5.3 methods compared we considered a number of methods for comparison.
furthermore, the data size is order of magnitude larger than any study reported in the literature.
these datasets were collected as part of normal user experience and hence have different characteristics than previously reported datasets collected in laboratory settings.
• q20: queries in q1 with at least 20 clicks (1000 queries total, 12,922 query-url pairs).
in order to better understand the effect of the amount of user interaction data available for a query on accuracy, we created subsets of our data (q1, q10, and q20) that contain different amounts of interaction data: • q1: human-rated queries with at least 1 click on results recorded (3500 queries, 28,093 query-url pairs) • q10: queries in q1 with at least 10 clicks (1300 queries, 18,728 query-url pairs).
this data collection was part of voluntary opt-in feedback submitted by users from october 11 through october 31. these three weeks (21 days) of user interaction data was filtered to include only the users in the english-u.s. market.
the user interactions were harvested from anonymous browsing traces that immediately followed a query submitted to the web search engine.
in addition for these queries we also had user interaction data for more than 120,000 instances of these queries.
for each query the top 10 returned search results were manually rated on a 6-point scale by trained judges as part of ongoing relevance improvement effort.
5.2 datasets for evaluation we used 3,500 queries that were randomly sampled from query logs(for a major web search engine.
we discuss this issue further when we consider extensions to the current work in section 7.
a drawback of this evaluation measure is that some preferences may be more valuable than others, which pairwise agreement does not capture.
the overall recall and precision are computed as the average of query recall and query precision, respectively.
• query recall: fraction of preferences obtained from explicit human judgment for q that were correctly predicted.
for our task, query precision and query recall for a query q are defined as: • query precision: fraction of predicted preferences for results for q that agree with preferences obtained from explicit human judgment.
specifically, we report the average query recall and precision.
while our task of computing pairwise agreement is different from the absolute relevance ranking task, the metrics are used in the similar way.
in order to compute the accuracy of our preference predictions with respect to the correct preferences, we adapt the standard recall and precision measures [20].
to avoid ambiguity in evaluation, we discard all ties (i.e., pairs with equal label).
we discuss other applications of our models beyond web search ranking in section 7. to create our set of test pairs we take each query and compute the cross-product between all search results, returning preferences for pairs according to the order of the associated relevance labels.
the evaluation is based on comparing preferences predicted by various models to the correct preferences derived from the explicit user relevance judgments.
furthermore, for many applications such as tuning ranking functions, pairwise preference can be used directly for training [1,4,9].
this allows us to compare to previous work [9,10].
5.1 evaluation methodology and metrics our evaluation focuses on the pairwise agreement between preferences for results.
then we describe the datasets (section 5.2) and the methods we compared in this study (section 5.3).
we first describe the methodology used, including our evaluation metrics (section 5.1).
we now describe our experimental setup.
