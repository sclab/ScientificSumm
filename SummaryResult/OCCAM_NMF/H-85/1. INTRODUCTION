we discuss our results and outline future directions and various applications of this work in section 7, which concludes the paper. 
• a thorough evaluation of our user behavior models, as well as of previously published state-of-the-art techniques, over a large set of web search sessions (sections 5 and 6).
• extensions of existing clickthrough strategies to include richer browsing and interaction features (section 4).
our contributions include: • a distributional model of user behavior, robust to noise within individual user sessions, that can recover relevance preferences from user interactions (section 3).
we present techniques to automatically interpret the collective behavior of users interacting with a web search engine to predict user preferences for search results.
automatically learning to interpret user behavior would allow systems to adapt to changing conditions, changing user behavior patterns, and different search settings.
hence, it is preferable to automatically induce feedback interpretation strategies from large amounts of user interactions.
furthermore, observations and insights obtained in laboratory settings do not necessarily translate to real world usage.
by using the aggregated behavior of large numbers of users (and not treating each user as an individual expert) we can correct for the noise inherent in individual interactions, and generate relevance judgments that are more accurate than techniques not specifically designed for the web search setting.
but the amount of the user interaction data is orders of magnitude larger than anything available in a non-web-search setting.
individual users may behave irrationally or maliciously, or may not even be real users; all of this affects the data that can be gathered.
a significant distinction is that web search is not controlled.
therefore, it is not clear whether these techniques will work for general real-world web search.
however, most traditional ir work was performed over controlled test collections and carefully-selected query sets and tasks.
recently, automatic or implicit relevance feedback has developed into an active area of research in the information retrieval community, at least in part due to an increase in available resources and to the rising popularity of web search.
if we could turn these interactions into relevance judgments, we could obtain large amounts of data for evaluating, maintaining, and improving information retrieval systems.
at the same time, millions of people interact daily with web search engines, providing valuable implicit feedback through their interactions with the search results.
however, explicit human ratings are expensive and difficult to obtain.
traditionally, search relevance is measured by using human assessors to judge the relevance of query-document pairs.
relevance measurement is crucial to web search and to information retrieval in general.
