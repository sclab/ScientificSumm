typical structures of a lattice and a wcn are given in figure 1. figure 1: typical structures of a lattice and a wcn. 
as stated in [18], although wcns are more compact than word lattices, in general the 1-best path obtained from wcn has a better word accuracy than the 1-best path obtained from the corresponding word lattice.
the 1-best path of a wcn is obtained from the path  containing the best hypotheses.
3. traverse the word lattice, and align all the transitions with the pivot, merging the transitions that  correspond to the same word (or label) and occur in the same time interval by summing their posterior  probabilities.
2. extract a path from the word lattice (which can be the 1-best, the longest or any random path), and call it the pivot path of the alignment.
as explained in [13], the three main steps for building a wcn from a word lattice are as follows: 1. compute the posterior probabilities for all edges in the word lattice.
one of the main advantages of wcn is that it also provides an alignment for all of the words in the lattice.
each edge (u, v) is labeled with a word hypothesis and its posterior probability, i.e., the probability of the word given the signal.
[13] propose a compact representation of a word lattice called word  confusion network (wcn).
[18] and hakkani-tur et al.
mangu et al.
the 1-best path transcript is obtained from the lattice using dynamic programming techniques.
each vertex in a lattice is  associated with a timestamp and each edge (u, v) is labeled with a word or phone hypothesis and its prior probability, which is the probability of the signal delimited by the timestamps of the vertices u and v, given the hypothesis.
typically, asr generates lattices that can be considered as directed acyclic graphs.
for best recognition results, a speaker-independent acoustic model and a  language model are trained in advance on data with similar characteristics.
it works in speaker-independent mode.
system we use an asr system for transcribing speech data.
