the authors also want to thank sven koenig and anonymous reviewers for their valuable comments. 
acknowledgments this material is based upon work supported by the darpa/ipto coordinators program and the air force research  laboratory under contract no.
however, similarly to [6], they fail to address the lack of global state knowledge, which is a fundamental issue in decentralized planning.
finally, value function techniques have been studied in context of single agent mdps [7] [9].
beyond oc-dec-mdp, there are other locally optimal algorithms for dec-mdps and  decpomdps [8] [12], [13], yet, they have traditionally not dealt with uncertain execution times and temporal constraints.
unfortunately, they fail to scale up to large-scale domains at present time.
furthermore, as discussed in section 4, there are globally optimal algorithms for solving dec-mdps with temporal constraints [1] [11].
in terms of related work, we have extensively discussed the  ocdec-mdp algorithm [4].
our heuristic solution method, called value function propagation (vfp), provided two  orthogonal improvements of oc-dec-mdp: (i) it speeded up  oc-decmdp by an order of magnitude by maintaining and manipulating a value function for each method rather than a separate value for each pair of method and time interval, and (ii) it achieved better solution qualities than oc-dec-mdp because it corrected the  overestimation of the opportunity cost of oc-dec-mdp.
in this  paper, we improved a state-of-the-art heuristic solution method for dec-mdps, called oc-dec-mdp, that has recently been shown to scale up to large dec-mdps.
decentralized markov decision process (dec-mdp) has been very popular for modeling of agent-coordination problems, it is very  difficult to solve, especially for the real-world domains.
