in the next two sections, we address both of these shortcomings. 
as a result, methods that precede the method mj  overestimate the value for enabling mj which, as we show later, can have disastrous consequences.
as we show later, oc-dec-mdp splits vj,[τ,τ ] into parts that may overestimate vj,[τ,τ ] when summed up again.
second, while oc-dec-mdp emphasizes on precise calculation of values vj,[τ,τ ], it fails to address a critical issue that determines how the values vj,[τ,τ ] are split given that the method mj has  multiple enabling methods.
since the runtime of the whole algorithm is proportional to the runtime of this operation, especially for big time horizons δ, the oc-  decmdp algorithm runs slow.
while such state representation is beneficial, in that the problem can be solved with a standard value iteration  algorithm, it blurs the intuitive mapping from time t to the expected total reward for starting the execution of mj at time t.  consequently, if some method mi enables method mj, and the values vj,[τ,τ ]∀τ,τ ∈[0,δ] are known, the operation that calculates the  values vi,[τ,τ ]∀τ, τ ∈ [0, δ] (during the value propagation phase), runs in time o(i2 ), where i is the number of time intervals 3 .
first, each of oc-dec-mdp states is a pair mj, [τ, τ ] , where [τ, τ ] is a time interval in which method mj can be executed.
there are two shortcomings of the oc-dec-mdp algorithm that we address in this paper.
if policy π does not improve π, the algorithm terminates.
we call this step a  probability propagation phase.
it then propagates the new probabilities pi,[τ,τ ] from source methods to sink methods.
step 2: given the values vi,[τ,τ ] from step 1, the algorithm chooses the most profitable method execution intervals which are stored in a new policy π .
we call this step a value propagation phase.
this propagation uses the probabilities pi,[τ,τ ] from previous algorithm iteration.
it then performs two steps: step 1: it propagates from sink methods to source methods the values vi,[τ,τ ], that represent the expected utility for executing method mi in the time interval [τ, τ ].
at each iteration, the algorithm starts with some policy π, which uniquely determines the probabilities pi,[τ,τ ] that method mi will be performed in the time interval [τ, τ ].
the idea of the  oc-decmdp algorithm is to start with the earliest starting time policy π0 (according to which an agent will start executing the method m as soon as m has a non-zero chance of being already enabled), and then improve it iteratively, until no further improvement is  possible.
specially, the oc-dec-mdp  algorithm [4] is particularly significant, as it has shown to easily scale up to domains with hundreds of methods.
4.2 locally optimal algorithms following the limited applicability of globally optimal algorithms for dec-mdps with temporal constraints, locally optimal  algorithms appear more promising.
however, csa complexity is double  exponential in the size of ti, and for our domains tj can store all values ranging from 0 to δ.
the complexity of our model could be reduced if we considered its more restricted version; in particular, if each method mj was allowed to be  enabled at time points t ∈ tj ⊂ [0, δ], the coverage set algorithm (csa) [1] could be used.
thus,  globally optimal algorithms used for solving real-world problems are unlikely to terminate in reasonable time [11].
this double dependency results from the fact, that the expected reward for starting the execution of method mj at time t also depends on the probability that method mj will be enabled by time t. consequently, if time is discretized, one needs to consider δ|m| candidate policies in order to find π∗ .
unfortunately, for our model, the optimal  policy for method mj also depends on policies for methods mi ∈ {m| m, mj ∈ c≺}.
4.1 optimal algorithms optimal joint policy π∗ is usually found by using the bellman  update principle, i.e., in order to determine the optimal policy for method mj, optimal policies for methods mk ∈ {m| mj, m ∈ c≺} are used.
