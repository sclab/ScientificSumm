in simple multi-issue bargaining the equitable information revelation strategy generalises the tit-for-tat strategy in single-issue bargaining, and extends to a tit-for-tat argumentation  strategy by applying the same principle across the logic  framework. 
the notion of balance may be applied to pairs of  utterances by treating them as degenerate dialogues.
in particular, the intimacy determines values for the parameters g and h in equation 1. as a simple example, if both io α (ψ∗t ) and io β (ψ∗t ) increase then g decreases, and as the remaining eight information-based logic components increase, h increases.
dx is the reputation of agent x. the relationship balance between agents α and β is: b∗t αβ = v ∗t α v ∗t β .
if ψt terminates at time t: v ∗t+1 x = ν × vx(ψt ) + (1 − ν) × v ∗t x (5) where ν is the learning rate, and x = α, β. additionally, v ∗t x continually decays by: v ∗t+1 x = τ × v ∗t x + (1 − τ) × dx, where x = α, β; τ is the decay rate, and dx is a 2 × 5 array being the decay limit distribution for the value to agent x of the intimacy of the relationship in the absence of any interaction.
the intimacy between agents α and β, i∗t αβ, is the pattern of the two 2 × 5 arrays v ∗t α and v ∗t β that are computed by an update function as each negotiation round terminates, i∗t αβ = ` v ∗t α , v ∗t β ´ .
5.3 intimacy and balance the balance in a negotiation dialogue, ψt , is defined as: bαβ(ψt ) = vα(ψt ) vβ(ψt ) for an element-by-element  difference operator that respects the structure of v (ψt ).
on autonomous agents and multi-agent systems (aamas 07) 1035 with agent γ, n is the set of needs chosen from the  ontology at some suitable level of abstraction, tt is the set of offers on the table at time t, com(β, γ, b) means agent β has an outstanding commitment with agent γ to execute the commitment b where b is defined in the ontology at some suitable level of abstraction, b is the number of such commitments, and there are n + 1 agents in the system.
table 2 shows a sample measure for each of the ten categories, in it the dialogue commences at time s and terminates at time t. in that table, u(·) is a suitable utility evaluation function, needs(β, χ) means agent β needs the need χ, cho(β, χ, γ) means agent β satisfies need χ by choosing to negotiate the sixth intl.
joint conf.
we use the term 2 × 5 array loosely to describe vα in that the elements of the array are lists of measures that will be determined by the agent"s requirements.
attaching a utilitarian measure to this utterance may not be so simple.
for example, if α receives the utterance today is tuesday then this may be translated into a  constraint on a single distribution, and the resulting decrease in entropy is the information gain.
this is one reason why the  utilitarian approach has no natural extension to the management of argumentation that is achieved here by our  informationbased approach.
the utility-based valuations measure utility gain are expressed in terms of some suitable utility evaluation function u(·) that can be difficult to define.
in general terms, the information-based valuations  measure the reduction in uncertainty, or information gain, that the dialogue gives to each agent, they are expressed in terms of decrease in entropy that can always be calculated.
α estimates the value of this dialogue to β as vβ(ψt ) by  assuming that β"s reasoning apparatus mirrors its own.
let ψt = (φt , ms , e), then α estimates the value of this dialogue to itself in the context of ms and e as a 2 × 5 array vα(ψt ) where: vx(ψt ) = „ il x (ψt ) io x (ψt ) ig x (ψt ) ii x(ψt ) ic x (ψt ) ul x (ψt ) uo x (ψt ) ug x (ψt ) ui x(ψt ) uc x (ψt ) « where the i(·) and u(·) functions are information-based and utility-based measures respectively as we now describe.
, μn has been exchanged between agent α and agent β. this  negotiation dialogue is evaluated by α in the context of α"s world model at time s, ms , and the environment e that includes utterances that may have been received from other agents in the system including the information sources {θi}.
5.2 valuing negotiation dialogues suppose that a negotiation commences at time s, and by time t a string of utterances, φt = μ1, .
the extent of this calculation is controlled by the parameter η. an even tighter restriction may be obtained with: sim(ϕ , ϕ) ≥ η and ϕ ≤ ψ for some ψ.
4 may be approximated to: 1 − x ϕ :sim(ϕ ,ϕ)≥η pt η,i (ϕ |ϕ, e) log pt η,i (ϕ |ϕ, e) pt η(ϕ |ϕ) where pt η,i (ϕ |ϕ, e) is the normalisation of pt i (ϕ |ϕ, e) for sim(ϕ , ϕ) ≥ η, and similarly for pt η(ϕ |ϕ).
we obtain a more computationally friendly measure by appealing to the structure of the ontology described in section 3.2, and the right-hand side of eqn.
4 containsp ϕ that sums over all possible observations ϕ .
for example, eqn.
the various measures given above involve extensive calculations.
computational note.
if ϕ ≤ o let: φ+(ϕ, o, κ) =˘ ϕ | pt (prefer(ϕ , ϕ, o)) > κ ¯ for some constant κ, and: c(α, β, ϕ) = 1 + 1 b∗ · x ϕ ∈φ+(ϕ,o,κ) pt +(ϕ |ϕ) log pt +(ϕ |ϕ) where pt +(ϕ |ϕ) is the normalisation of pt (ϕ |ϕ) for ϕ ∈ φ+(ϕ, o, κ), b∗ = ( 1 if |φ+(ϕ, o, κ)| = 1 log |φ+(ϕ, o, κ)| otherwise as above we aggregate this measure for observations in a particular context o, and measure confidence as before.
here we measure the  consistency in expected acceptable observations, or the lack of expected uncertainty in those possible observations that are better than the original statement.
the previous measure requires that an ideal distribution, pt i (ϕ |ϕ, e), has to be specified for each ϕ. here we measure the extent to which the  observation ϕ is preferable to the original statement ϕ. given a predicate prefer(c1, c2, e) meaning that α prefers c1 to c2 in environment e. then if ϕ ≤ o: c(α, β, ϕ) = x ϕ pt (prefer(ϕ , ϕ, o))pt (ϕ |ϕ) and: c(α, β, o) = p ϕ:ϕ≤o pt β(ϕ)c(α, β, ϕ) p ϕ:ϕ≤o pt β(ϕ) certainty in observation.
this equation measures confidence for a single statement ϕ. it makes sense to aggregate these values over a class of statements, say over those ϕ that are in the ontological context o, that is ϕ ≤ o: c(α, β, o) = 1 − p ϕ:ϕ≤o pt β(ϕ) [1 − c(α, β, ϕ)] p ϕ:ϕ≤o pt β(ϕ) where pt β(ϕ) is a probability distribution over the space of statements that the next statement β will make to α is ϕ. similarly, for an overall estimate of β"s confidence in α: c(α, β) = 1 − x ϕ pt β(ϕ) [1 − c(α, β, ϕ)] preferred observations.
that is: c(α, β, ϕ) = 1 − x ϕ pt i (ϕ |ϕ, e) log pt i (ϕ |ϕ, e) pt(ϕ |ϕ) (4) where the 1 is an arbitrarily chosen constant being the maximum value that this measure may have.
here we measure the relative  entropy between this ideal distribution, pt i (ϕ |ϕ, e), and the distribution of expected observations, pt (ϕ |ϕ).
this  distribution will be a function of α"s context with β denoted by e, and is pt i (ϕ |ϕ, e).
consider a distribution of  observations that represent α"s ideal in the sense that it is the best that α could reasonably expect to observe.
ideal observations.
in section 5.2 confidence measures are applied to valuing  fulfilment of promises in the legitimacy category - we formerly called this honour [14], to the execution of commitments - we formerly called this trust [13], and to valuing  dialogues in the goals category - we formerly called this reliability [14].
5.1 confidence confidence measures generalise what are commonly called trust, reliability and reputation measures into a single  computational framework that spans the logic categories.
finally we define the intimacy of a relationship as an  aggregation of the value of its component dialogues.
second we evaluate each dialogue as it progresses in terms of the logic  framework - this evaluation employs the confidence measures.
on autonomous agents and multi-agent systems (aamas 07) subsequently occurs (the observation).
we first measure the confidence that an agent has for another by observing, for each utterance, the difference between what is said (the utterance) and what 1034 the sixth intl.
a relationship, ψ∗t , is a sequence of dialogues.
a dialogue, ψt , between agents α and β is a sequence of inter-related utterances in context.
