for this reason, standard correlation measures (such as the correlation  coefficient between the score and the judgment of a document), or order correlation measures (such as kendall tau between the score and judgment induced orders) are not adequate. 
a good  performance measure should correlate with user satisfaction,  taking into account that users will dislike having to delve deep in the results to find relevant documents.
evaluating the retrieval results from document scores and human judgments is not trivial and has been the subject of many investigations in the ir community.
since content-match features are very unreliable (and even more so link features, as we will see) we need to ask a human to evaluate the results in order to compare the quality of features.
this is so  because no document features have been found yet that can effectively estimate the relevance of a document to a user query.
involving a human in the evaluation process is extremely cumbersome and expensive; however, human judgments are crucial for the evaluation of search engines.
results were selected for judgment based on their commercial search engine placement; in other words, the subset of  labeled results is not random, but biased towards documents considered relevant by pre-existing ranking algorithms.
485,656 of the results in the query set (about 0.73% of all results, or about 17.3 results per query) were rated by human judges as to their relevance to the given query, and labeled on a six-point scale (the labels being definitive, excellent, good, fair, bad and detrimental).
in fact, only 9,525,566 of the result urls (about 14.25%) were covered by the graph.
it is important to point out that our 2.9 billion url web graph does not cover all these result urls.
our query set was produced by sampling 28,043 queries from the msn search query log, and retrieving a total of 66,846,214 result urls for these queries (using commercial search engine technology), or about 2,838 results per query on average.
as we will see, this property affects the computational cost of hits.
the mean out-degree of crawled web pages is 38.11; the mean in-degree of discovered pages (whether crawled or not) is 6.10. also, it is worth pointing out that there is a lot more variance in in-degrees than in out-degrees; some popular pages have millions of incoming links.
thus, at the end of the crawl there were 2,433,985,395 urls in the frontier set of the crawler that had been discovered, but not yet  downloaded.
these pages contain 17,672,011,890 hyperlinks (after eliminating duplicate  hyperlinks embedded in the same web page), which refer to a total of 2,897,671,002 urls.
our web graph is based on a web crawl that was  conducted in a breadth-first-search fashion, and successfully retrieved 463,685,607 html pages.
our evaluation is based on two data sets: a large web graph and a substantial set of queries with associated results, some of which were labeled by human judges.
