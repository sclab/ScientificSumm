oh et al [16] use the naive bayesian frame to combine link information with content. 
[11] combine text kernels and co-citation kernels for  classification.
joachims et al.
[3] use co-citation information in their classification model.
chakrabarti et al.
the experiments of [21] show that using terms from the linked  document improves the classification accuracy.
the following are some work on combining documents and links, but the methods are loosely related to our approach.
on the other hand, the inference for rmns is intractable and requires belief propagation.
however the model does not give an off-the-shelf  solution, because the success highly depends on the arts of designing the potential functions.
rmns apply  conditional random fields to define a set of potential functions on cliques of random variables, where the link structure provides hints to form the cliques.
the model was applied to web page classification, where web pages are  entities and hyperlinks are treated as relationships.
[19] propose relational markov networks (rmns) for entity classification, by describing a conditional distribution of entity classes given entity attributes and relationships.
taskar et al.
as the result of matrix factorization, the factor forms a factor graph, a miniature of the original graph, preserving the major structure of the original graph.
in turn, such information is passed to the factor of the source web page.
the factor of the destination web page contains information of its  outgoing links.
in our approach, the link is constructed with the linkage between the factor of the source web page and the factor of the  destination web page, instead of the destination web page itself.
in other words, the overall link structure is not utilized in phits.
in the model, the outgoing links of the destination web page have no effect on the source web page.
in plsi+phits, the link is constructed with the linkage from the topic of the source web page to the destination web page.
the major difference between the approach of [6] (plsi+phits) and our approach is in the part of link analysis.
cohn and hofmann [6] construct the latent space from both  content and link information, using content analysis based on  probabilistic lsi (plsi) [10] and link analysis based on phits [5].
our  approach consider the content and the directed linkage between topics of source and destination web pages in one step, which implies the topic combines the information of web page as authorities and as hubs in a single set of factors.
but it is difficult to combine the content information into that framework.
the framework combines the hub and authority information of web pages.
[25] and [24] propose a directed graph regularization framework for semi-supervised learning.
zhou et al.
achlioptas et al[2] decompose the web into hub and authority attributes then combine them with content.
[23] uses the undirected graph regularization framework for document classification.
zhang et al.
the content information is only used for weighing the links by the textual similarity of both ends of the links.
in the algorithm, all links are treated as undirected edge of the link graph.
the algorithm incorporates link structure and the co-citation patterns.
[9] propose a clustering algorithm for web document  clustering.
he et al.
instead of using two categories, pagerank [17] uses a single category for the recursive notion, an authority is a web page with many incoming links from authorities.
using recursive notion, a hub is a web page with many outgoing links to authorities, while an authority is a web page with many incoming links from hubs.
in the link analysis approach, the framework of hubs and  authorities (hits) [12] puts web page into two categories, hubs and authorities.
though our approach also uses latent space to  represent web pages (documents), we consider the link structure as well as the content of web pages.
the commonly used singular value decomposition (svd) method ensures that the data points in the latent space can optimally reconstruct the original documents.
analysis tasks, such as  classification, could be performed on the latent space.
the similarity between documents could be defined by the dot products of the corresponding vectors of documents in the latent space.
the latent space implicitly  captures a large portion of information of documents, therefore it is called the latent semantic space.
lsi maps documents into a lower dimensional latent space.
in the content analysis part, our approach is closely related to latent semantic indexing (lsi) [8].
