(9), we can obtain more interpretable results, which could be used for clustering. 
(9) solving eq.
(4) becomes min u,v,z j (u, v, z), s.t.z ≥ 0.
then the problem of eq.
the dimensionality of the latent space is set to the number of clusters we want.
as the example with nonnegative constraints shown in section 3, we represent each cluster by a latent topic, i.e.
actually, we can use the idea of nonnegative matrix factorization for clustering [20] to directly cluster web pages.
when we come to the clustering application, we can use this model to find z, then apply k-means to partition the web pages into clusters.
therefore, we allow the components take any possible real  values.
the values of the entries are the weights of linear model, instead of the probabilities of web pages belonging to latent  topics.
a entry of matrix z means the relationship of a web page and a factor.
in our paper, we mainly discuss the application of classification.
la(u, z) = µh(a, zuz ), where h(a, b) =p i,j [1 − aijbij]+ .
therefore, we can apply other types of loss, such as hinge loss or smoothed hinge loss, e.g.
actually, squared loss does not precisely describe the underlying noise model, because the weights of adjacency matrix can only take nonnegative  values, in our case, zero or one only, and the components of  content matrix c can only take nonnegative integers.
(3) use squared loss due to computationally convenience.
(2) and lc in eq.
the loss functions la in eq.
