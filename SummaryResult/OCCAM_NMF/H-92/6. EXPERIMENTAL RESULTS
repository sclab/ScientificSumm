the improvements observed over large test sets of queries (1,000 total, between 466 and 495 with implicit feedback available) are both substantial and statistically significant. 
furthermore, incorporating implicit feedback features directly into the learned ranking function is more effective than using implicit feedback for reranking.
our rich set of implicit features, such as time on page and deviations from the average behavior, provides advantages over using clickthrough alone as an indicator of interest.
0 50 100 150 200 250 300 350 0.1 0.2 0.3 0.4 0.5 0.6 -0.4 -0.35 -0.3 -0.25 -0.2 -0.15 -0.1 -0.05 0 0.05 0.1 0.15 0.2 frequency average gain figure 6.8: gain of bm25f+all over original bm25f ranking to summarize our experimental results, incorporating implicit feedback in real web search setting resulted in significant improvements over the original rankings, using both bm25f and rn baselines.
one possible explanation is that these easy queries tend to be navigational (i.e., having a single, highly-ranked most appropriate answer), and user interactions with lower-ranked results may indicate divergent information needs that are better served by the less popular results (with correspondingly poor overall relevance ratings).
interestingly, incorporating user behavior information degrades accuracy for queries with high original map score.
figure 6.8 reports the map improvements over the baseline bm25f run for each query with map under 0.6. note that most of the improvement is for poorly performing queries (i.e., map < 0.1).
method map gain p(1) gain rn 0.269 0.632 rn+all 0.321 0.051 (19%) 0.693 0.061(10%) bm25f 0.236 0.525 bm25f+all 0.292 0.056 (24%) 0.687 0.162 (31%) table 6.2: mean average precision (map) on attempted queries for best performing methods we now analyze the cases where implicit feedback was shown most helpful.
0.45 0.5 0.55 0.6 0.65 0.7 1 3 5 10k precision rn rn+all bm25 bm25+all figure 6.7: precision at k ndcg at k for bm25f, bm25f+all, rn, and rn+all on test queries with user interactions we summarize the results on the map measure for attempted queries in table 6.2. map improvements are both substantial and significant, with improvements over the bm25f ranker most pronounced.
when implicit feedback is available, the bm25f+all system returns relevant document at top 1 almost 70% of the time, compared 53% of the time when implicit feedback is not considered by the original bm25f system.
0.6 0.65 0.7 0.75 0.8 1 3 5 10k ndcg rn rn+all bm25 bm25+all figure 6.6: ndcg at k for bm25f, bm25f+all, rn, and rn+all on test queries with user interactions similarly, gains on precision at top 1 are substantial (figure 6.7), and are likely to be apparent to web search users.
the ndcg at 1 of bm25f+all increases from 0.6 to 0.75 (a 31% relative gain), achieving performance comparable to rn+all operating over a much richer feature set.
the gains at top 1 are dramatic.
figure 6.6 reports ndcg for the subset of the test queries with the implicit feedback features.
we now consider the performance on the queries for which user interactions were available.
this is not surprising: web search is heavy-tailed, and there are many unique queries.
out of the 1000 queries in test, between 46% and 49%, depending on the train-test split, had sufficient interaction information to make predictions (i.e., there was at least 1 search session in which at least 1 result url was clicked on by the user).
unfortunately, less than half had sufficient interactions to attempt reranking.
so far we reported results averaged across all queries in the test set.
map gain p(1) gain bm25f 0.184 -  0.503bm25f-rerank-ct 0.215 0.031* 0.577 0.073* bm25f-rerankimplicit 0.218 0.003 0.605 0.028* bm25f+implicit 0.222 0.004 0.620 0.015* rn 0.215 -  0.597rn+all 0.248 0.033* 0.629 0.032* table 6.1: mean average precision (map) for all strategies.
the gains marked with * are significant at p=0.01 level using two tailed t-test.
while not intuitive to interpret, map allows quantitative comparison on a single metric.
0.4 0.45 0.5 0.55 0.6 0.65 1 3 5 10 k precision rn rn+all bm25 bm25+all figure 6.5: precision at k for bm25f, bm25f+all, rn, and rn+all for varying k we summarize the performance of the different ranking methods in table 6.1. we report the mean average precision (map) score for each system.
this demonstrates the complementary nature of implicit feedback with other features available to a state of the art web search engine.
0.5 0.52 0.54 0.56 0.58 0.6 0.62 0.64 0.66 0.68 0.7 1 2 3 4 5 6 7 8 9 10k ndcg rn rn+all bm25 bm25+all figure 6.4: ndcg at k for bm25f, bm25f+all, rn, and rn+all for varying k furthermore, enriching the rn features with implicit feedback set exhibits significant gain on all measures, allowing rn+all to outperform all other methods.
in other words, implicit feedback incorporates sufficient information to replace the hundreds of other features available to the ranknet learner trained on the rn feature set.
figure 6.4 reports ndcg at k and figure 6.5 reports precision at k. interestingly, while the original rn rankings are significantly more accurate than bm25f alone, incorporating implicit feedback features (bm25f+all) results in ranking that significantly outperforms the original rn rankings.
rn incorporates bm25f, link-based features, and hundreds of other features.
we now consider incorporating user behavior into a much richer feature set, rn (section 5.3) used by a major web search engine.
for example, for queries with first relevant result at position 5 (ptr=5), there are more clicks on the non-relevant results in higher ranked positions than on the first relevant result at position 5. as we will see, learning over a richer behavior feature set, results in substantial accuracy improvement over clickthrough alone.
figure 6.3 shows relative clickthrough frequencies for queries with known relevant items at positions other than the first position; the position of the top relevant result (ptr) ranges from 2-10 in the figure.
users often click on results above the relevant one presumably because the short summaries do not provide enough information to make accurate relevance assessments and they have learned that on average  topranked items are relevant.
unfortunately, as joachims and others have shown, presentation also influences which results users click on quite dramatically.
if users considered only the relevance of a result to their query, they would click on the topmost relevant results.
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 2 3 5 result position relativeclickfrequency ptr=2 ptr=3 ptr=5 figure 6.3: relative clickthrough frequency for queries with varying position of top relevant result (ptr).
0.5 0.52 0.54 0.56 0.58 0.6 0.62 0.64 0.66 0.68 1 2 3 4 5 6 7 8 9 10k ndcg bm25 bm25-rerank-ct bm25-rerank-all bm25+all figure 6.1: ndcg at k for bm25f, bm25f-rerankct, bm25f-rerank-all, and bm25f+all for varying k 0.35 0.4 0.45 0.5 0.55 0.6 0.65 1 3 5 10 k precision bm25 bm25-rerank-ct bm25-rerank-all bm25+all figure 6.2: precision at k for bm25f, bm25f-rerankct, bm25f-rerank-all, and bm25f+all for varying k interestingly, using clickthrough alone, while giving significant benefit over the original bm25f ranking, is not as effective as considering the full set of features in table 4.1. while we analyze user behavior (and most effective component features) in a separate paper [1], it is worthwhile to give a concrete example of the kind of noise inherent in real user feedback in web search setting.
the improvement is consistent across the top 10 results and largest for the top result: ndcg at 1 for bm25f+all is 0.622 compared to 0.518 of the original results, and precision at 1 similarly increases from 0.5 to 0.63. based on these results we will use the direct feature combination (i.e., bm25f+all) ranker for subsequent comparisons involving implicit feedback.
incorporating all user feedback (either in reranking framework or as features to the learner directly) results in significant improvements (using two-tailed t-test with p=0.01) over both the original bm25f ranking as well as over reranking with clickthrough alone.
figures 6.1 and 6.2 report ndcg and precision for bm25f, as well as for the strategies reranking results with user feedback (section 3.1).
we first experimented with different methods of re-ranking the output of the bm25f search results.
we then drill down to examine the effects on reranking for the attempted queries in more detail, analyzing where implicit feedback proved most beneficial.
we first present the results over all 1000 test queries (i.e., including queries for which there are no implicit measures so we use the original web rankings).
each split contained 1500 training, 500 validation, and 1000 test queries, all query sets disjoint.
we compare our methods over strong baselines (bm25f and rn) over the ndcg, precision at k, and map measures defined in section 5.2. the results were averaged over three random splits of the overall dataset.
we compare alternative methods of exploiting implicit feedback, both by re-ranking the top results (i.e., the bm25f-rerankct and bm25f-rerankall methods that reorder bm25f results), as well as by integrating the implicit features directly into the ranking process (i.e., the rn+all and bm25f+all methods which learn to rank results over the implicit feedback and other features).
implicit feedback for web search ranking can be exploited in a number of ways.
