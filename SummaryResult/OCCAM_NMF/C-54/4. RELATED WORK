finally, none of the web caching methods address the symmetric issue of large data uploads. 
in addition, in our case, the user community that accesses a certain data file may also be very dispersed from a network point of view and thus cannot take advantage of any of the caching schemes.
hence, these policies would lead to the immediate  replacement of our relatively large data files even though they may be used frequently.
most of the web caching schemes that are in use today employ a replacement policy that gives a  priority to replacing the largest sized items over smaller-sized ones.
cache misses are especially common in the type of large data-based services on which we are working.
hence, the point where the server is reached can be used to take a central decision but then the actual service request can be forwarded to a set of active clients, i.e., the  down8 load and upload operations.
we try to address the situation where a request arrives at a server, meaning all the caches report a miss.
in fact, appoint can work in  tandem with collaborative web caching if they are deployed together.
we do not compete with these approaches.
collaborative web caching systems, the most relevant of these for our research, focus on creating  either a hierarchical, hash-based, central directory-based, or multicast-based caching schemes.
except for this  recent peer-to-peer approach, web caching is mostly a  wellstudied topic in the realm of server/proxy level caching [8, 11, 14, 17].
it creates a pure  peer-topeer collaborative web cache among the web browser caches of the machines in a local-area network.
squirrel [13] forms the middle ground.
from our perspective, although appoint employs some of the techniques used in peer-to-peer systems, it is also closely related to current web caching architectures.
we want to expand our  research, in the future, to address this issue.
confirming the authenticity of the indirectly delivered data sets is not yet addressed with appoint.
hence, other issues like anonymity, decentralization, and persistence of storage were less important in our decisions.
with appoint, we want to improve  existing client-server systems in terms of performance by using idle networking resources among active clients.
our goal is different than these approaches.
also, other  approaches, like seti@home [21], made other resources, such as idle cpus, work together over the internet to solve large scale computational problems.
some of these systems have focused on anonymity while  others have focused on persistence of storage.
past [18], eternity service [7], cfs [10], and oceanstore [15] are some peer-to-peer storage systems.
many peer-to-peer storage systems have also recently emerged.
other systems followed these popular systems, each addressing a different flavor of sharing over the internet.
unfortunately, it suffers from scalability issues, i.e., floods of messages between peers in order to map connectivity in the system are required.
gnutella is a pure (decentralized) peer-to-peer file exchange system.
our application domain, where the data is already freely available to the public, forms a prime candidate for such a peer-to-peer approach.
for our peer-to-peer transfer approach (appoint),  napster is the forefather where a directory service is centralized on a server and users exchange music files that they have stored on their local disks.
this work blends raster data management (stored in pyramids [22]) with vector data stored in quadtrees [19, 20].
it is assumed that this data server manages vast databases that are impractical to be stored on individual clients.
other related work has been reported in [16] where a client-server architecture is described that is designed to  provide end users with access to a server.
this also allows dropping unnecessary parts of the image from the main memory on the server.
both the client and the server keep a common mask that indicates what parts of the image are available on the client and what needs to be requested.
on the client side, the image is  reconstructed into a pyramid representation to speed up zooming and panning operations.
in this case, while the server holds the full representation of the large  image, only a limited amount of data needs to be transferred to the client to enable it to display a currently requested view into the image.
it presents a technique based on wavelet transformations that allows the  minimization of the amount of data needed to be transferred over the network between the server and the client.
work described in [9] examines a client-server  architecture for viewing large images that operates over a  lowbandwidth network connection.
although the advantage of this solution is the minimization of both  hardware and software resources on the client site, the resulting product has severe limitations in terms of available  functionality and response time (each user action results in a new bitmap being transferred to the client).
the solution presented by most of these vendors is based on performing all the calculations on the server side and transferring only bitmaps that  represent results of user queries and commands.
the goal in this  approach is to enable remote users, typically only equipped with standard web browsers, to access the company"s  spatial database server and retrieve information in the form of pictorial maps from them.
one specific approach has been adopted by numerous web-based mapping services (mapquest [5], mapsonus [6], etc.).
there has been a substantial amount of research on  remote access to spatial data.
