proof: since the distance between any two nodes in hypercube equals to the hamming distance between them, so it is obvious that the theorem 2"s conclusion stands from definition 5. 
proof: since the diameter of n-dimensional hypercube is n, and the diameter of m-dimensional hypercube is m, so it is easy to know that the diameter of h(k,u,m,v,n) model is m+n from the definition 5. property 2: the distance between any two nodes a(i1, j1) and b(i2, j2) in h(k,u,m,v,n) model is d(a,b)= dh(i1, i2)+dh(j1, j2), where dh represents the hamming distance.
3) any node a in h(k,u,m,v,n) can be encoded as (i, j), where i(i=i1i2…in, 0 ≤ i1,i2,…in ≤ v-1) is the  in-cluster-hypercubenode-code of node a, and j(j=j1j2…jm, 0 ≤ j1,j2,…jm ≤ u-1) is the out-cluster-hypercube-node-code of node a. obviously, the h(k,u,m,v,n) model has the following good properties: property 1: the diameter of h(k,u,m,v,n) model is m+n.
(the graph constructed through above steps is called a k-levels hierarchical hypercube abbreviated as h(k,u,m,v,n).)
and the nodes in the k different hypercubes are connected into m-dimensional hypercubes according to the following rules: the nodes with same in-cluster-hypercube-node-codes and different  out-clusterhypercube-node-codes are connected into an m-dimensional hypercube.
2) the k different hypercubes obtained above are encoded as j1j2…jm, which are called out-cluster-hypercube-node-codes, where 0 ≤ j1,j2,…jm ≤ u-1,u=[ m k ].
definition 5 (k-levels hierarchical hypercube): let there are n nodes totally, then a k-levels hierarchical hypercube named h(k,u,m,v,n) can be constructed as follows: 1) the n nodes are divided into k clusters averagely, and the [n/k] nodes in any cluster are connected into an n-dimensional hypercube: in the n-dimensional hypercube, any node is encoded 55 as i1i2…in, which are called in-cluster-hypercube-node-codes, where 0 ≤ i1,i2,…in ≤ v-1,v=[ n kn / ],[j] equals to an integer not less than j. so we can obtain k such kind of different hypercubes.
