their work  motivates the need to rely on the sizes of agents" valuation functions in stating worst-case results. 
our work also addresses the issue of communication complexity, and we are able to derive  algorithms that provide significant communication guarantees despite nisan and segal"s negative results.
this is in fact the basic  framework of learning theory.
in this model algorithms are allowed to ask questions about agent valuations and receive honest  responses, without any insight into how the agents internally compute their valuations.
their results apply to the black-box model of  computational complexity.
they show that for many rich classes of valuations, the worst-case communication  complexity of computing an optimal allocation is exponential.
nisan and segal [12] study the communication  complexity of preference elicitation.
we will show that demand queries can simulate equivalence queries until we have enough information about valuations to imply a  solution to the elicitation problem.
in contrast, our work shows that given a more general (yet still quite standard) version of demand query than the type they consider, the complexity of preference elicitation is no greater than the complexity of learning.
they show that certain classes of functions can be efficiently learned yet not efficiently elicited, and vice-versa.
they consider models with membership and equivalence queries in query learning, and value and demand queries in preference  elicitation.
[5] provide results relating the complexities of query learning and preference elicitation.
blum et al.
because we allow ourselves demand queries, we are able to derive an elicitation scheme for general valuation functions.
their work only makes use of value queries, which are quite limited in power.
since their work is also grounded in learning theory, they allow dependence on the size of the target  valuation as we do (though read-once valuations can always be succinctly represented anyway).
read-once formulas can represent certain substitutabilities, but no complementarities, whereas the opposite holds for toolbox dnf.
[19] consider the problem of learning restricted classes of valuation functions which can be represented using read-once formulas and toolbox dnf.
zinkevich et al.
related work.
our focus is on the time and communication complexity of preference elicitation  regardless of incentive constraints, and on the relationship between the complexities of learning and preference elicitation.
180 for now, we leave the issue of incentives aside when  deriving elicitation algorithms.
also, with our scheme entire revelation only happens in the worst-case.
we expect this to be an important consideration in practice.
there is no burden on the agents to formulate their  valuations in an encoding scheme of the auctioneer"s choosing.
the advantage of our scheme is that agents can be viewed as black-boxes that provide incremental information about their valuations.
of course, a one-shot combinatorial auction where agents provide their entire valuation functions at once would also have polynomial communication in the size of the agents" valuations, and only require one query.
finally, we use our conversion procedure to generate combinatorial auction protocols from learning algorithms for polynomials, monotone dnf, and linear-threshold functions.
introducing this parameter also allows us to guarantee polynomial worst-case  communication, which usually cannot be achieved in the number of goods and agents alone.
we argue that complexity guarantees for elicitation schemes should allow dependence on this parameter.
preference elicitation schemes have not  traditionally considered this last parameter.
here we mean polynomial in the number of goods, agents, and the sizes of the agents" valuation functions in a given  encoding scheme.
the resulting elicitation algorithm guarantees elicitation in a polynomial number of value and demand queries.
we show that any exact learning algorithm with  membership and equivalence queries can be converted into a  preference elicitation algorithm with value and demand queries.
though the goals of learning and preference  elicitation differ somewhat, it is clear that these problems share similar structure, and it should come as no surprise that techniques from one field should be relevant to the other.
in  preference elicitation, the goal is to elicit enough partial  information about preferences to be able to compute an optimal allocation.
in learning theory, the goal is to learn a function via various types of queries, such as what is the function"s value on these inputs?
there has been recent work exploring the links between the preference elicitation problem in combinatorial auctions and the problem of learning an unknown function from  computational learning theory [5, 19].
these considerations motivate the need for auction protocols that minimize the communication and information revelation required to determine an optimal  allocation of goods.
even if agents can efficiently compute their valuations, they might still be reluctant to reveal them entirely in the course of an auction, because such information may be valuable to their competitors.
it is in the interest of such agents to have auction protocols which require them to bid on as few bundles as possible.
furthermore, it might even be hard for agents to determine their valuations for single bundles [14].
communicating valuations in a one-shot fashion can be prohibitively expensive if the number of goods is only moderately large.
since there are an exponential number of bundles (in the number of goods), communicating values over these bundles can be  problematic.
in a combinatorial auction, agents may bid on bundles of goods rather than individual goods alone.
