in these applications, it is used  primarily as an identification tool and ranking criterion. 
typical use of this non-parametric tool includes  detection of rare events in time series (e.g., a hard drive failure  prediction [17]) and bioinformatics applications (e.g., finding informative genes from microarray data).
to our knowledge, no attempt has been made yet to  extend its properties as an infrastructure for determining with whom and to what extent information should be shared, as presented in this paper.
while the wilcoxon test is a widely used statistical procedure [22, 14], it is usually used for comparing two sets of single-variate data.
on autonomous agents and multi-agent systems (aamas 07) them, while our wilcoxon-based approach will give them the  highest rank in terms of similarity.
joint conf.
while these two functions are associated with  almost identical reservation values (for any sampling cost) and mean, the kullback-leibler method will assign a poor correlation between 208 the sixth intl.
for example, consider the two functions f(x) = ( x mod2)/100 and g(x) = ( x mod2)/100 defined over the  interval (0, 200).
however, the method will perform poorly in scenarios in which the functions  alternate between different levels while keeping the general  structure and moments.
this measure, which can also be applied on continuous random variables, relies on a natural distance measure from a true probability distribution (either observation-based or calculated) to an arbitrary probability distribution.
the most relevant method for our purposes is the kullback-leibler relative entropy index that is used in probability theory and  information theory [12].
similarly, clustering algorithms  designed for the task of class identification in spatial databases (e.g., relying on a density-based notion [4]) are not useful for our case, because our data has no spatial attributes.
however the  applicability of these to fast-paced domains is quite limited because they rely on a large set of existing data.
many clustering techniques have been used in data mining [2], with particular focus on incremental updates of the clustering, due to the very large size of the databases [3].
an  additional difficulty is defining the distance measure.
of particular importance is that the ca needs to find similarity between functions, defined over a continuous interval, with no distinct pre-defined attributes.
in spite of the richness of available clustering algorithms (such as the famous k-means clustering algorithm [11], hierarchical methods, bayesian classifiers [6], and maximum entropy), various characteristics of fast-paced domains do not align well with the features of  attributesbased clustering mechanisms, suggesting these mechanisms would not perform well in such domains.
given space considerations, our review of this area is  restricted to some representative approaches for clustering.
this capability is closely related to clustering and classification, an area widely studied in machine learning.
selective-sharing relies on the ability to find similarity between specific parts of the probability distribution function associated with a characteristic of different users.
however, collaborative filtering systems exhibit poor performance when there is not sufficient information about the users and when there is not sufficient information about a new user whose taste the system attempts to predict [7].
collaborative filtering, which makes predictions (filtering) about the interests of a user [7], operates similarly to selective-sharing.
in addition to the interruption management literature reviewed in section 2, several other areas of prior work are relevant to the selective-sharing mechanism described in this paper.
