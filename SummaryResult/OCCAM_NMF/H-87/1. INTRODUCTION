section 6 concludes the main findings in this study. 
section 5 reports the experiments and results.
section 4 outlines the rocchio and lr approaches to af, respectively.
section 3 analyzes the differences among the trec and tdt metrics (utilities and tracking cost) and the potential implications of those differences.
the organization of the paper is as follows: section 2 introduces the four benchmark corpora (trec10 and trec11, tdt3 and tdt5) used in this study.
specifically, we focus on how much the performance of these methods depends on parameter tuning, what the most influential parameters are in these methods, how difficult (or how easy) to optimize these influential parameters using cross-corpus validation, how strong these methods perform on multiple benchmarks with the systematic tuning of parameters on other corpora, and how efficient these methods are in running af on large benchmark corpora.
stimulated by those findings, we decided to include rocchio and lr in our  crossbenchmark evaluation for robustness testing.
furthermore, a recent paper [13] reported that the joint use of rocchio and lr in a bayesian framework outperformed the results of using each method alone on the trec11 corpus.
it was recently evaluated in adaptive filtering and was found to have relatively strong performance (section 5.1).
logistic regression is a classical method in statistical learning, and one of the best in batch-mode text categorization [15][14].
rocchio-style classifiers have been popular in af, with good performance in benchmark evaluations (trec and tdt) if appropriate parameters were used and if combined with an effective threshold calibration strategy [2][4][7][8][9][11][13].
in this paper we address the above question by conducting a cross-benchmark evaluation with two effective approaches in af: incremental rocchio and regularized logistic regression (lr).
current literature does not offer an answer because no thorough investigation on the robustness of af methods has been reported.
now the important question is: which methods (if any) are robust under the condition of using cross-corpus validation to tune parameters?
notice that the validation-set topics often do not overlap with the test-set topics, thus the parameter optimization is performed under the tough condition that the validation data and the test data may be quite different from each other.
this leaves only one option (assuming tuning on the test set is not an alternative), that is, choosing an external corpus as the validation set.
in tdt5, for example, there is only one labeled training example per topic at the start; parameter optimization on such training data is doomed to be ineffective.
available training examples, on the other hand, are often insufficient for tuning the parameters.
most af methods have pre-specified parameters that may influence the performance significantly and that must be determined before the test process starts.
by robust we mean consistent and strong performance across benchmark corpora with a systematic method for parameter tuning across multiple corpora.
we argue that robustness is an important measure for evaluating and comparing af methods.
addressing the third issue is the main focus in this paper.
although these works provide valuable insights for understanding the problems and possible solutions, it is difficult to draw conclusions regarding the effectiveness and robustness of current methods because the third problem has not been thoroughly investigated.
the first two problems have been studied in the adaptive filtering literature, including topic profile adaptation using incremental rocchio, gaussian-exponential density models, logistic regression in a bayesian framework, etc., and threshold optimization strategies using probabilistic calibration or local fitting techniques [1][2][9][10][11][12][13].
none of these problems is addressed in the literature of statistical learning for batch classification where all the training data are given at once.
these conditions make adaptive filtering a difficult task in statistical learning (online classification), for the following reasons: 1) it is difficult to learn accurate models for prediction based on extremely sparse training data; 2) it is not obvious how to correct the sampling bias (i.e., relevance feedback on system-accepted documents only) during the adaptation process; 3) it is not well understood how to effectively tune parameters in af methods using cross-corpus validation where the validation and evaluation topics do not overlap, and the documents may be from different sources or different epochs.
furthermore, topics of interest might change over time, with new topics appearing and growing, and old topics shrinking and diminishing.
that is, the user would be willing to provide a few positive examples for each topic of interest at the start, and might or might not be able to provide additional labeling on a small portion of incoming documents through relevance feedback.
the above conditions attempt to mimic realistic situations where an af system would be used.
• tdt2004 was the first time that trec and tdt metrics were jointly used in evaluating af methods on the same benchmark (the tdt5 corpus) where non-stationary topics dominate.
• relevance feedback (rf) was not allowed in the tdt evaluations for af (or topic tracking in the tdt terminology) until 2004.
• relevance feedback was available but only for the  systemaccepted documents (with a yes decision) in the trec evaluations for af.
starting from 1997 in the topic detection and tracking (tdt) area and 1998 in the text retrieval conferences (trec), benchmark evaluations have been conducted by nist under the following conditions[6][7][8][3][4]: • a very small number (1 to 4) of positive training examples was provided for each topic at the starting point.
the task is for the system to make an online topic membership decision (yes or no) for every document, as soon as it arrives, with respect to each pre-defined topic of interest.
adaptive filtering (af) has been a challenging research topic in information retrieval.
