the solution of the modified objective function is called the maximum a posteriori (map) estimate, which reduces to the maximum likelihood solution for standard lr if 0=λ . 
how to find an effective μ r is an open issue for research, depending on the user"s belief about the parameter space and the optimal range.
tuning λ (≥0) is theoretically justified for reducing model complexity (the effective degree of freedom) and avoiding over-fitting on training data [5].
the second term in the objective function is for regularization, equivalent to adding a gaussian prior to the regression coefficients with mean μ r and covariance variance matrix ι⋅λ2/1 , where ι is the identity matrix.
we modified the standard (above) version of lr to allow more flexible optimization criteria as follows: ⎭ ⎬ ⎫ ⎩ ⎨ ⎧ −++= ∑= ⋅− 2 1 )1log()(minarg μλ rrr rr r weysw n i xwy i w map ii where )( iys is taken to be α , β and γ for query, positive and negative documents respectively, which are similar to those in rocchio, giving different weights to the three kinds of training examples: topic descriptions (queries), on-topic documents and off-topic documents.
if t11su is the metric, for example, with the penalty ratio of 2:1 for misses and false alarms (section 3.1), the optimal threshold for lr is 33.0)12/(1 =+ for all topics.
once the regression coefficients are optimized on the training data, the filtering prediction on each incoming document is made as: ( ) ⎩ ⎨ ⎧ − + =− )( )( ),|( no yes wxypsign optnew θ rr note that w r is constantly updated whenever a new relevance judgment is available in the testing phase of af, while the optimal threshold optθ is constant, depending only on the  predefined utility (or cost) function for evaluation.
given a training set of labeled documents { }),(,),,( 11 nn yxyxd r l r = , the standard regression problem is defined as to find the maximum likelihood estimates of the regression coefficients (the model parameters): { } { } { }))exp(1(1logminarg )|(logmaxarg)|(maxarg ii xwyn i w wdp w wdp w mlw rr r r r r r r ⋅−+∑ == == this is a convex optimization problem which can be solved using a standard conjugate gradient algorithm in o(inf) time for training per topic, where i is the average number of iterations needed for convergence, and n and f are the number of training documents and number of features respectively [14].
4.2 logistic regression for af logistic regression (lr) estimates the posterior probability of a topic given a document using a sigmoid function )1/(1),|1( xw ewxyp rrrr ⋅− +== where x r is the document vector whose elements are term weights, w r is the vector of regression coefficients, and }1,1{ −+∈y is the output variable corresponding to yes or no with respect to a particular topic.
results of more complex variants of rocchio are also discussed when relevant.
this simple version of rocchio has been commonly used in the past tdt benchmark evaluations for topic tracking, and had strong performance in the tdt2004 evaluations for adaptive filtering with and without relevance feedback (section 5.1).
hence, we decided to use a relatively simple version of rocchio as the baseline, i.e., with a universal threshold tuned on a validation corpus and fixed for all topics in the testing phase.
instead, our focus here is to investigate the robustness of rocchio-style methods in terms of how much their performance depends on elaborate system tuning, and how difficult (or how easy) it is to get good performance through cross-corpus parameter optimization.
it is beyond the scope of this paper to compare all the different ways to adapt rocchio-style methods for af.
more elaborate methods include probabilistic threshold calibration which converts the non-probabilistic similarity scores to probabilities (i.e., )|( dtp r ) for utility optimization [9][13], and margin-based local regression for risk reduction [11].
the simplest is to use a universal threshold for all topics, tuned on a validation set and fixed during the testing phase.
multiple approaches have been developed.
the predictions on a new document are made by computing the cosine similarity between each topic prototype and the document vector, and then comparing the resulting scores against a threshold: ⎩ ⎨ ⎧ − + =− )( )( ))),((cos( no yes dtpsign new θ rr threshold calibration in incremental rocchio is a challenging research topic.
to distinguish the two, we call the first case simply rocchio and the second case prf rocchio where prf stands for  pseudorelevance feedback.
both cases are part of our experiments in this paper (and part of the tdt 2004 evaluations for af).
if relevance feedback is available (as is the case in trec adaptive filtering), the new document is added to the pool of either )(td+ or )(td− , and the prototype is recomputed accordingly; if relevance feedback is not available (as is the case in tdt event tracking), the system"s prediction (yes) is treated as the truth, and the new document is added to )(td+ for updating the prototype.
the prototype of a topic is updated each time the system makes a yes decision on a new document for that topic.
the three terms are given pre-specified weights of βα, and γ , controlling the relative influence of these components in the prototype.
the third term is the weighted centroid of the set )(td− of negative training examples which are the nearest neighbors of the positive centroid.
the second term is the weighted centroid of the set )(td+ of positive training examples, each of which is a vector of  withindocument term weights.
4.1 incremental rocchio for af we employed a common version of rocchio-style classifiers which computes a prototype vector per topic (t) as follows: |)(| ' |)(| )()( )(')( td d td d tqtp tddtdd − ∈ + ∈ ∑∑ −+ −+= rr rr rr γβα the first term on the rhs is the weighted vector representation of topic description whose elements are terms weights.
