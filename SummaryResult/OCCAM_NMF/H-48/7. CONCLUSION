an ir system that is easy to use should be good at retrieving documents that are relevant to users"  information needs, even if the queries provided by the users do not contain the same keywords as the relevant documents. 
it should also be noted that this framework is  generalizable to any ir system, in that it evaluates how well ir systems evaluate users" information needs as represented by their queries.
can be gained from evaluating an ir system this way.
further, this evaluation framework is not  metricspecific: information in terms of any metric (map, p@10, etc.)
in addition, this framework takes advantage and stretches the amount of information we can get from existing test  collections.
most importantly, it provides a controlled manner in which to measure the performance of qe with respect to query-document term mismatch.
the evaluation framework proposed in this paper is  attractive for several reasons.
further, we can model the  behavior of expert versus non-expert users by manipulating the amount of query-document term mismatch introduced into the collection.
by systematically removing query terms from relevant documents, we can measure the degree to which qe contributes to a search by showing the difference  between the performances of a qe system and its  keywordonly baseline when query terms have been removed from known relevant documents.
evaluations of ir systems employing qe performed only on the entire collection do not take into account that the purpose of qe is to mitigate the effects of term mismatch in retrieval.
the proposed evaluation framework allows us to measure the degree to which different ir systems overcome (or don"t overcome) term mismatch between queries and relevant  documents.
