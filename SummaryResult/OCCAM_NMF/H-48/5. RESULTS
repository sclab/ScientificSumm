0 1 2 3 4 5 number of query terms removed from relevant documents 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 recall okapi fb okapi tcs ql ap89: recall at 1000 documents with query terms removed (title queries) figure 9: recall (at 1000) of the four ir systems on the ap89 collection, using trec title queries and as a function of the number of query terms removed. 
0 1 2 3 4 5 number of query terms removed from relevant documents 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.5 precisionat10documents(p10) okapi fb okapi tcs ql ap89: p10 with query terms removed (title queries) figure 8: precision at 10 of the four ir systems on the ap89 collection, using trec title queries, and as a function of the number of query terms removed.
0 1 2 3 4 5 number of query terms removed from relevant documents 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 meanaverageprecision(map) okapi fb okapi tcs ql ap89: mean average precision with query terms removed (title queries) figure 7: map of the four ir systems on the ap89 collection, using trec title queries and as a  function of the number of query terms removed.
perhaps the most interesting result from our evaluation is that although the keyword-only baselines performed  consistently and as expected on both collections with respect to query term removal from relevant documents, the  performances of the engines implementing qe techniques differed dramatically between collections.
on the shorter queries, tcs seems to have a harder time catching up to the performance of okapi fb as terms are removed.
as before, the performance of the okapi and ql systems, the non-qe baseline systems, sharply degrades as query terms are removed.
as with the ap89 description queries, okapi fb is again the best performer of the four systems in the evaluation.
5.3 the ap89 collection: using the title queries figures 7, 8, and 9 show the performance of the four ir systems on the ap89 collection, using the trec topic titles as queries.
this is again a case where the performance of a system on the entire collection is not necessarily indicative of how it handles query-document term mismatch.
it is interesting to note that on each metric for the ap89 description queries, tcs performs more poorly than all the other systems on the original collection, but quickly  surpasses the baseline systems and approaches okapi fb"s  performance as terms are removed.
if modeling this in terms of expert versus non-expert users, we could conclude that tcs might be a better search engine for non-experts to use on the ap89 collection, while okapi fb would be best for an expert searcher.
at two query terms removed, tcs starts outperforming okapi fb.
looking at p10 in figure 5, we can see that tcs and okapi fb score similarly on p10, starting at the point where one query term is removed from relevant documents.
we would expect to be handicapped when query terms are removed.
0 1 2 3 4 5 6 7 number of query terms removed from relevant documents 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 recall okapi fb okapi tcs ql ap89: recall at 1000 documents with query terms removed (description queries) figure 6: recall (at 1000) of the four ir systems on the ap89 collection, using trec description queries, and as a function of the number of query terms removed.
p at 10 is measured as a function of the number of query terms removed.
0 1 2 3 4 5 6 7 number of query terms removed from relevant documents 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 precisionat10documents(p10) okapi fb okapi tcs ql ap89: p10 with query terms removed (description queries) figure 5: precision at 10 of the four ir systems on the ap89 collection, using trec description queries.
map is measured as a function of the number of query terms removed.
0 1 2 3 4 5 6 7 number of query terms removed from relevant documents 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 meanaverageprecision(map) okapi fb okapi tcs ql ap89: mean average precision with query terms removed (description queries) figure 4: map of the four ir systems on the ap89 collection, using trec description queries.
this is all the more interesting, based on the fact that qe in okapi fb takes place after the first search iteration, which 0 1 2 3 4 5 6 7 number of query terms removed from relevant documents 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 recall okapi fb okapi tcs indri fsupp: recall at 1000 documents with query terms removed figure 3: the recall (at 1000) of the four retrieval systems on the fsupp collection as a function of the number of query terms removed (the horizontal axis).
on fsupp, tcs outperformed the other engines consistently (see figures 1, 2, and 3); on the ap89 collection, okapi fb is clearly the best performer (see figures 4, 5, and 6).
the most interesting difference  between the performance on the fsupp collection and the ap89 collection is the reversal of okapi fb and tcs.
5.2 the ap89 collection: using the description queries figures 4, 5, and 6 show the performance of the four ir systems on the ap89 collection, using the trec topic  descriptions as queries.
however, when we look at the comparison of tcs to ql when query terms are removed from the relevant  documents, we can see that the qe in tcs is indeed contributing positively to the search.
other evaluation) to hurt performance on the fsupp  collection.
0 1 2 3 4 5 6 7 number of query terms removed from relevant documents 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.5 0.55 0.6 precisionat10documents(p10) okapi fb okapi tcs ql fsupp: p10 with query terms removed figure 2: the performance of the four retrieval  systems on the fsupp collection in terms of precision at 10 and as a function of the number of query terms removed (the horizontal axis).
because overall search engine performance is frequently measured in terms of map, and because other evaluations of qe often only consider performance on the entire collection (i.e., they do not consider term mismatch), the qe implemented in tcs would be considered (in  an0 1 2 3 4 5 6 7 number of query terms removed from relevant documents 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 meanaverageprecision(map) okapi fb okapi tcs ql fsupp: mean average precision with query terms removed figure 1: the performance of the four retrieval  systems on the fsupp collection in terms of mean  average precision (map) and as a function of the  number of query terms removed (the horizontal axis).
because tcs employs  implicit query expansion using an external domain specific knowledge base, it is less sensitive to term removal (i.e., mismatch) than the okapi fb, which relies on terms from the top-ranked documents retrieved by an initial  keywordonly search.
tcs outperforms the ql keyword baseline on every  measure except for map on the original collection (i.e., prior to removing any query terms).
the performance of okapi with feedback (okapi fb) is somewhat surprising in that on the original collection (i.e., prior to query term removal), its  performance is worse than that of okapi without feedback on all three measures.
as expected, the  performance of the keyword-only ir systems, ql and okapi, drops quickly as query terms are removed from the relevant  documents in the collection.
5.1 fsupp collection figures 1, 2, and 3 show the performance (in terms of map, p10 and recall, respectively) for the four search  engines on the fsupp collection.
