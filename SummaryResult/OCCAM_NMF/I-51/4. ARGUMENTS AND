in the following sections we will present these elements. 
however, in order to do so, they need a specific interaction protocol, a preference relation between contradicting arguments, and a  decision policy to generate counterarguments (including  counterexamples).
they can engage a joint deliberation process.
the case must satisfy the justification α.d and the solution of c must be different than the predicted by α. by exchanging arguments and counterarguments (including  counterexamples), agents can argue about the correct solution of a given problem, i.e.
a counterexample c is a case that contradicts an argument α. thus a counterexample is also a counterargument, one that states that a specific argument α is not always true, and the evidence  provided is the case c. specifically, for a case c to be a  counterexample of an argument α, the following conditions have to be met: α.d c and α.s = c.s, i.e.
in the example in figure 1, if an agent generates the argument α = ai, p, walk, (cars_passing = no) , an agent that thinks that the correct solution is wait might answer with the counterargument β = aj, p, wait, (cars_passing = no ∧ traffic_light = red) ,  meaning that, although there are no cars passing, the traffic light is red, and the street cannot be crossed.
a counterargument β is an argument offered in opposition to another argument α. in our framework, a counterargument  consists of a justified prediction aj, p, s , d generated by an agent aj with the intention to rebut an argument α generated by another agent ai, that endorses a solution class s different from that of α.s for the problem at hand and justifies this with a justification d .
in the  example depicted in figure 1, an agent ai may generate the argument α = ai, p, wait, (traffic_light = red) , meaning that the agent ai believes that the correct solution for p is wait because the attribute traffic_light equals red.
a justified prediction α is generated by an agent ai to argue that ai believes that the correct solution for a given problem p is α.s, and the evidence provided is the justification α.d.
using this information, we can define three types of arguments: justified predictions,  counterarguments, and counterexamples.
in the context of mac systems, agents argue about predictions for new problems and can provide two kinds of information: a) specific cases p, s , and b) justified predictions: a, p, s, d .
in the remainder of this section we will see how this  general definition of argument can be instantiated in specific kind of arguments that the agents can generate.
counterarguments for our purposes an argument α generated by an agent a is  composed of a statement s and some evidence d supporting s as  correct.
