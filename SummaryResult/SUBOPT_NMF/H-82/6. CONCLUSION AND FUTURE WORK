But how can the crawler discover the query interfaces?
The main challenge is to automatically segment the returned pages so that we can  identify the sections of the pages that present the values corresponding to each attribute.
a calendar with links for every day).
We proposed three different query generation policies for the Hidden Web : a policy that picks queries at random from a list of keywords , a policy that picks queries based on their frequency in a generic text collection , and a policy which adaptively picks a good query based on the content of the pages 