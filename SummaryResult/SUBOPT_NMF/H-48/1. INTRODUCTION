IR evaluations are comparative in nature (cf.
Often, the terms chosen by users in their queries are different than those appearing in the documents relevant to their information needs.
In order to measure that a particular IR system is able to overcome query-document term mismatch by retrieving documents that are relevant to a user"s query, but that do not necessarily contain the query terms themselves, we  systematically introduce term mismatch into the test collection by removing query terms from known relevant documents.
Generally , IR evaluations show how System A did in relation to System B on the 