The top row shows how the solution time for the MILP scales as we  increase the number of agents |M|, the global time horizon bτ, and the number of resources |Ω|.
Figure 4 shows runtime and policy value for trials in which common input variables are scaled together.
These  resources are in finite supply, and optimal policies for the shop will determine when each agent may hold the limited  resources to take actions and earn individual rewards.
Top row shows CPU time, and bottom row shows the joint reward of agents" MDP policies.
Overall , we believe that these 