POMDPs rely entirely on the mechanism of reward accumulation maximization, i.e., formation of the action selection procedure to achieve necessary state sequencing.
There is recent interest in using POMDPs in hybrid solutions [17], in which the POMDPs can be used together with other control approaches to provide results not easily achievable with either approach by itself.
2 Entropy was calculated using log base equal to the number of  possible locations within the domain; this properly scales entropy  expression into the range [0, 1] for all domains.
The tag game experiment data also revealed the different emphasis DBC and POMDPs place 