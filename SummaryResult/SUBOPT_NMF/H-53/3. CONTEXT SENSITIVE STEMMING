By the chain rule of probability one can write the probability of any word sequence as Pr(w1w2...wN ) = NY i=1 Pr(wi|w1...wi−1) (1) An n-gram model approximates this probability by  assuming that the only words relevant to predicting Pr(wi|w1...wi−1) are the previous n − 1 words; i.e.
Pr(wi|w1...wi−1) = Pr(wi|wi−n+1...wi−1) A straightforward maximum likelihood estimate of n-gram probabilities from a corpus is given by the observed  frequency of each of the patterns Pr(wi|wi−n+1...wi−1) = #(wi−n+1...wi) #(wi−n+1...wi−1) (2) where #(.)
The simplest and most 