We first show that groups of users who amply discuss a certain feature are more likely to agree on a common rating for that feature.
Previous work has demonstrated that contradictory reporting and underlying user biases make judging the true worth of a  service difficult.
We look at two sources of information besides numerical ratings: linguistic evidence from the textual comment accompanying a review, and patterns in the time sequence of reports.
Second, we show that a user"s rating partly reflects the difference between true quality and prior expectation of quality as inferred from previous reviews.
Both give 