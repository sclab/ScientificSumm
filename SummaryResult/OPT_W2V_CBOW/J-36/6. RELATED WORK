Most of this work differs significantly from our own in that it considers single-agent planning as opposed to the game-theoretic setting.
The work on bounded rationality [23, 24, 53, 58] differs from the models that we consider here in that instead of putting hard limitations on the computational power of the agents, we instead restrict their a priori knowledge of the state of the world, requiring them to spend time (and therefore money/utility) to learn about it.
This issue can also be viewed as another perspective on the general question of exploration versus exploitation that arises often in AI : 