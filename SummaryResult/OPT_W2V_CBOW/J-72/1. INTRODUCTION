Since their work is also grounded in learning theory, they allow dependence on the size of the target  valuation as we do (though read-once valuations can always be succinctly represented anyway).
In contrast, our work shows that given a more general (yet still quite standard) version of demand query than the type they consider, the complexity of preference elicitation is no greater than the complexity of learning.
The advantage of our scheme is that agents can be viewed as black-boxes that provide incremental information about their valuations.
Though the goals of learning and preference elicitation differ somewhat , it is 