Thus, the margin found by an  optimization is not guaranteed to be one that maximizes the margin for the global data set of examples {x1, .
The learner may encounter an example that lies within the margins, but farther from the margins than M. Such an example means the hypothesis is no longer globally optimal for the data set, but it is considered good enough for continued use without immediate retraining.
We propose three ways to relax Online SVMs: â€¢ Reduce the size of the optimization problem by only optimizing over the last p examples.
Note that this is not equivalent 