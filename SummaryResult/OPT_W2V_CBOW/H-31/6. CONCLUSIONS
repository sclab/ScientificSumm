We exploit this property to develop a new per-term smoothing algorithm for Poisson language models, which is shown to outperform term-independent smoothing for both Poisson and multinomial models.
We compare the new models with the popular multinomial retrieval models both analytically and experimentally.
Furthermore, we show that a  mixture background model for Poisson can be used to improve the performance and robustness over the standard Poisson background model.
We  derive several smoothing methods for this family of models, including single-stage smoothing and two-stage smoothing.
Further exploring the flexibilities over multinomial language models , such as length normalization and pseudo-feedback could 