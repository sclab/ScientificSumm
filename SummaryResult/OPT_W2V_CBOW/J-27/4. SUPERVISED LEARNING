W.l.g we may assume the error is bounded (since we are looking at what is essentially a finite set) therefore the probability that erσ(h, fb0 ) > ε cannot be too small, hence fb0 is not  PAClearnable with a sample of size n ✷ The following theorem gives an upper bound on the  sample complexity required for learning a set of functions with finite fat shattering dimension.
Suppose that a learning algorithm observes a finite set of production data which it assumes comes from a Cobb-Douglas  production function and returns a hypothesis that is a polynomial of bounded degree.
The 