We find that  resampling documents helps increase individual feedback model precision by removing noise terms, while sampling from the query improves robustness (worst-case performance) by  emphasizing terms related to multiple query aspects.
We then perform model combination over several enhanced models, each based on a slightly modified query sampled from the original query.
Existing pseudo-relevance feedback methods typically  perform averaging over the top-retrieved documents, but  ignore an important statistical dimension: the risk or variance associated with either the individual document models, or their combination.
Treating the baseline feedback method as a black box , and the output feedback model as 