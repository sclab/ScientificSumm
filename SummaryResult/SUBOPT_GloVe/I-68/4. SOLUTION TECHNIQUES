This double dependency results from the fact, that the expected reward for starting the execution of method mj at time t also depends on the probability that method mj will be enabled by time t. Consequently, if time is discretized, one needs to consider Δ|M| candidate policies in order to find π∗ .
While such state representation is beneficial , in that the problem can be solved with a standard value iteration algorithm , it blurs the intuitive mapping from time t to the expected total reward for starting the execution of mj at time t. Consequently , if some method 