Evaluation is an important aspect of information retrieval research, but it is only a semi-solved problem: for most retrieval tasks, it is impossible to judge the relevance of every document; there are simply too many of them.
This method creates large sets of judgments that are reusable for training or evaluating new systems that did not contribute to the pool [21].
Research groups would be able to share the relevance judgments they have done in-house for pilot studies, new tasks, or new topics.
While we can say that it was right 75 % of the time , or that it 