We exploit this property to develop a new per-term smoothing algorithm for Poisson language models, which is shown to outperform term-independent smoothing for both Poisson and multinomial models.
Further exploring the flexibilities over multinomial language models, such as length normalization and pseudo-feedback could be good future work.
We compare the new models with the popular multinomial retrieval models both analytically and experimentally.
Furthermore, we show that a  mixture background model for Poisson can be used to improve the performance and robustness over the standard Poisson background model.
We present a new family of query generation language models for retrieval based 