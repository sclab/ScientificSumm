Second, because the second term does not depend on the actions of agent i, any action by agent i that improves D, also improves G. This term which measures the amount of alignment between two rewards has been dubbed factoredness in previous work [2, 17].
3.3 Agent Learning The objective of each agent is to learn the best values of d that will lead to the best system performance , G. In this paper we assume that each agent will have a reward function and will aim to maximize its reward using its own reinforcement learner [ 15 ] ( 