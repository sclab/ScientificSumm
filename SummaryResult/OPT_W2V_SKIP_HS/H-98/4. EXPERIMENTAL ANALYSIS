The parameters are still fit using maximum likelihood estimation, but a model of noisy class labels is used in addition to allow for the possibility that the class was mislabeled.
In addition, we also compare the error of the classifiers at their default thresholds and with the probabilities.
The basic observation made about na¨ıve Bayes in previous work is that it tends to produce estimates very close to zero and one [1, 17].
We note that even though it is computationally efficient to perform leave-one-out cross-validation for the na¨ıve Bayes classifier , this may not be desirable since the distribution 