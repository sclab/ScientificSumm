If we also have fixed the parameter θ of the simulator, the resulting data point becomes part of our data set D. This data set, then, contains data only in the form of pure strategies of  players and their corresponding payoffs, and, consequently, in order to formulate the designer"s problem as optimization, we must first  determine or approximate the set of Nash equilibria of each game Γθ.
this work, the expression gives an upper bound on the probability that some setting of θ (i.e., storage cost) in the interval [0,320] will result in total day-0 procurement that is no greater in any  equilibrium than the target specified by α and taken here to be 2.
Tables 1 and 2 suggest that the existence of any equilibrium with φ(a) < 2.7 is unlikely for any θ that we have data for, although this judgment, as we mentioned, is only with respect to the profiles we have actually sampled.
Since our data comes in the form of payoff experience and not as the value of an objective function for given settings of the control variable, we can no longer rely on the methods for optimizing  functions using simulations.
We refer to it as BestFirstSearch, as it selects with probability one a strategy profile a ∈ Snb(a, ˜D) that has the smallest ˆin D. Finally we define an estimator for a set of Nash equilibria.
Observe, that even the lower bound of our estimated set of Nash equilibria is well above the target day-0 procurement of 2.
In what follows, we estimate only a sample Nash equilibrium using this technique, although this restriction can be removed at the expense of additional computation time.
All of this evidence has been in the form of simple interpolation and extrapolation of estimates of the Nash equilibrium correspondence.
Additionally, we generated an estimate of the Nash equilibrium correspondence, ˆφ∗ (θ), by applying Definition 6 with δ =2.5E6.
Let φ(a) = P6 i=1 ai be the  aggregation function representing the sum of day-0 procurement of the six agents participating in a particular supply-chain game (for mixed strategy profiles s, we take expectation of φ with respect to the  mixture).
To answer this question directly, suppose that we set a  conservative threshold α = 2 on aggregate day-0 procurement.12 Linear 11 It is unclear how meaningful the results of learning would be if Ds were added to the training data set.
As we can see, the correspondence ˆφ∗ (θ) has little predictive power based on Do, and reveals no interesting structure about the game.
To derive an approximate  probabilistic bound on the probability that no θ ∈ Θ could have achieved the designer"s objective, let ∪J j=1Θj, be a partition of Θ, and assume that the function sup{φ∗ (θ)} satisfies the Lipschitz condition with Lipschitz constant Aj on each subset Θj.13 Since we have  determined that raising the storage cost above 320 is undesirable due to secondary considerations, we restrict attention to Θ = [0, 320].
We further restrict it to the set [0,1.5], since any strategy below 0 is illegal and strategies above 1.5 are extremely aggressive (thus unlikely to provide refuting deviations beyond those available from included strategies, and certainly not part of any desirable equilibrium).
We present some theoretical support for this method of estimating the set of Nash equilibria below.
However, the ultimate goal is to be able to  either set the storage cost parameter to a value that would curb day-0 procurement in equilibrium or conclude that this is not possible.
309 4.4.2 Analysis of Search Data To corroborate the initial evidence from the learning methods, we estimated ˆφ∗ (θ) (again, using δ =2.5E6) on the data set D = {Do, Ds}, where Ds is data generated through the application of BestFirstSearch.
The first has been explored empirically before, whereas the second is introduced here as the method specifically designed to approximate a set of Nash equilibria.
For comparison, to get the entire description of an empirical game defined by the restricted finite joint strategy space for each value of θ ∈ {0, 50, 100, 150, 200} would have required at least 23100 games (sampling each profile 10 times).
Combining the evidence gathered from these two very different approaches to estimating the outcome correspondence yields a much more compelling picture of the  relationship between storage costs and day-0 procurement than either method used in isolation.
It also confirms that high levels of day-0 procurement are a rational response to the 2004 tournament setting of average storage cost, which corresponds to θ = 100.
Instead of taking the conservative upper bound based on data over the entire function domain Θ, we take the average of upper bounds obtained at each Θj.
We will further denote each Θj by (aj , bj].14 Then, the following Proposition gives us an approximate upper bound15 on the probability that sup{φ∗ (θ)} ≤ α.
The designer"s welfare function W (N(θ), θ) is then given by I{sup{φ∗ (θ)} ≤ α}, where α is the maximum acceptable level of day-0 procurement and I is the indicator function.
We also rely on their result that ui(a)|¯ui(a) = ¯ui(a)−Zi(a)/[σi(a)/ p ni(a)] are independent with posterior  distributions N(¯ui(a), σ2 i (a)/ni(a)), where ni(a) is the number of samples taken of payoffs to i for pure profile a, and Zi(a) ∼ N(0, 1).
Now, we consider cases of incomplete data and use the results we have just obtained to construct an upper bound (restricted to profiles represented in data) on the distribution of sup{φ∗ (θ)} and inf{φ∗ (θ)} (assuming that both are attainable): Pr{sup{φ∗ (θ)} ≤ x} ≤D Pr{∃a ∈ D : φ(a) ≤ x ∧ a ∈ N(θ)} ≤ X a∈D:φ(a)≤x Pr{a ∈ N(θ)} = X a∈D:φ(a)≤x Pr{ (a) = 0}, where x is a real number and ≤D indicates that the upper bound accounts only for strategies that appear in the data set D. Since the events {∃a ∈ D : φ(a) ≤ x ∧ a ∈ N(θ)} and {inf{φ∗ (θ)} ≤ x} are equivalent, this also defines an upper bound on the  probability of {inf{φ∗ (θ)} ≤ x}.
If ui(·)|¯ui(·) are independent for all i ∈ I and a ∈ A, we have the following result (from this point on we omit conditioning on ¯ui(·) for brevity): PROPOSITION 1.
The values thus derived comprise the Tables 1 and 2. φ∗ (θ) θ = 0 θ = 50 θ = 100 <2.7 0.000098 0 0.146 <3 0.158 0.0511 0.146 <3.9 0.536 0.163 1 <4.5 1 1 1 Table 1: Upper bounds on the distribution of inf{φ∗ (θ)}  restricted to D for θ ∈ {0, 50, 100} when N(θ) is a set of Nash equilibria.
0 1 2 3 4 5 6 7 8 9 10 0 50 100 150 200 Storage Cost TotayDay-0Procurement LWA LWLR QR SearchMin SearchMax Figure 2: Aggregate day-0 procurement estimates based on search in strategy profile space compared to function  approximation techniques trained on Do.
The posterior distribution of the optimum mean of n samples, derived by Chang and Huang [3], is Pr (ui(a) ≤ c) = 1 − Φ "p ni(a)(¯ui(a) − c) σi(a) # , (3) where a ∈ A and Φ(·) is the N(0, 1) distribution function.
Pr{ _ θ∈Θ sup{φ(θ)} ≤ α} ≤D 5X j=1 X y,z∈D:y+z≤cj 0 @ X a:φ(a)=z Pr{ (a) = 0} 1 A × × 0 @ X a:φ(a)=y Pr{ (a) = 0} 1 A , where cj = 2α + Aj(bj − aj) and ≤D indicates that the upper bound only accounts for strategies that appear in the data set D. 13 A function that satisfies the Lipschitz condition is called Lipschitz continuous.
0.00E+00 5.00E+06 1.00E+07 1.50E+07 2.00E+07 2.50E+07 3.00E+07 3.50E+07 4.00E+07 4.50E+07 5.00E+07 2.1 2.4 2.7 3 3.3 3.6 3.9 4.2 4.5 4.8 5.1 5.4 5.7 6 6.3 6.6 6.9 7.2 Total Day-0 Procurement ε−boundFigure 3: Values of ˆ for profiles explored using search when θ = 320.
Since the game we are interested in is infinite, it is necessary to terminate BestFirstSearch before exploring the entire space of  strat7 For example, we can use active learning techniques [5] to improve the quality of payoff function approximation.
Since the designer"s decision  depends only on φ∗ (θ), we present all of our results in terms of the value of the aggregation function.
If we look at the third column of this table, which corresponds to θ = 100, we can gather that no profile a in our data with φ(a) < 3 is very likely to be played in equilibrium.
This choice can provide only an estimate of the actual tournament behavior of a typical agent.
4.4.3 Extrapolating the Solution Correspondence We have reasonably strong evidence that the outcome  correspondence is decreasing.
Table 1 also provides additional evidence that the agents in the 2004 TAC/SCM tournament were indeed rational in procuring large numbers of components at the beginning fo the game.
8 Generally, search is terminated once the set of candidate equilibria is small enough to draw useful conclusions about the likely range of equilibrium strategies in the game.
Strategy profiles explored are presented in terms of the corresponding values of φ(a).
Between 5 and 10 games were run for each profile after discarding games that had various flaws.10 We used search to generate a simulated data set Ds, performing between 12 and 32 iterations of BestFirstSearch for each of the above settings of θ.
Instead, the approach we advocate here is to collect as much evidence as is feasible given resource constraints, and make the most compelling judgment based on this evidence, if at all  possible. 
We conclude that it is unlikely that day-0 procurement could ever be reduced to a desirable level using any reasonable setting of the storage cost parameter.
Furthermore, we cannot rely directly on the convergence results that abound in the simulation optimization literature, and must  establish probabilistic analysis methods tailored for our problem  setting.
We also used  control variates to reduce the variance of payoff estimates, as in our previous empirical game-theoretic analysis of TAC/SCM-03 [19].
We can then accept this as another piece of evidence that the designer could not find a suitable setting of θ to achieve his objectives-indeed, the designer seems unlikely to achieve his objective even if he could persuade participants to play a desirable equilibrium!
4.5 Probabilistic Analysis Our empirical analysis has produced evidence in support of the conclusion that no reasonable setting of storage cost was likely to sufficiently curb excessive day-0 procurement in TAC/SCM "04.
Recall that during the 2004 tournament, the designers of the  supplychain game chose to dramatically increase storage costs as a  measure aimed at curbing day-0 procurement, to little avail.
The bound at an interval is then taken to be the maximum of the upper bound for this interval and the average upper bound for all intervals.
310 and finite variance, that is, Ui(a) = ui(a) + ˜ξi(a), where Ui(a) is the observed payoff to i when a was played, ui(a) is the actual  corresponding payoff, and ˜ξi(a) is a mean-zero normal random  variable.
In terms of our claims in maxj Aj Aj max{Aj ,ave(Aj)} 1 0.00772 0.00791 Table 3: Approximate upper bound on probability that some setting of θ ∈ [0, 320] will satisfy the designer objective with target α = 2.
, 1.5} in order to have positive probability of generating strategic neighbors.9 A  baseline data set Do was generated by sampling 10 randomly generated strategy profiles for each θ ∈ {0, 50, 100, 150, 200}.
Pr „ max i∈I max b∈Ai ui(b, a−i) − ui(a) ≤ « = = Y i∈I Z R Y b∈Ai\ai Pr(ui(b, a−i) ≤ u + )fui(a)(u)du, (2) where fui(a)(u) is the pdf of N(¯ui(a), σi(a)).
Even if we can assume that a function that we approximate from data is Lipschitz continuous, we rarely actually know the Lipschitz constant for any subset of Θ.
One advantage of this method is that it can be applied to any data set and does not require the use of a simulator.
Once approximate payoff functions are available for all players, the Nash equilibria may be either found analytically or approximated using numerical techniques, depending on the learning model.
We take ¯ui(a) to be the sample mean over all Ui(a) in D, and follow Chang and Huang [3] to assume that we have an improper prior over the actual payoffs ui(a) and sampling was independent for all i and a.
We restrict the agent strategies to be a  multiplier on the quantity of the day-0 requests by one of the  finalists, Deep Maize, in the 2004 TAC/SCM tournament.
As we had suspected, the most conservative approach to estimating the upper bound on slope, presented in the first column of the table, provides us little information here.
The estimated set of aggregate day-0 outcomes is very close to that for θ = 200, indicating that there is little additional benefit to raising storage costs above 200.
Our approach takes as given some set of design options, in this case defined by the storage cost parameter.
Figure 3 shows the detailed -bounds for all profiles in terms of their corresponding values of φ.
The data for θ = 320 were collected in the same way as for other storage cost settings, with 10 randomly generated profiles followed by 33 iterations of BestFirstSearch.
The quadratic regression model makes it possible to compute equilibria of the learned game analytically.
In this section, we develop and apply methods for evaluating the sensitivity of our -bound calculations to such stochastic effects.
Certainly, we have not checked all the profiles but only a small proportion of them (infinitesimal, if we consider the  entire continuous domain of θ and strategy sets).
We now derive a generic probabilistic bound that a profile a ∈ A is an -Nash equilibrium.
In words, the estimate of a set of equilibrium outcomes is the  convex hull of all aggregated strategy profiles with -bound below some fixed δ.
φ∗ (θ) θ = 150 θ = 200 θ = 320 <2.7 0 0 0.00132 <3 0.0363 0.141 1 <3.9 1 1 1 <4.5 1 1 1 Table 2: Upper bounds on the distribution of inf{φ∗ (θ)}  restricted to D for θ ∈ {150, 200, 320} when N(θ) is a set of Nash equilibria.
4.2.1 Payoff Function Approximation The first method for estimating Nash equilibria based on data uses supervised learning to approximate payoff functions of  mech6 We do not address whether and how other measures (e.g.,  constraining procurement directly) could have achieved design  objectives.
The minimum prediction for aggregate procurement at this level of storage costs given by any experimental methods is approximately 3.
The bounds above provide some general evidence, but ultimately we are interested in a concrete probabilistic assessment of our  conclusion with respect to the data we have sampled.
First, we note that the addition of the search data narrows the range of potential equilibria substantially.
Combining the results (2) and (3), we obtain a probabilistic  confidence bound that (a) ≤ γ for a given γ.
We currently determine termination time in a  somewhat ad-hoc manner, based on observations about the current set of candidate equilibria.8 4.3 Data Generation Our data was collected by simulating TAC/SCM games on a  local version of the 2004 TAC/SCM server, which has a configuration setting for the storage cost.
Additionally, suppose that noise is additive with zero-mean the entire game on average, so in practice we would probably want the threshold to be even lower.
Thus, if we fix a strategy for each agent at the beginning of the simulation and record the corresponding profits at the end, we will have obtained a data point in the form (a, U(a)).
15 It is approximate in a sense that we only take into account  strategies that are present in the data.
4.2.2 Search in Strategy Profile Space When we have access to a simulator, we can also use directed search through profile space to estimate the set of Nash equilibria, which we describe here after presenting some additional notation.
The expected total day-0 procurement in  equilibrium was taken as the estimate of an outcome.
The results of this estimate are plotted against the results of the learning methods trained on Do 11 in Figure 2.
Furthermore, the actual point  predictions of the learning methods and those based on -bounds  after search are reasonably close.
311 Due to the fact that our bounds are approximate, we cannot use them as a conclusive probabilistic assessment.
We now define each subset j to be the interval between two points for which we have produced data.
The first one simply takes the highest slope that the function attains within the available data and uses this constant value for every subinterval.
And third, could increasing storage costs sufficiently have reduced day-0 procurement to an acceptable level, and if so, what should the setting of storage costs have been?
This is quite high, as it corresponds to an  expected commitment of 1/3 of the total supplier capacity for the  entire game.
It is this third question that defines the mechanism design aspect of our analysis.6 To apply our methods, we must specify the agent strategy sets, the designer"s welfare function, the mechanism parameter space, and the source of data.
For the other methods we applied replicator dynamics [7] to a discrete approximation of the learned game.
We define Snb(a, D) as the set of all strategic neighbors of a available in the data set D. Similarly, we define Snb(a, ˜D) to be all strategic neighbors of a not in D. Finally, for any a ∈ Snb(a, D) we define the deviating agent as i(a, a ).
We model the designer"s welfare function as a threshold on the sum of day-0 purchases.
Since simulation cost is  extremely high (a game takes nearly 1 hour to run), we were able to run a total of 2670 games over the span of more than six months.
However, the other two  estimation approaches, found in columns two and three of Table 3,  suggest that we are indeed quite confident that no reasonable setting of θ ∈ [0, 320] would have done the job.
Here we systematically explore the relationship between storage costs and 5 This is often the case for real games of interest, where natural language or algorithmic descriptions may substitute for a formal specification of strategy and payoff functions.
9 Of course, we do not restrict our Nash equilibrium estimates to stay in this discrete subset of [0,1.5].
Below, we describe the two methods we used in our study.
Given the tremendous  difficulty of the problem, this result is very strong.16 Still, we must be very cautious in drawing too heroic a conclusion based on this  evidence.
This evidence supports the initial intuition that day-0  procurement should be decreasing with storage costs.
Thus, we are faced with a task of estimating it from data.
In contrast, all three learning methods suggest that total day-0 procurement is a decreasing function of storage costs.
Our threshold here still represents a commitment of over 20% of the suppliers" capacity for extrapolation of the maximum of the outcome correspondence  estimated from D yields θ = 320.
The correspondence ˆφ∗ (θ) for D = {Do, Ds} is the interval between SearchMin and SearchMax.
However, we  believe that the general form of the results should be robust to changes in the full agent behavior.
However, since the actual upper bound is generally greater for each subinterval, the resulting probabilistic bound may be deceiving.
If a simulator is available, we can generate additional data to build confidence in our initial estimates.7 We tried the following methods for approximating payoff  functions: quadratic regression (QR), locally weighted average (LWA), and locally weighted linear regression (LWLR).
These estimates are based on simulating game instances, and are subject to sampling noise contributed by the various stochastic  elements of the game.
Our predictions  underestimate this outcome to some degree, but show that any rational outcome was likely to have high day-0 procurement.
A strategic neighbor of a pure strategy profile a is a profile that is identical to a in all but one strategy.
4.4 Results 4.4.1 Analysis of the Baseline Data Set We applied the three learning methods described above to the baseline data set Do.
The maximum prediction is considerably higher at 4.5.
Since we are reasonably confident that φ∗ (θ) is decreasing in θ, we also do not expect that setting θ somewhere between 200 and 320 will achieve the desired result.
An alternative method is to take an upper bound on slope  obtained within each subinterval using the available data.
When Snb(a, ˜D) = ∅ (i.e., all strategic neighbors are represented in the data), a is confirmed as an ˆ-Nash equilibrium.
The designer selects a value θ of storage costs, expressed as an annual  percentage of the baseline value of components in the inventory (charged daily), from the set Θ = R+ .
Consequently, increasing the costs further would be undesirable even if day-0 procurement could eventually be curbed.
0 1 2 3 4 5 6 7 8 9 10 0 50 100 150 200 Storage Cost TotalDay-0Procurement LWA LWLR QR BaselineMin BaselineMax Figure 1: Aggregate day-0 procurement estimates based on Do.
4.2 Estimating Nash Equilibria The objective of TAC/SCM agents is to maximize profits  realized over a game instance.
The proofs of this and all subsequent results are in the Appendix.
The correspondence ˆφ∗ (θ) is the interval between  BaselineMin and BaselineMax.
Indeed, the additional data may actually increase the learning variance.
In particular, if one is an equilibrium, the other may be as well.
The -bound, ˆ, of a pure strategy profile a is defined as maxa ∈Snb(a,D) max{ui(a,a )(a )−ui(a,a )(a), 0}.
12 Recall that designer"s objective is to incentivize aggergate day-0 procurement that is below the threshold α.
A final method that we tried is a compromise between the two above.
14 The treatment for the interval [0,50] is identical.
If two profiles are close in terms of aggregation values, they may be likely to have similar -bounds.
4.1 TAC/SCM Design Problem We describe our empirical design analysis methods by presenting a detailed application to the TAC/SCM scenario introduced above.
Instead, we take this as another piece of evidence to complement our findings.
First, does increasing storage costs actually reduce day-0 procurement?
In the actual 2004 competition, aggregate day-0 procurement was equivalent to 5.71 on the scale used here [9].
Particularly, we would like to say something about what happens for the settings of θ for which we have no data.
the aggregate quantity of components procured on day 0 in  equilibrium.
In this work, we  instead concentrate on search in strategy profile space.
To achieve the desired reduction in day-0 procurement requires redesigning other aspects of the mechanism.
In doing so, we consider several questions raised during and after the tournament.
We define ˆφ∗ (θ) = Co({φ(a) : a ∈ Bδ}) for a fixed δ to be an estimator of φ∗ (θ).
Thus, Θ = [0, 50] [ (50, 100] [ (100, 150] [ (150, 200] [ (200, 320], with j running between 1 and 5, corresponding to subintervals above.
Thus, we need methods for approximating Nash equilibria for  infinite games.
Thus, it is feasible to sample the entire payoff matrix of the game.
Indeed, a fundamental aspect of our design problem involves estimating the Nash equilibrium correspondence.
Furthermore, payoffs to agents are almost always negative at θ = 320.
This produces the most conservative bound, and in many situations it is unlikely to be informative.
For a set K, define Co(K) to be the convex hull of K. Let Bδ be the set of candidates at level δ.
In principle our methods could be applied to a different or larger design space, though with corresponding complexity growth.
The results of evaluating the expression for Pr{ _ θ∈Θ sup{φ∗ (θ)} ≤ α} when α = 2 are presented in Table 3.
All other behavior is based on the behavior of Deep Maize and is  identical for all agents.
Different methods of approximating the upper bound on slope in each subinterval j are used.
Thus, we can apply it when Ds = ∅.
Here, we tried three methods of doing this.
We say that a is a candidate δ-equilibrium for δ ≥ ˆ.
Nor can we expect ever to obtain enough evidence to make completely objective  conclusions.
Second, was the excessive day-0  procurement that was observed during the 2004 tournament rational?
Agent strategies in simulated games were selected from the set {0, 0.3, 0.6, .
This definition allows us to exploit structure  arising from the aggregation function.
Suppose that all agents have finite (and small) pure strategy sets, A.
Thus, we assume that ˜ξi(a) is normal with distribution N(0, σ2 i (a)).
308 anism participants from a data set of game experience [17].
We designate the known variance of ˜ξi(a) by σ2 i (a).
The gray region corresponds to ˆφ∗ (320) with δ =2.5M.
Our search method operates by exploring deviations from  candidate equilibria.
This  produces a much less conservative upper bound on probabilities.
That our predictions tend to underestimate tournament outcomes reinforces this conclusion.
10 For example, if we detected that any agent failed during the game (failures included crashes, network connectivity problems, and other obvious anomalies), the game would be thrown out.
The results are shown in Figure 1.
egy profiles.
DEFINITION 5.
DEFINITION 4.
DEFINITION 6.
PROPOSITION 2.
