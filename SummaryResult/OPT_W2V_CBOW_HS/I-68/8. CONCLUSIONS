In this  paper, we improved a state-of-the-art heuristic solution method for DEC-MDPs, called OC-DEC-MDP, that has recently been shown to scale up to large DEC-MDPs.
Our heuristic solution method, called Value Function Propagation (VFP), provided two  orthogonal improvements of OC-DEC-MDP: (i) It speeded up  OC-DECMDP by an order of magnitude by maintaining and manipulating a value function for each method rather than a separate value for each pair of method and time interval, and (ii) it achieved better solution qualities than OC-DEC-MDP because it corrected the  overestimation of the opportunity cost of OC-DEC-MDP.
Decentralized Markov Decision Process (DEC-MDP) has been very popular for modeling of agent-coordination problems, it is very  difficult to solve, especially for the real-world domains.
Beyond OC-DEC-MDP, there are other locally optimal algorithms for DEC-MDPs and  DECPOMDPs [8] [12], [13], yet, they have traditionally not dealt with uncertain execution times and temporal constraints.
Finally, value function techniques have been studied in context of single agent MDPs [7] [9].
However, similarly to [6], they fail to address the lack of global state knowledge, which is a fundamental issue in decentralized planning.
Furthermore, as discussed in Section 4, there are globally optimal algorithms for solving DEC-MDPs with temporal constraints [1] [11].
Acknowledgments This material is based upon work supported by the DARPA/IPTO COORDINATORS program and the Air Force Research  Laboratory under Contract No.
In terms of related work, we have extensively discussed the  OCDEC-MDP algorithm [4].
Unfortunately, they fail to scale up to large-scale domains at present time.
The authors also want to thank Sven Koenig and anonymous reviewers for their valuable comments. 
