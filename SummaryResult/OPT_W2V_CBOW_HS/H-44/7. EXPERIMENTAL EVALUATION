Index size continues to reduce on both datasets, as we increase the value of .
We also measured wall-clock times on a sample of the queries with results indicating improvements in execution time by up to a factor of 12. 
For both the WIKI and UKGOV datasets, we compare temporally coalesced indexes for different values of the error threshold computed using Algorithm 1 with the non-coalesced index as a baseline.
In order to evaluate this aspect, we compared the top-k results computed using a coalesced index against the ground-truth result obtained from the original index, for  different cutoff levels k. Let Gk and Ck be the top-k documents from the ground-truth result and from the coalesced index respectively.
iii) The SB approach achieves close-to-optimal performance on both datasets, if allowed to consume at most three times the optimal amount of space (i.e., κ = 3.0), which on our datasets still corresponds to a space reduction over Popt by more than one order of magnitude.
This large dataset, totaling 0.7 TBytes, contains the full editing history of the English Wikipedia from January 2001 to December 2005 (the time of our  download).
The required space S(M), as defined earlier, is equal to the total number of postings in the materialized sublists.
The second dataset used in our experiments was based on a subset of the European Archive [13], containing weekly crawls of 11 .gov.uk websites throughout the years 2004 and 2005 amounting close to 2 TBytes of raw data.
Figure 3 plots, for cutoff levels 10 and 100, the mean of RR@k and KT@k along with 5% and 95% percentiles, for different values of the threshold starting from 0.01.
We built a time-travel query workload using the query log temporarily made available recently by AOL  Research as follows - we first extracted the 300 most frequent keyword queries that yielded a result click on a Wikipedia  article (for e.g., french revolution, hurricane season 2005, da vinci code etc.).
WIKI UKGOV # Postings Ratio # Postings Ratio - 8,647,996,223 100.00% 7,888,560,482 100.00% 0.00 7,769,776,831 89.84% 2,926,731,708 37.10% 0.01 1,616,014,825 18.69% 744,438,831 9.44% 0.05 556,204,068 6.43% 259,947,199 3.30% 0.10 379,962,802 4.39% 187,387,342 2.38% 0.25 252,581,230 2.92% 158,107,198 2.00% 0.50 203,269,464 2.35% 155,434,617 1.97% Table 1: Index sizes for non-coalesced index (-) and coalesced indexes for different values of Table 1 summarizes the index sizes measured as the total number of postings.
For = 0.01, the smallest value of in our experiments, RR@100 for WIKI is 0.98 indicating that the results are -1 -0.5 0 0.5 1 ε 0.01 0.05 0.10 0.25 0.50 Relative Recall @ 10 (WIKI) Kendall's τ @ 10 (WIKI) Relative Recall @ 10 (UKGOV) Kendall's τ @ 10 (UKGOV) (a) @10 -1 -0.5 0 0.5 1 ε 0.01 0.05 0.10 0.25 0.50 Relative Recall @ 100 (WIKI) Kendall's τ @ 100 (WIKI) Relative Recall @ 100 (UKGOV) Kendall's τ @ 100 (UKGOV) (b) @100 Figure 3: Relative recall and Kendall"s τ observed on coalesced indexes for different values of almost indistinguishable from those obtained through the original index.
Note that EPC values are smaller on WIKI than on  UKGOV, since terms in the query workload employed for WIKI are relatively rarer in the corpus.
ii) For the PG method we see that for an acceptable performance degradation of only 10% (i.e., γ = 1.10) the required space drops by more than one order of magnitude in comparison to Popt on both datasets.
For both datasets, we started with the coalesced index produced by a moderate threshold setting of = 0.10.
7.2 Impact of Temporal Coalescing Our first set of experiments is aimed at evaluating the  approximate temporal coalescing technique, described in  Section 5, in terms of index-size reduction and its effect on the result quality.
For each extracted query, we randomly picked a time point for each month covered by the dataset.
We conducted a comprehensive series of experiments on two real-world datasets to evaluate the techniques proposed in this paper.
In order to reduce the computational effort, boundaries of elementary time intervals were rounded to day granularity before  computing the sublist materializations.
We used the following two measures for  comparison: (i) Relative Recall at cutoff level k (RR@k), that measures the overlap between Gk and Ck, which ranges in [0, 1] and is defined as RR@k = |Gk ∩ Ck|/k .
In other words, the mean EPC reflects the expected length of the  index list (in terms of index postings) that needs to be scanned for a random time point and a random term from the query workload.
Note that on the UKGOV dataset, even accurate coalescing ( = 0) manages to reduce the index size to less than 38% of the original size.
We built a corresponding query workload as mentioned before, this time choosing  keyword queries that led to a site in the .gov.uk domain (e.g., minimum wage, inheritance tax , citizenship ceremony dates etc.
However, note that the postings in the materialized sublists still retain their  original timestamps.
On the relatively less dynamic UKGOV dataset (as can be seen from the σ values above), results were even better, with high values of RR and KT seen throughout the spectrum of values for both cutoff values.
(ii) Kendall"s τ (see [7, 14] for a detailed definition) at  cutoff level k (KT@k), measuring the agreement between two results in the relative order of items in Gk ∩ Ck, with value 1 (or -1) indicating total agreement (or disagreement).
Thus, we obtained a total of 7,200 (= 300 × 24) time-travel queries for the UKGOV dataset.
7.1 Setup and Datasets The techniques described in this paper were implemented in a prototype system using Java JDK 1.5.
), and randomly sampling a time point for every month within the two year period spanned by the dataset.
We filtered out documents not belonging to MIME-types text/plain and text/html, to obtain a dataset that totals 0.4 TBytes and which we refer to as UKGOV in rest of the paper.
It is reassuring to see from these results that approximate temporal coalescing induces minimal disruption to the query results, since RR@k and KT@k are within reasonable limits.
To assess performance we  compute the expected processing cost (EPC) for all terms in the respective query workload assuming a uniform  probability distribution among query time-points.
For a comparative evaluation of the four approaches - Popt, Sopt, PG, and SB - we measure space and performance as follows.
All  experiments described below were run on a single SUN V40z  machine having four AMD Opteron CPUs, 16GB RAM, a large network-attached RAID-5 disk array, and running Microsoft Windows Server 2003.
7.3 Sublist Materialization We now turn our attention towards evaluating the  sublist materialization techniques introduced in Section 6.
How does the reduction in index size affect the query  results?
The PG and SB methods, for different values of their respective parameter, produce  solutions whose space and performance lie in between the  extremes that Popt and Sopt represent.
For the extreme value of = 0.5, which results in an index size of just 2.35% of the original, the RR@100 and KT@100 are about 0.8 and 0.6 respectively.
Note that for = 0, results coincide with those obtained by the original index, and hence are omitted from the graph.
= 0.01, has a considerable effect by reducing the index size almost by an order of magnitude.
The collection statistics (i.e., N and avdl) and term  statistics (i.e., DF) were computed at monthly granularity for both datasets.
We indexed all encyclopedia articles excluding  versions that were marked as the result of a minor edit (e.g., the correction of spelling errors etc.).
Table 2 lists the obtained space and performance figures.
This yielded a total of 892,255 documents with 13,976,915 versions having a mean (µ) of 15.67 versions per document at standard deviation (σ) of 59.18.
For the PG approach, we varied its  parameter γ, which limits the maximal performance degradation, between 1.0 and 3.0.
The thus extracted queries contained a total of 422 distinct terms.
As these results demonstrate,  approximate temporal coalescing is highly effective in reducing  index size.
Solutions for the SB approach were obtained running simulated annealing for R = 50, 000 rounds.
All data and indexes are kept in an Oracle 10g database that runs on the same machine.
In total 522 terms appear in the extracted queries.
For our experiments we used two different datasets.
Analogously, for the SB approach the parameter κ, as an upper-bound on the allowed space blowup, was varied between 1.0 and 3.0.
We report the mean EPC, as well as the 5%- and 95%-percentile.
Even the relative order of these common results is quite high, as the mean KT@100 is close to 0.95.
i) As  expected, Popt achieves optimal performance at the cost of an enormous space consumption.
Sopt, to the contrary, while consuming an optimal amount of space, provides only poor expected processing cost.
Based on the depicted results, we make the following key observations.
This included a total of 502,617 documents with 8,687,108  versions (µ = 17.28 and σ = 13.79).
The English Wikipedia revision history (referred to as WIKI in the remainder) is available for free download as a single XML file.
This resulted in a total of 18, 000 (= 300 × 60) time-travel queries.
The Sopt and Popt approaches are, by their definition, parameter-free.
Even a small threshold value, e.g.
