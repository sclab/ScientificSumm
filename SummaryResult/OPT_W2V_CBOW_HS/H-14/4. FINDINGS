Subjects found the search easier on QuerySuggestion and QueryDestination than the other systems for known-item tasks.4 For exploratory tasks, only searches conducted on QueryDestination were easier than on the other systems.5 Subjects indicated that exploratory tasks on the three non-baseline systems were more stressful (i.e., less relaxing) than the  knownitem tasks.6 As we will discuss in more detail in Section 4.1.3, subjects regarded the familiarity of Baseline as a strength, and may have struggled to attempt a more complex task while learning a new interface feature such as query or destination suggestions.
4.1.4 Summary The findings obtained from our study on subjects" perceptions of the four systems indicate that subjects tend to prefer QueryDestination for the exploratory tasks and QuerySuggestion for the known-item searches.
Our findings also show that our subjects felt that QueryDestination produced more relevant and useful suggestions for exploratory tasks than the other systems.11 All other observed differences between the systems were not statistically significant.12 The difference between performance of QueryDestination and SessionDestination is explained by the approach used to generate destinations (described in Section 2).
On the other hand, QueryDestination was shown to lead to heightened perceptions of search success and task ease, clarity, and familiarity for the exploratory tasks.
4.3.3 Summary Analysis of log interaction data gathered during the study indicates that although subjects submitted fewer queries and clicked fewer search results on QueryDestination, their engagement with suggestions was highest on this system, particularly for exploratory search tasks.
The final questionnaire also included open-ended questions that asked subjects to explain their system ranking, and describe what they liked and disliked about each system: Baseline: Subjects who preferred Baseline commented on the familiarity of the system (e.g., was familiar and I didn"t end up using suggestions (S36)).
Subjects selected almost twice as many destinations per query when using QueryDestination compared to SessionDestination.24 As discussed earlier, this may be explained by the lower perceived relevance and usefulness of destinations recommended by SessionDestination.
In this section we use the data derived from the experiment to address our hypotheses about query suggestions and destinations, providing information on the effect of task type and topic familiarity where appropriate.
Table 4 presents the mean average response to these statements for each system and task type.
In addition, they were asked to complete three 5-point semantic differentials indicating their response to the attitude statement: The task we asked you to perform was: The paired stimuli offered as possible responses were clear/unclear, simple/complex, and familiar/ unfamiliar.
Measure Known-item Exploratory QS QD SD QS QD SD Usage 35.7 33.5 23.4 30.0 35.2 25.3 Results indicate that QuerySuggestion was used more for  knownitem tasks than SessionDestination22 , and QueryDestination was used more than all other systems for the exploratory tasks.23 For well-specified targets in known-item search, subjects appeared to use query refinement most heavily.
15 F(3,136) = 6.34, p = .001 16 F(1,136) = 18.95, p < .001 17 F(1,136) = 6.82, p = .028; Known-item tasks were also more simple on QS (F(3,136) = 3.93, p = .01; Tukey post-hoc test: p = .01); α = .167 Known-item Exploratory 0 100 200 300 400 500 600 Task categories Baseline QSuggest Time(seconds) Systems 348.8 513.7 272.3 467.8 232.3 474.2 359.8 472.2 QDestination SDestination As can be seen in the figure above, the task completion times for the known-item tasks differ greatly between systems.18 Subjects attempting these tasks on QueryDestination and QuerySuggestion complete them in less time than subjects on Baseline and SessionDestination.19 As discussed in the previous section, subjects were more familiar with the known-item tasks, and felt they were simpler and clearer.
Perceptions of search process (lower = better).
Those who did not prefer this system commented on the lack of specificity in the suggested domains (Should just link to site-specific query, not site itself (S16); Sites were not very specific (S24); Too general/vague (S28)14 ), and the quality of the suggestions (Not relevant (S11); Irrelevant (S6)).
Those who did not prefer this system criticized suggestion quality (e.g., Not relevant (S11); Popular 10 F(2,102) = 5.00, p = .009; Tukey post-hoc tests: all p ≤ .012 11 F(2,102) = 4.01, p = .01; α = .0167 12 Tukey post-hoc tests: all p ≥ .143 13 One-way repeated measures ANOVA: F(3,105) = 1.50, p = .22 queries weren"t what I was looking for (S18)) and the quality of results they led to (e.g., Results (after clicking on suggestions) were of low quality (S35); Ultimately unhelpful (S1)).
SessionDestination"s recommendations came from the end of users" session trails that often transcend multiple queries.
Scale Known-item Exploratory B QS QD SD B QS QD SD Queries 1.9 4.2 1.5 2.4 3.1 5.7 2.7 3.5 Result clicks 2.6 2 1.7 2.4 3.4 4.3 2.3 5.1 Subjects submitted fewer queries and clicked on fewer search results in QueryDestination than in any of the other systems.21 As 18 F(3,136) = 4.56, p = .004 19 Tukey post-hoc tests: all p ≤ .021 20 F(3,136) = 1.06, p = .37 21 Queries: F(3,443) = 3.99; p = .008; Tukey post-hoc tests: all p ≤ .004; Systems: F(3,431) = 3.63, p = .013; Tukey post-hoc tests: all p ≤ .011 discussed in the previous section, subjects using this system felt more successful in their searches yet they exhibited less of the traditional query and result-click interactions required for search success on traditional search systems.
8 F(3,136) = 4.07, p = .008; Tukey post-hoc tests: all p ≤ .002 9 QS: d(K,E) = (.26, .52); QD: d(K,E) = (.77, 1.50); SD: d(K,E) = (.48, .28) SessionDestination, although only for exploratory search tasks.10 Additional comments on QuerySuggestion conveyed that subjects saw it as a convenience (to save them typing a reformulation) rather than a way to dramatically influence the outcome of their search.
Scale / Differential Known-item Exploratory QS QD SD QS QD SD Effectiveness 2.7 2.5 2.6 2.8 2.3 2.8 CloseToGoal 2.9 2.7 2.8 2.7 2.2 3.1 Re-use 2.9 3 2.4 2.5 2.5 3.2 1 Relevant 2.6 2.5 2.8 2.4 2 3.1 2 Useful 2.6 2.7 2.8 2.7 2.1 3.1 3 Appropriate 2.6 2.4 2.5 2.4 2.4 2.6 All {1,2,3} 2.6 2.6 2.6 2.6 2.3 2.9 The results show that all three experimental systems improved subjects" perceptions of their search effectiveness over Baseline, although only QueryDestination did so significantly.8 Further examination of the effect size (measured using Cohen"s d) revealed that QueryDestination affects search effectiveness most positively.9 QueryDestination also appears to get subjects closer to their information goal (CloseToGoal) than QuerySuggestion or 4 easy: F(3,136) = 4.71, p = .0037; Tukey post-hoc tests: all p ≤ .008 5 easy: F(3,136) = 3.93, p = .01; Tukey post-hoc tests: all p ≤ .012 6 relaxing: F(1,136) = 6.47, p = .011 7 This question was conditioned on subjects" use of Baseline and their previous Web search experiences.
Scale Known-item Exploratory B QS QD SD B QS QD SD Success 2.0 1.3 1.4 1.4 2.8 2.3 1.4 2.6 1 Clear 1.2 1.1 1.1 1.1 1.6 1.5 1.5 1.6 2 Simple 1.9 1.4 1.8 1.8 2.4 2.9 2.4 3 3 Familiar 2.2 1.9 2.0 2.2 2.6 2.5 2.7 2.7 All {1,2,3} 1.8 1.4 1.6 1.8 2.2 2.2 2.2 2.3 Subject responses demonstrate that users felt that their searches had been more successful using QueryDestination for exploratory tasks than with the other three systems (i.e., there was a two-way interaction between these two variables).15 In addition, subjects perceived a significantly greater sense of completion with  knownitem tasks than with exploratory tasks.16 Subjects also found known-item tasks to be more simple, clear, and familiar.
Differential Known-item Exploratory B QS QD SD B QS QD SD Easy 2.6 1.6 1.7 2.3 2.5 2.6 1.9 2.9 Restful 2.8 2.3 2.4 2.6 2.8 2.8 2.4 2.8 Interesting 2.4 2.2 1.7 2.2 2.2 1.8 1.8 2 Relaxing 2.6 1.9 2 2.2 2.5 2.8 2.3 2.9 All 2.6 2 1.9 2.3 2.5 2.5 2.1 2.7 Each cell in Table 1 summarizes subject responses for 18  tasksystem pairs (18 subjects who ran a known-item task on Baseline (B), 18 subjects who ran an exploratory task on QuerySuggestion (QS), etc.).
4.2.3 Summary Analysis of subjects" perception of the search tasks and aspects of task completion shows that the QuerySuggestion system made subjects feel more successful (and the task more simple, clear, and familiar) for the known-item tasks.
4.3.2 Suggestion Usage To determine whether subjects found additional features useful, we measure the extent to which they were used when they were provided.
Consequently, the completion time increased slightly between these two systems perhaps as the subjects assessed the value of the proposed suggestions, but reaped little benefit from them.
The refined queries proposed by QuerySuggestion were used the most for the known-item tasks.
However, none of the differences between systems" ratings are significant.13 One possible explanation for these systems being rated higher could be that although the popular destination systems performed well for exploratory searches while QuerySuggestion performed well for known-item searches, an overall ranking merges these two performances.
4.2.1 Subject Perceptions In the post-search questionnaire, subjects were asked to indicate on a 5-point Likert scale the extent to which they agreed with the following attitude statement: I believe I have succeeded in my performance of this task (Success).
The value corresponding to the differential All represents the mean of all three differentials, providing an overall measure of subjects" feelings.
QueryDestination: Subjects who preferred this system commented mainly on support for accessing new information sources (e.g., provided potentially helpful and new areas / domains to look at (S27)) and bypassing the need to browse to these pages (Useful to try to ‘cut to the chase" and go where others may have found answers to the topic (S3)).
Systems Baseline QSuggest QDest SDest Ranking 2.47 2.14 1.92 2.31 These results indicate that subjects preferred QuerySuggestion and QueryDestination overall.
QuerySuggestion: Subjects who rated QuerySuggestion highest commented on rapid support for query formulation (e.g., was useful in (1) saving typing (2) coming up with new ideas for query expansion (S12); helps me better phrase the search term (S24); made my next query easier (S21)).
4.3.1 Queries and Result Clicks Searchers typically interact with search systems by submitting queries and clicking on search results.
4.2 Search Tasks To gain a better understanding of how subjects performed during the study, we analyze data captured on their perceptions of task completeness and the time that it took them to complete each task.
Over all tasks there appeared to be a slight preference for QueryDestination, but as other results show, the effect of task type on subjects" perceptions is significant.
However, when the task is more demanding, searchers appreciate suggestions that have the potential to dramatically influence the direction of a search or greatly improve topic coverage.
In the post-search questionnaires, we asked subjects to complete four 5-point semantic differentials indicating their responses to the attitude statement: The search we asked you to perform was.
4.1 Subject Perceptions In this section we present findings on how subjects perceived the systems that they used.
We applied two-way analysis of variance (ANOVA) to each differential across all four systems and two task types.
Perceptions of task and task success (lower = better).
Those who did not prefer this system disliked the lack of support for query formulation (Can be difficult if you don"t pick good search terms (S20)) and difficulty locating relevant documents (e.g., Difficult to find what I was looking for (S13); Clunky current technology (S30)).
Subjects generally felt that the recommendations offered by SessionDestination were of low relevance and usefulness.
Work is obviously needed in improving the quality of the suggestions in all systems, but subjects seemed to distinguish the settings when each of these systems may be useful.
4.1.1 Search Process To address the first research question wanted insight into subjects" perceptions of the search experience on each of the four systems.
4.2.2 Task Completion Time In addition to asking subjects to indicate the extent to which they felt the task was completed, we also monitored the time that it took them to indicate to the experimenter that they had finished.
The average value in each cell is computed for 18 subjects on each task type and system.
4.1.3 System Ranking In the final questionnaire that followed completion of all tasks on all systems, subjects were asked to rank the four systems in descending order based on their preferences.
However, more subjects commented on the irrelevance of the suggestions (e.g., did not seem reliable, not much help (S30); Irrelevant, not my style (S21), and the related need to include explanations about why the suggestions were offered (e.g., Low-quality results, not enough information presented (S35)).
Responses to post-search (per-system) and final questionnaires are used as the basis for our analysis.
For exploratory searches, users benefited more from being pointed to alternative information sources than from suggestions for iterative refinements of their queries.
Perceptions of system support (lower = better).
The average obtained differential values are shown in Table 1 for each system and each task type.
Since these tasks had no clearly defined termination criteria (i.e., the subject decided when they had gathered sufficient information), subjects generally spent longer searching, and consulted a broader range of information sources than in the known-item tasks.
As illustrated by the examples in Figure 3, the known-item tasks required subjects to retrieve a finite set of answers (e.g., find three interesting things to do during a weekend visit to Kyoto, Japan).
A stopwatch rather than system logging was used for this since we wanted to record the time regardless of system interactions.
We did not include these in the post-search questionnaire when subjects used the Baseline system as they refer to interface support options that Baseline did not offer.
The values for the three semantic differentials are included at the bottom of the table, as is their overall average under All.
This increases the likelihood that topic shifts adversely affect their relevance.
Given that there was no difference in the tasks attempted on each system, theoretically the perception of the tasks" simplicity, clarity, and familiarity should have been the same for all systems.
Suggestion usage is defined as the proportion of submitted queries for which suggestions were offered and at least one suggestion was clicked.
The following Likert scales and semantic differentials were used: • Likert scale A: Using this system enhances my effectiveness in finding relevant information.
However, we observe a clear interaction effect between the system and subjects" perception of the actual tasks.
The paired stimuli offered as responses were: relaxing/stressful, interesting/ boring, restful/tiring, and easy/difficult.
Table 6 shows the average usage for each system and task category.
In contrast, when subjects were exploring, they seemed to benefit most from the recommendation of additional information sources.
As well as eliciting feedback on each system from our subjects, we also recorded several aspects of their interaction with each system in log files.
This relative ranking reflects subjects" overall perceptions, but does not separate them for each task category.
Overall, subjects submitted most queries in QuerySuggestion, which is not surprising as this system actively encourages searchers to iteratively re-submit refined queries.
17 These responses confirm differences in the nature of the tasks we had envisaged when planning the study.
Suggestion uptake (values are percentages).
The end-point in such tasks was less well-defined and may have affected subjects" perceptions of when they had completed the task.
Figure 4 shows the average task completion time for each system and each task type.
It may be the case that subjects" queries on this system were more effective, but it is more likely that they interacted less with the system through these means and elected to use the popular destinations instead.
14 Although the destination systems provided support for search within a domain, subjects mainly chose to ignore this.
(CloseToGoal) • Likert scale C: I would re-use the queries/destinations suggested if I encountered a similar task in the future (Re-use) • Semantic differential A: The queries/destinations suggested by the system were: relevant/irrelevant, useful/useless, appropriate/inappropriate.
Average query iterations and result clicks (per task).
Suggestions to incrementally refine the current query may be preferred by searchers on known-item tasks when they may have just missed their information target.
Table 2 presents the average responses for each of these scales and differentials, using the labels after each of the first three Likert scales in the bulleted list above.
The elapsed time from when the subject began issuing their first query until when they indicated that they were done was monitored using a stopwatch and recorded for later analysis.
Although our system offers additional interface affordances, we begin this section by analyzing querying and clickthrough behavior of our subjects to better understand how they conducted core search activities.
Even though all systems can at times offer irrelevant suggestions, subjects appeared to prefer having them rather than not (e.g., one subject remarked suggestions were helpful in some cases and harmless in all (S15)).
(Effectiveness)7 • Likert scale B: The queries/destinations suggested helped me get closer to my information goal.
In contrast, the exploratory tasks were multi-faceted, and required subjects to find out more about a topic or to find sufficient information to make a decision.
Table 5 shows the average number of query iterations and search results clicked for each system-task pair.
Subjects interacted similarly with Baseline and SessionDestination systems, perhaps due to the low quality of the popular destinations in the latter.
In this section, we analyze three interaction aspects: query iterations, search-result clicks, and subject engagement with the additional interface features offered by the three non-baseline systems.
The task completion times for the exploratory tasks were approximately equal on all four systems20 , although the time on Baseline was slightly higher.
Task completion times on both systems were significantly lower than on the other systems for known-item tasks.
SessionDestination: Subjects who preferred this system commented on the utility of the suggested domains (suggestions make an awful lot of sense in providing search assistance, and seemed to help very nicely (S5)).
Parametric statistical testing is used in this analysis and the level of significance is set to < 0.05, unless otherwise stated.
Table 3 presents the mean average rank assigned to each of the systems.
4.3 Subject Interaction We now focus our analysis on the observed interactions between searchers and systems.
Baseline may have taken longer than the other systems since users had no additional support and had to formulate their own queries.
Relative ranking of systems (lower = better).
These comments demonstrate a diverse range of perspectives on different aspects of the experimental systems.
There appears to be a clear division between the systems: QuerySuggestion was preferred for known-item tasks, while QueryDestination provided most-used support for exploratory tasks. 
To investigate this and related issues, we will next analyze usage of the suggestions on the three non-baseline systems.
The most positive response across all systems for each differential-task pair is shown in bold.
All Likert scales and semantic differentials used a 5-point scale where a rating closer to one signifies more agreement with the attitude statement.
4.1.2 Interface Support We solicited subjects" opinions on the search support offered by QuerySuggestion, QueryDestination, and SessionDestination.
Mean average task completion time (± SEM).
Table 5.
Table 4.
Table 6.
Table 3.
Table 2.
Table 1.
Figure 4.
