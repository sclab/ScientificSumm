We show that the maxflow-based  algorithm scales better than private history in the presence of colluders without the centralized trust required in previous work [9] [20].
We show that by having each peer keep a 102 private history of the actions of other peers toward her, and using discriminating server selection, the Reciprocative  decision function can scale to large populations and moderate levels of turnover.
This results in a higher level of cooperation than with  private history.
We show that if Reciprocative peers treat strangers (peers with no history) using a policy that adapts to the  behavior of previous strangers, peers have little incentive to whitewash and whitewashing can be nearly eliminated from the system.
Therefore, we propose a family of scalable and robust incentive  techniques, based upon a novel Reciprocative decision function, to  address these challenges and provide different tradeoffs: • Discriminating Server Selection: Cooperation requires  familiarity between entities either directly or indirectly.
In particular, we use a prisoners" dilemma model to capture the  essential tension between individual and social utility, asymmetric payoff matrices to allow asymmetric transactions between peers, and a learning-based [14] population dynamic model to specify the behavior of individual peers, which can be changed continuously.
The rest of the paper is organized as follows.
In the example in Figure 1, C can falsely claim that A served him, thus deceiving B into providing service.
Long-term history exacerbates this problem by  allowing peers with many previous transactions to exploit that  history for many new transactions.
In a wireless ad-hoc network, overall packet  latency and loss rate increase when nodes refuse to forward packets on behalf of others [26].
In many of these systems, users have natural disincentives to cooperate because cooperation consumes their own resources and may degrade their own performance.
While social dilemmas have been studied extensively, P2P  applications impose a unique set of challenges, including: • Large populations and high turnover: A file sharing  system such as Gnutella and KaZaa can exceed 100, 000  simultaneous users, and nodes can have an average life-time of the order of minutes [33].
However, the large populations and high turnover of P2P systems makes it less likely that repeat interactions will occur with a familiar entity.
We then proceed to the incentive techniques in Section 4.
The basic idea is that B would only believe C if C had already provided  service to B.
In Section 4.1, we describe the challenges of large populations and high turnover and show the effectiveness of discriminating server selection and shared history.
• Short-term History: History also creates the possibility that a previously well-behaved peer with a good reputation will turn traitor and use his good reputation to exploit other peers.
Avoiding this tragedy of the commons [18] requires incentives for cooperation.
In Section 4.4, we show how traitors disrupt cooperation and how short-term  history deals with them.
We adopt a game-theoretic approach in addressing this problem.
• Adaptive Stranger Policy: Zero-cost identities allows  noncooperating peers to escape the consequences of not  cooperating and eventually destroy cooperation in the system if not stopped.
Many peer-to-peer (P2P) systems rely on cooperation among  selfinterested users.
In the example in Figure 1, A wants service from B, B wants service from C, and C wants service from A.
We show that a maxflow-based algorithm that computes reputation subjectively promotes cooperation despite collusion among 1/3 of the population.
For example, in a file-sharing system, overall download latency and failure rate increase when users do not share their resources [3].
We describe the model in Section 2 and the reciprocative decision function in Section 3.
As a result, each user"s  attempt to maximize her own utility effectively lowers the overall A BC Figure 1: Example of asymmetry of interest.
A wants service from B, B wants service form C, and C wants service from A. utility of the system.
In Section 4.3, we present the problem of zero-cost identities and show how an  adaptive stranger policy promotes persistent identities.
• Shared History: Scaling to higher turnover and mitigating asymmetry of interest requires shared history.
We show that with shared history, B knows that A served C and consequently will serve A.
To eliminate this cost, we have developed a constant mean running time variation, which trades effectiveness for  complexity of computation.
We show that short-term  history prevents traitors from disrupting cooperation.
• Asymmetry of interest: Asymmetric transactions of P2P systems create the possibility for asymmetry of interest.
• Zero-cost identity: Many P2P systems allow peers to  continuously switch identities (i.e., whitewash).
The cost of the maxflow algorithm is its O(V 3 ) running time, where V is the number of nodes in the system.
In Section 4.2, we describe collusion and  demonstrate how subjective reputation mitigates it.
The adaptive stranger policy does this without requiring centralized allocation of identities, an entry fee for newcomers, or rate-limiting [13] [9] [25].
The peer could be making a strategic decision or someone may have hijacked her identity (e.g., by compromising her host).
Further examples are file preservation [25], discussion boards [17], online auctions [16], and overlay routing [6].
However, if everyone keeps only private history, no one will provide service because B does not know that A has served C, etc.
• Maxflow-based Subjective Reputation: Shared history  creates the possibility for collusion.
Strategies that work well in traditional prisoners" dilemma games such as Tit-for-Tat [4] will not fare well in the P2P context.
The cost of shared history is a distributed  infrastructure (e.g., distributed hash table-based storage) to store the history.
We discuss related work in Section 5 and conclude in Section 6. 
If everyone provides service, then the system operates optimally.
Consider the example in Figure 1.
