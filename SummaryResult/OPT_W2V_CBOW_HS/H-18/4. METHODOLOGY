The two steps are: (1) based on current term  clustering Cluˆt, for each document d, the algorithm segments all the sentences Sd into p segments sequentially (some  segments may be empty), and put them into the p segments ˆS of the whole training set D (all possible cases of different segmentation Segd and alignment Alid are checked) to find the optimal case, and (2) based on the current segmentation and alignment, for each term t, the algorithm finds the best term cluster of t based on the current segmentation Segd and alignment Alid.
For the initial alignment Ali(0) , we can first assume that the order of segments for each d is the same.
For multi-document  segmentation without term clustering and term weight estimation (|D| > 1, k = l, w = 0), only iteration of Step 2.2 and 2.3 are required.
In the case of topic segmentation, the two random  variables are the term variable T and the segment variable S, and each sample is an occurrence of a term T = t in a particular segment S = s. I(T; S) is used to measure how dependent T and S are.
Or we can find the optimal segmentation just for each document d which maximizes the WMI, Seg (0) d = argmaxˆsIw(T; ˆS), where w = w (0) ˆt .
After a document is segmented into sentences Input: Joint probability distribution P(D, Sd, T), number of text segments p ∈ {2, 3, ..., max(sd)}, number of term clusters k ∈ {2, 3, ..., l} (if k = l, no term co-clustering required), and weight type w ∈ {0, 1}, indicating to use I or Iw, respectively.
If |D| = 1, k = l, and w = 0, check all sequential  segmentations of d into p segments and find the best one Segd(s) = argmaxˆsI( ˆT; ˆS), and return Segd; otherwise, if w = 1 and k = l, go to 3.1; Stage 2: 2.1 If k < l, for each term t, find the best cluster ˆt as Clu(i+1)(t) = argmaxˆtI( ˆT; ˆS(i)) based on Seg(i) and Ali(i); 2.2 For each d, check all sequential segmentations of d into p segments with mapping s → ˆs → ˆs, and find the best one Ali (i+1) d (Seg (i+1) d (s)) = argmaxˆsI( ˆT(i+1); ˆS) based on Clu(i+1)(t) if k < l or Clu(0)(t) if k = l; 2.3 i + +.
Simultaneously finding a good term clustering and estimated term weights is impossible, since when moving a term to a new term cluster to maximize Iw( ˆT; ˆS), we do not know that the weight of this term should be the one of the new cluster or the old cluster.
There are two cases that are not shown in the algorithm in Figure 2: (a) single-document  segmentation or multi-document segmentation with the same  sequential order of segments, where alignment is not required, and (b) multi-document segmentation with different  sequential orders of segments, where alignment is necessary.
First, a good guess of term weights can be made by using the distributions of term frequency along sentences for each document and averaging them to get the initial values of wˆt: wt = ( ED(t) maxt ∈T (ED(t )) )(1 − ES(t) maxt ∈T (ES(t )) ), (6) where ES(t) = 1 |Dt| d∈Dt (1 − s∈Sd p(s|t)log|Sd| 1 p(s|t) ), where Dt is the set of documents which contain Term t. Then, for the initial segmentation Seg(0) , we can simply segment documents equally by sentences.
Thus, instead of minimizing the loss of MI, we can maximize MI after topic segmentation, computed as: I( ˆT; ˆS) = ˆt∈ ˆT ˆs∈ ˆS p(ˆt, ˆs)log p(ˆt, ˆs) p(ˆt)p(ˆs) , (2) where p(ˆt, ˆs) are estimated by the term frequency tf of Term Cluster ˆt and Segment ˆs in the training set D. Note that here a segment ˆs includes sentences about the the same topic among all documents.
If Clu, Seg, or Ali changed, go to 2.1; otherwise, if w = 0, return Clu(i), Seg(i), and Ali(i); else j = 0, go to 3.1; Stage 3: 3.1 Update w (i+j+1) ˆt based on Seg(i+j), Ali(i+j), and Clu(i) using Equation (3); 3.2 For each d, check all sequential segmentations of d into p segments with mapping s → ˆs → ˆs, and find the best one Ali (i+j+1) d (Seg (i+j+1) d (s)) = argmaxˆsIw( ˆT(i); ˆS) based on Clu(i) and w (i+j+1) ˆt ; 3.3 j + +.
The alignment mapping function of the former case is simply just Alid(ˆsi) = ˆsi, while for the latter one"s alignment mapping function Alid(ˆsi) = ˆsj, i and j may be different.
(3) Finally, M(snd , p, kp) = maxi,j[M(si−1, p − 1, kp/j) +PIw( ˆT; ˆsAlid(ˆsL )=j(si, si+1, ..., snd ))], where kp is just the combination of all p segments and 1 ≤ i ≤ nd + 1, which is the optimal Iw and the corresponding segmentation is the best.
The  computational steps for the two cases are listed below: Case 1 (no alignment): For each document d: (1) Compute pw(ˆt), partial pw(ˆt, ˆs) and partial pw(ˆs)  without counting sentences from d. Then put sentences from i to j into Part k, and compute partial WMI PIw( ˆT; ˆsk(si, si+1, ..., sj)) ˆt∈ ˆT pw(ˆt, ˆsk)log pw(ˆt, ˆsk) pw(ˆt)pw(ˆsk) , where Alid(si, si+1, ..., sj) = k, k ∈ {1, 2, ..., p}, 1 ≤ i ≤ j ≤ nd, and Segd(sq) = ˆsk for all i ≤ q ≤ j.
Term cluster weights are used to adjust p(ˆt, ˆs), pw(ˆt, ˆs) = wˆtp(ˆt, ˆs) ˆt∈ ˆT ;ˆs∈ ˆS wˆtp(ˆt, ˆs) , (4) and Iw( ˆT; ˆS) = ˆt∈ ˆT ˆs∈ ˆS pw(ˆt, ˆs)log pw(ˆt, ˆs) pw(ˆt)pw(ˆs) , (5) where pw(ˆt) and pw(ˆs) are marginal distributions of pw(ˆt, ˆs).
The optimal co-clustering is the mapping Clux : X → ˆX and Cluy : Y → ˆY that minimizes the loss: I(X; Y ) − I( ˆX; ˆY ), which is equal to maximizing I( ˆX; ˆY ).
4.2 Weighted Mutual Information In topic segmentation and alignment of multiple  documents, if P(D, ˆS, T) is known, based on the marginal  distributions P(D|T) and P( ˆS|T) for each term t ∈ T, we can categorize terms into four types in the data set: • Common stop words are common both along the  dimensions of documents and segments.
First, basic items of probability for computing Iw are computed excluding Doc d, and then partial WMI by putting every possible sequential segment (including empty segment) of d into every segment of the set.
combinations of L segments chosen from all p segments, j ∈ kL, the set of L segments chosen from all p segments, and kL/j is the combination of L − 1 segments in kL except Segment j.
After finding a good term clustering, term weights are estimated if w = 1.
If Iw( ˆT; ˆS) changes, go to step 6; otherwise, stop and return Clu(i), Seg(i+j), Ali(i+j), and w (i+j) ˆt ; Figure 2: Algorithm: Topic segmentation and  alignment based on MI or WMI.
However, I(T; S) cannot be  computed for documents before segmentation, since we do not have a set of S due to the fact that sentences of Document d, si ∈ Sd, is not aligned with other documents.
We introduce term weights (or term cluster weights) wˆt = ( ED(ˆt) maxˆt ∈ ˆT (ED(ˆt )) )a (1 − EˆS(ˆt) maxˆt ∈ ˆT (EˆS(ˆt )) )b , (3) where ED(ˆt) = d∈D p(d|ˆt)log|D| 1 p(d|ˆt) , EˆS(ˆt) = ˆs∈ ˆS p(ˆs|ˆt)log| ˆS| 1 p(ˆs|ˆt) , and a > 0 and b > 0 are powers to adjust term weights.
They are common along the dimension of documents only for the same segment, and they are not common along the dimensions of segments.
Then M(sm, L) = maxi[M(si−1, L − 1) + PIw( ˆT; ˆsL(si, ..., sm))], where 0 ≤ m ≤ nd, 1 < L < p, 1 ≤ i ≤ m + 1, and when i > m, no sentences are put into ˆsk when compute PIw (note PIw( ˆT; ˆs(si, ..., sm)) = 0 for single-document  segmentation).
4.3 Iterative Greedy Algorithm Our goal is to maximize the objective function, I( ˆT; ˆS) or Iw( ˆT; ˆS), which can measure the dependence of term  occurrences in different segments.
To reinforce the contribution of cue words in the MI computation, and simultaneously  reduce the effect of the other three types of words, similar as the idea of the tf-idf weight [22], we use entropies of each term along the dimensions of document D and segment ˆS, i.e.
The steps of Case 1 and 2 are similar, except in Case 2, alignment is considered in addition to segmentation.
However, since we do not know either the term weights or P(D, ˆS, T), we need to estimate them, but wˆt depends on p(ˆs|t) and ˆS, while ˆS and p(ˆs|t) also depend on wˆt that is still unknown.
Usually a = 1 and b = 1 as default, and maxˆt ∈ ˆT (ED(ˆt )) and maxˆt ∈ ˆT (EˆS(ˆt )) are used to normalize the entropy values.
• Document-dependent stop words that depends on the personal writing style are common only along the  dimension of segments for some documents.
Single document  segmentation without term clustering and term weight estimation (|D| = 1, k = l, w = 0) only requires Stage 1 (Step 1).
Second, the optimal sum of PIw for L segments and the leftmost m sentences, M(sm, L), is found.
For the case of two random variables, we have I(X; Y ) = x∈X y∈Y p(x, y)log p(x, y) p(x)p(y) , (1) Obviously, when random variables X and Y are  independent, I(X; Y ) = 0.
4.3.1 Initialization In Step 0, the initial term clustering Clut and topic  segmentation and alignment Segd and Alid are important to avoid local maxima and reduce the number of iterations.
Then M(sm, L, kL) = maxi,j[M(si−1, L − 1, kL/j) + PIw( ˆT; ˆsAlid(ˆsL )=j(si, si+1, ..., sm))], where 0 ≤ m ≤ nd, 1 < L < p, 1 ≤ i ≤ m + 1, kL ∈ Set(p, L), which is the set of all p!
4.1 Mutual Information MI I(X; Y ) is a quantity to measure the amount of  information which is contained in two or more random variables [8, 10].
Finally, the maximal WMI is found among  different sums of M(sm, p − 1) and PIw for Segment p. 
We present the iterative greedy algorithm in Figure 2 to find a local maximum of I( ˆT; ˆS) or Iw( ˆT; ˆS) with simultaneous term weight estimation.
The optimal solution is the mapping Clut : T → ˆT, Segd : Sd → ˆS , and Alid : ˆS → ˆS, which maximizes I( ˆT; ˆS).
The optimal Iw is found and the corresponding segmentation is the best.
Generally, first we do not know the estimate term weights, which depend on the optimal topic segmentation and alignment, and term clusters.
Case 2 (alignment required): For each document d: (1) Compute pw(ˆt), partial pw(ˆt, ˆs), and partial pw(ˆs), and PIw( ˆT; ˆsk(si, si+1, ..., sj)) similarly as Case 1.
At Stage 1, the global maximum can be found based on I( ˆT; ˆS) using dynamic programming in Section 4.4.
Thus, an iterative algorithm is required to estimate term weights wˆt and find the best  segmentation and alignment to optimize the objective function Iw concurrently.
This is the criterion of MI for clustering.
This algorithm can is iterative and greedy for multi-document cases or single-document cases with term weight estimation and/or term co-clustering.
• Noisy words are other words which are not common along both dimensions.
Thus, we first do term clustering at Stage 2, and then estimate term weights at Stage 3.
At Stage 2, Step 2.1 is to find the best term clustering and Step 2.2 is to find the best segmentation.
Initialize Clu (0) t , Seg (0) d , and Ali (0) d ; Initialize w (0) ˆt using Equation (6) if w = 1; Stage 1: 1.
A cue word usually has a large value of ED(ˆt) but a small value of EˆS(ˆt).
For the initial term clustering Clu(0) , first cluster labels can be set randomly, and after the first time of Step 3, a good initial term clustering is obtained.
• Cue words are the most important elements for  segmentation.
At Stage 3, similar as Stage 2, Step 3.1 is term weight re-estimation and Step 3.2 is to find a better segmentation.
Output: Mapping Clu, Seg, Ali, and term weights wˆt.
Otherwise, since it is just a one step algorithm to solve the task of single-document  segmentation [6, 15, 25], the global maximum of MI is guaranteed.
(3) Finally M(snd , p) = maxi[M(si−1, p − 1)+ PIw( ˆT; ˆsp(si, ..., snd ))], where 1 ≤ i ≤ nd+1.
If term weight estimation is required (w = 1), Stage 3 (Step 3.1, 3.2, and 3.3) is  executed iteratively.
For Stage 1, using  dynamic programming can still find the global optimum, but for Stage 2 and Stage 3, we can only find the optimum for each step of topic segmentation and alignment of a  document.
(2) Let M(sm, 1, k) = PIw( ˆT; ˆsk(s1, s2, ..., sm)), where k ∈ {1, 2, ..., p}.
Entropy based on P(D|T) and P( ˆS|T) can be used to  identify different types of terms.
We will show later that term co-clustering reduces the  accuracy of the results and is not necessary, and for  singledocument segmentation, term weights are also not required.
and each sentence is segmented into words, each word is stemmed.
ED(ˆt) and EˆS(ˆt), to compute the weight.
However, if the term clustering in Stage 2 is not accurate, then the term weight estimation at Stage 3 may have a bad result.
If both are required (k < l, w = 1), Stage 2 and 3 run one after the other.
We now describe a novel algorithm which can handle  singledocument segmentation, shared topic detection, and  multidocument segmentation and alignment based on MI or WMI.
If term clustering is required (k < l), Stage 2 (Step 2.1, 2.2, and 2.3) is executed iteratively.
(2) Let M(sm, 1) = PIw( ˆT; ˆs1(s1, s2, ..., sm)).
Then the joint probability distribution P(D, Sd, T) can be estimated.
Here we only show the dynamic programming for Step 3.2 using WMI (Step 1 and 2.2 are similar but they can use either I or Iw).
It also can detect shared topics among documents by checking the proportion of overlapped sentences on the same topics, as described in Sec 5.2.
This algorithm can handle both single-document and multi-document  segmentation.
Thus, intuitively, the value of MI  depends on how random variables are dependent on each other.
Moreover, this problem is NP-hard [10], even though if we know the term weights.
Totally there are eight cases, |D| = 1 or |D| > 1, k = l or k < l, w = 0 or w = 1.
4.4 Algorithm Optimization In many previous works on segmentation, dynamic  programming is a technique used to maximize the objective function.
This cycle is repeated to find a local maximum based on MI I until it  converges.
Finally, this distribution can be used to compute MI in our algorithm.
Thus, an iterative greedy algorithm is  desired to find the best solution, even though probably only local maxima are reached.
Finally, at Step 3.3, this  algorithm converges and return the output.
They are repeated to find a local maximum based on WMI Iw until it converges.
4.3.2 Different Cases After initialization, there are three stages for different cases.
Similarly, at Step 1, 2.2, and 3.2 of our algorithm, we can use dynamic programming.
L!(p−L)!
Initialization: 0. i = 0.
