Thus, the extension of topic  segmentation from single documents to identifying similar segments from multiple similar documents with the same topic is a natural and necessary direction, and multi-document topic segmentation is expected to have a better performance since more information is utilized.
Many methods based on sentence similarity require that these words are removed before topic segmentation can be performed [15].
We also introduce the new criterion of WMI based on term weights learned from  multiple similar documents, which can improve performance of topic segmentation further.
The MI focuses on  measuring the difference among segments whereas previous research focused on finding the similarity (e.g.
These words are common in a document.
Topic segmentation tasks usually fall into two  categories [15]: text stream segmentation where topic transition is identified, and coherent document segmentation in which documents are split into sub-topics.
In Section 5, experiments about single-document segmentation, shared topic detection, and multi-document segmentation are described, and results are presented and discussed to evaluate the performance of our algorithm.
Not only can this approach increase the contribution of cue words, but it can also decrease the effect of common stop words, noisy word, and document-dependent stop words.
In this paper, we view the problem of topic segmentation as an optimization issue using information theoretic  techniques to find the optimal boundaries of a document given the number of text segments so as to minimize the loss of mutual information (MI) (or a weighted mutual information (WMI)) after segmentation and alignment.
The rest of this paper is organized as follows: In Section 2, we review related work.
Also, we have addressed the problem of topic  segmentation and alignment across multiple documents, whereas most existing research focused on segmentation of single documents.
Traditional approaches perform topic segmentation on  documents one at a time [15, 25, 6].
The major contribution of this paper is that it introduces a novel method for topic segmentation using MI and shows that this method performs better than previously used  criteria.
The former category has applications in automatic speech recognition, while the latter one has more applications such as partial-text query of long documents in information retrieval, text summary, and quality measurement of multiple documents.
Second, even though stop words can be identified, hard classification of stop words and  nonstop words cannot represent the gradually changing amount of information content of each word.
First, identifying stop words is another  issue [12] that requires estimation in each domain.
Traditional approaches using similarity measurement based on term frequency generally have the same assumption that similar vocabulary tends to be in a coherent topic segment [15, 25, 6].
Conclusions and some future directions of the research work are discussed in Section 6. 
Most of them perform badly in subtle tasks like coherent document segmentation [15].
In Section 4, we first propose the iterative greedy algorithm of topic segmentation and alignment with term co-clustering, and then describe how the algorithm can be optimized by  usFigure 1: Illustration of multi-document  segmentation and alignment.
Removing common stop words may result in the loss of useful  information in a specific domain.
Previous research in connection with TDT falls into the former  category, targeted on topic tracking of broadcast speech data and newswire text, while the latter category has not been studied very well.
Topic alignment of multiple similar documents can be achieved by clustering sentences on the same topic into the same cluster.
It can detect shared topics among documents to judge if they are multiple documents on the same topic.
Many researchers have worked on topic detection and  tracking (TDT) [26] and topic segmentation during the past decade.
Obviously, our approach can handle single documents as a special case when multiple documents are unavailable.
Inspired by this, we give each term (or term cluster) a weight based on entropy among different documents and different segments of  documents.
Terms can be co-clustered as in [10] at the same time, given the number of clusters, but our experimental results show that this method results in a worse segmentation (see Tables 1, 4, and 6).
Usually, human readers can identify topic transition based on cue words, and can ignore stop words.
Section 3 contains a formulation of the problem of topic segmentation and alignment of multiple documents with term co-clustering, a review of the criterion of MI for clustering, and finally an introduction to WMI.
However, they usually suffer the issue of  identifying stop words.
At a finer granularity, users may  actually be looking to obtain sections of a document similar to a particular section that presumably discusses a topic of the users interest.
Multi-document segmentation and alignment can utilize information from similar documents and improves the performance of topic segmentation greatly.
For example, additional document-dependent stop words are removed together with the generic stop words in [15].
Topic segmentation intends to identify the boundaries in a document with the goal to capture the latent topical  structure.
Often, end-users seek documents that have the  similar content.
Our results in Figure 3 show that term weights are useful for multi-document topic segmentation and alignment.
There are two reasons that we do not remove stop words directly.
We employ a soft  classification using term weights.
Single-document topic segmentation is just a special case of the multi-document topic segmentation and alignment problem.
Some of our prior work is in [24].
We propose an iterative greedy algorithm based on dynamic programming and show that it works well in practice.
cosine distance) of segments [15, 25, 6].
Search engines, like, Google, provide links to obtain similar pages.
This is equal to maximizing the MI (or WMI).
ing dynamic programming.
