The Next Generation Air Transportation Systems (NGATS) initiative aims to address this issues and, not only account for a threefold increase in traffic, but also for the increasing heterogeneity of aircraft and decreasing restrictions on flight paths.
An adaptive, multi-agent approach is an ideal fit to this naturally distributed problem where the complex interaction among the aircraft, airports and traffic controllers renders a pre-determined centralized solution severely suboptimal at the first deviation from the expected plan.
In this work, we explore four different agent reward functions, and compare them to simulating various changes to the system and selecting the best solution (e.g,  equivalent to a Monte-Carlo search).
In a reinforcement learning approach, the selection of the agent reward has a large impact on the performance of the system.
There is therefore a strong need to explore new, distributed and adaptive solutions to the air flow control problem.
Unlike many other flow problems where the increasing traffic is to some extent absorbed by improved hardware (e.g., more servers with larger memories and faster CPUs for internet routing) the air traffic domain needs to find mainly algorithmic  solutions, as the infrastructure (e.g., number of the airports) will not change significantly to impact the flow problem.
Because aircraft flight plans consist of a sequence of fixes, this  representation allows localized fixes (or agents) to have direct impact on the flow of air traffic1 .
Though a truly distributed and adaptive solution (e.g., free flight where  aircraft can choose almost any path) offers the most potential in terms of optimizing flow, it also provides the most  radical departure from the current system.
Finally, in Section 5, we discuss the  implications of these results and provide and map the required work to enable the FAA to reach its stated goal of increasing the traffic volume by threefold. 
The first explored reward consisted of the system reward.
The main contribution of this paper is to present a  distributed adaptive air traffic flow management algorithm that can be readily implemented and test that algorithm using FACET.
In this approach, the agents" actions are to set the separation that approaching aircraft are required to keep.
As a consequence, a shift to such a system presents tremendous difficulties both in terms of implementation (e.g., scheduling and airport  capacity) and political fallout (e.g., impact on air traffic  controllers).
In Section 4 we present results in domains with one and two congestions, explore different trade-offs of the system objective function, discuss the scaling properties of the different agent rewards and  discuss the computational cost of achieving certain levels of performance.
This simple agent-action pair allows the agents to slow down or speed up local traffic and allows agents to a have significant impact on the overall air traffic flow.
In order to efficiently and safely route this air traffic, current traffic flow control relies on a centralized,  hierarchical routing strategy that performs flow projections ranging from one to six hours.
In Section 3, we present the agent-based approach, focusing on the selection of the agents and their action space along with the agents" learning algorithms and reward structures.
The efficient, safe and reliable management of our ever increasing air traffic is one of the fundamental challenges facing the aerospace industry today.
The last two rewards were personalized rewards based on estimations to lower the computational burden of the  reward computation.
Though our approach comes from the second category, we aim to bridge the gap by using FACET to test our algorithms, a simulator introduced and widely used (i.e., over 40 organizations and 5000 users) by work in the first category [4, 11].
Furthermore, as the traffic flow increases, the current  procedures increase the load on the system, the airports, and the air traffic controllers (more aircraft per region)  without providing any of them with means to shape the traffic patterns beyond minor reroutes.
In Section 2, we describe the air traffic flow problem and the simulation tool, FACET.
In this paper, we focus on agent based system that can be implemented readily.
As a consequence, the system is slow to respond to developing weather or airport conditions leading potentially minor local delays to cascade into large regional congestions.
All three personalized rewards aim to align agent rewards with the system reward and ensure that the rewards remain sensitive to the agents" actions.
Previous work in this domain fell into one of two distinct categories: The first principles based modeling approaches used by domain experts [5, 8, 10, 13] and the algorithmic approaches explored by the learning and/or agents  community [6, 9, 12].
Agents learn the most appropriate separation for their location using a reinforcement learning (RL) algorithm [15].
On a typical day, more than 40,000 commercial flights operate within the US airspace [14].
The second reward was a personalized agent reward based on collectives [3, 17, 18].
In 2005, weather, routing decisions and airport conditions caused 437,667 delays, accounting for 322,272 hours of delays.
The total cost of these delays was estimated to exceed three billion dollars by industry [7].
In this approach, we assign an 342 978-81-904262-7-5 (RPS) c 2007 IFAAMAS agent to a fix, a specific location in 2D.
