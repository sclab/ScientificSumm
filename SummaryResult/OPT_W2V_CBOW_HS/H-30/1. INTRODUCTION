In this work, we show how the model can be extended and used for query  expansion using a technique that we call latent concept expansion (LCE).
Most previous techniques, by default, generate terms  independently.
Recently, a Markov random field (MRF) model for  information retrieval was proposed that goes beyond the  simplistic bag of words assumption that underlies BM25 and the (unigram) language modeling approach to information  retrieval [20, 22].
Query expansion techniques have been well studied for various models in the past and have shown to significantly improve effectiveness in both the relevance feedback and pseudo-relevance feedback setting [12, 21, 28, 29].
In Section 2 we describe related query expansion approaches.
Therefore, by performing query expansion using the MRF model, we are able to study the dynamics between term dependence and query expansion.
First, LCE provides a mechanism for combining term  dependence with query expansion.
Finally, our proposed approach seamlessly provides a  mechanism for generating both single and multi-term concepts.
Previous query expansion techniques are based on bag of words models.
There have been several approaches that make use of generalized concepts, however such approaches were somewhat heuristic and done outside of the model [19, 28].
Section 3 provides an overview of the MRF model and  details our proposed latent concept expansion technique.
For this reason, there has been a strong interest in query expansion techniques.
The MRF model, however, has been shown to be highly effective across a number of tasks, including ad hoc retrieval [14, 16], named-page finding [16], and Japanese language web search [6].
Such  techniques are used to augment the original query to produce a representation that better reflects the underlying  information need.
Query expansion  techniques in the past have implicitly only made use of term occurrence features.
In Section 4 we evaluate our proposed model and analyze the results.
Until now, the model has been solely used for ranking  documents in response to a given query.
Our approach is both formally motivated and a natural  extension of the underlying model.
By using more robust feature sets, it is possible to produce better expansion terms that  discriminate between relevant and non-relevant documents better.
Most past term dependence models have failed to show consistent, significant improvements over unigram baselines, with few exceptions [8].
Next, as we will show, the MRF model allows arbitrary features to be used within the model.
Users of information retrieval systems are required to  express complex information needs in terms of Boolean  expressions, a short list of keywords, a sentence, a question, or possibly a longer narrative.
The remainder of this paper is laid out as follows.
The MRF model generalizes the unigram, bigram, and other various dependence models [14].
Finally, Section 5 concludes the paper and  summarizes the major results. 
A great deal of information is lost during the process of translating from the information need to the actual query.
There are three primary contributions of our work.
