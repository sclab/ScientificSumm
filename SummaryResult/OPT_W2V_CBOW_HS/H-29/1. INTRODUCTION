We also perform model combination using several improved feedback language models obtained by a small number of new queries sampled from the original query.
Figure 1: Estimating the uncertainty of the feedback model for a single query. 
With this in mind, we wish to treat many important quantities calculated by the  retrieval system, whether a relevance score for a document, or a weight for a query expansion term, as random  variables whose true value is uncertain but where the  uncertainty about the true value may be quantified by replacing the fixed value with a probability distribution over possible values.
The use of an expectation is reasonable for practical and  theoretical reasons, but by itself ignores potentially valuable information about the risk of the feedback model.
Therefore, we propose a method for estimating uncertainty associated with an individual feedback model in terms of a posterior distribution over language models.
Next, a single feedback model vector is computed according to some sort of average,  centroid, or expectation over the set of possibly-relevant  document models.
To do this, we systematically vary the inputs to the baseline feedback method and fit a Dirichlet distribution to the output.
A model"s weight combines two complementary factors: the model"s probability of generating the query, and the variance of the model, with high-variance models getting lower weight.
Not only do we not know the queries that will be presented to our retrieval algorithm ahead of time, but the user"s  information need may be vague or incompletely specified by these queries.
For example, the document vectors may be combined with equal weighting, as in Rocchio, or by query likelihood, as may be done using the Relevance Model1 .
Our main hypothesis in this paper is that estimating the uncertainty in feedback is useful and leads to better  individual feedback models and more robust combined models.
Current algorithms for pseudo-relevance feedback (PRF) tend to follow the same basic method whether we use  vector space-based algorithms such as Rocchio"s formula [16], or more recent language modeling approaches such as  Relevance Models [10].
In this way, retrieval algorithms may attempt to quantify the risk or uncertainty associated with their  output rankings, or improve the stability or precision of their internal calculations.
As we show later, the mean and mode may vary significantly from the single feedback model proposed by the baseline method.
First, a set of top-retrieved documents is obtained from an initial query and assumed to approximate a set of relevant documents.
We use the posterior mean or mode as the improved feedback model estimate.
1 For example, an expected parameter vector conditioned on the query observation is formed from top-retrieved  documents, which are treated as training strings (see [10], p. 62).
Uncertainty is an inherent feature of information retrieval.
Even if the query were perfectly specified, language in the collection documents is inherently complex and ambiguous and matching such language effectively is a formidable problem by itself.
This process is shown in Figure 1.
