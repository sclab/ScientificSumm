On the document side, recent work by Zhou & Croft [21] explored the idea of adding noise to documents, re-scoring them, and using the stability of the resulting rankings as an estimate of query difficulty.
The essence of their method is that they allow skipping of some top-ranked documents, based on a clustering criterion, in order to select a more  varied and novel set of documents later in the ranking for use by a traditional pseudo-feedback method.
Ando and Zhang proposed a method that they call  structural feedback [3] and showed how to apply it to query  expansion for the TREC Genomics Track.
In a set of TREC topics they found wide average variation in the rank-distance of terms from different expansion  methods.
investigated combining terms from individual distributional methods using a term-reranking combination heuristic.
Our use of query variation was inspired by the work of YomTov et al.
(7) In the case H = 1, we have a single left singular vector φ: qexp = qorig + (φT qorig)φ so that the dot product φT qorig is a type of dynamic weight on the expanded query that is based on the similarity of the original query to the expanded query.
This is related to our use of document sampling to estimate the risk of the feedback model built from the different sets of top-retrieved  documents.
[17] proposed an approach to improving the robustness of pseudo-relevance feedback using a method they call selective sampling.
In particular, the studies of Amati et al.
This seems in accord with our own findings for individual feedback models.
In a series of simulations that simplified the problem to 2-feature documents, they found that average precision degrades as term frequency variance - high  noiseincreases.
Principal component analysis (PCA) is then applied to the ˆwi to obtain the matrix Φ of H left singular vectors φh that are used to obtain the new, expanded query qexp = qorig + ΦT Φqorig.
Finally, in language modeling approaches to feedback, Tao and Zhai [18] describe a method for more robust feedback that allows each document to have a different feedback α.
[11] used  queryspecific variance estimates of classifier outputs to perform improved model combination.
The feedback weights are derived automatically using  regularized EM.
Their combination method gave modest positive  improvements in average precision.
The idea of examining the overlap between lists of  suggested terms has also been used in early query expansion approaches.
The use of variance as a feedback model quality measure occurs indirectly through the application of PCA.
Our approach is related to previous work from several  areas of information retrieval and machine learning.
Xu and Croft"s method of Local Context  Analysis (LCA) [19] includes a factor in the empirically-derived weighting formula that causes expansion terms to be  preferred that have connections to multiple query terms.
Greiff, Morgan and Ponte [8] explored the role of variance in term weighting.
Estimates of output variance have recently been used for improved text classification.
and Carpineto et al.
Model combination is performed using heuristics.
These studies use the idea of creating multiple  subqueries and then examining the nature of the overlap in the documents and/or expansion terms that result from each subquery.
They  propose tailoring the stopping parameter η based on a function of some quality measure of feedback documents. 
Downweighting terms with high variance resulted in improved average precision.
It would be interesting to study the connections between this approach and our own  modelfitting method.
[5], and Amati et al.
Instead of using sampling, they were able to derive closed-form expressions for classifier variance by assuming base classifiers using simple types of inference networks.
Their study did not find significant improvements in either robustness (RI) or MAP on their corpora.
They used r query variations to obtain R different sets Sr of top-ranked  documents that have been intersected with the top-ranked  documents obtained from the original query qorig.
Lee et al.
Sakai et al.
A roughly equal balance of query and expansion model is implied by their EM stopping condition.
[20], Carpineto et al.
For each Si, the normalized centroid vector ˆwi of the documents is  calculated.
[2], among others.
