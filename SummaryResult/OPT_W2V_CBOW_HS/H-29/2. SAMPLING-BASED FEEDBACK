Our sample space is the set of all possible language  models LF that may be output as feedback models.
Second, sampling is an effective way to estimate basic properties of the feedback posterior distribution, which can then be used for improved model combination.
2.4 Visualizing feedback distributions Before describing how we fit and use the Dirichlet  distribution over feedback models, it is instructive to view some examples of actual feedback model distributions that result from bootstrap sampling the top-retrieved documents from different TREC topics.
Like the document scoring function f(D, Q), the feedback algorithm may implement a complex, non-linear scoring formula, and so as its inputs vary, the resulting feedback models may have a complex distribution over the space of feedback models (the sample space).
We treat the feedback algorithm as a black box and  assume that the inputs to the feedback algorithm are the  original query and the corresponding top-retrieved documents, with a score being given to each document.
Although the Dirichlet is a unimodal distribution, and in general quite limited in its expressiveness in the sample space, it is a  natural match for the multinomial language model, can be  estimated quickly, and can capture the most salient features of confident and uncertain feedback models, such as the overall spread of the distibution.
To incorporate feedback in the LM approach, we assume a model-based scheme in which our goal is take the query and resulting ranked documents DQ(k, C) as input, and output an expansion language model ˆθE, which is then interpolated with the original query model ˆθQ: ˆθNew = (1 − α) · ˆθQ + α · ˆθE (1) This includes the possibility of α = 1 where the original query mode is completely replaced by the feedback model.
foo2-401.map-Dim:5434,Size:12*12units,gaussianneighborhood (a) Topic 401 Foreign minorities, Germany foo2-402.map-Dim:5698,Size:12*12units,gaussianneighborhood (b) Topic 402 Behavioral genetics foo2-459.map-Dim:8969,Size:12*12units,gaussianneighborhood (c) Topic 459 When can a lender foreclose on property Figure 2: Visualization of expansion language model  variance using self-organizing maps, showing the distribution of language models that results from resampling the inputs to the baseline expansion method.
Single-term A single term is chosen from the original query.
Note that instead of treating each top document as equally likely, we sample according to the estimated probabilities of relevance of each document in DQ(k, C).
, αN }, setting each to αi = μ · p(wi | C), where μ is a parameter and p(· | C) is the collection language model estimated from a set of documents from collection C. The parameter fitting converges very quickly - typically just 2 or 2 Because our points are language models in the  multinomial simplex, we extended SOM-PAK to support  JensenShannon divergence, a widely-used similarity measure  between probability distributions.
2.1 Modeling Feedback Uncertainty Given a query Q and a collection C, we assume a  probabilistic retrieval system that assigns a real-valued document score f(D, Q) to each document D in C, such that the score is proportional to the estimated probability of relevance.
For example, using the Indri [12] query language, a  leave-oneout variant of the initial query that omits the term ‘ireland" for TREC topic 404 is: #weight(0.5 #combine(ireland peace talks) 0.5 #combine(peace talks)) 2.7 Combining enhanced feedback models from multiple query variants When using multiple query variants, the resulting  enhanced feedback models are combined using Bayesian model combination.
The nature of f(D, Q) may be complex: for example, if the retrieval  system supports structured query languages [12], then f(D, Q) may represent the output of an arbitrarily complex  inference network defined by the structured query operators.
Moreover, while random  sampling is useful in certain cases, it is perfectly acceptable to allow deterministic sampling distributions, but these must be designed carefully in order to approximate an accurate output variance.
Given the parameters of an N-dimensional Dirichlet  distribution Dir(α) the mean μ and mode x vectors are easy to calculate and are given respectively by μi = αiP αi (2) and xi = αi−1P αi−N .
The prior probability of a word"s membership in the relevant class is given by the probability of the original query in the entire enhanced expansion model. 
, αN }, we fit the Dirichlet parameters to the B  language model samples according to maximum likelihood  using a generalized Newton procedure, details of which are given in Minka [13].
We assume that the output of the feedback algorithm is a vector of term weights to be used to add or reweight the terms in the  representation of the original query, with the vector normalized to form a probability distribution.
The score of a document D with respect to a query Q and collection C is given by p(Q|D) with respect to language models ˆθQ and ˆθD estimated for the query and document respectively.
We view the the inputs to the feedback black box as random variables, and analyze the feedback model as a random variable that changes in  response to changes in the inputs.
The assumption is that one of the query terms is a noise term.
Bootstrap sampling allows us to simulate the approximate effect of perturbing the parameters within the black box feedback algorithm by perturbing the inputs to that  algorithm in a systematic way, while making no assumptions about the nature of the feedback algorithm.
For information retrieval, the number of samples we will have available is likely to be quite small for performance  reasons - usually less than ten.
For simplicity, we start by assuming the latent feedback  distribution has the form of a Dirichlet distribution.
In theory, the scoring function can vary from query to query, although in this study for simplicity we keep the scoring function the same for all queries.
The  assumption is that the given terms are a complete description of the information need.
To do this, we treat each word as an item to be classified as belonging to a relevant or non-relevant class, and derive a class probability for each word by combining the scores from each query variant.
Specifically, we sample k documents with replacement from DQ(k, C), and calculate an expansion language model θb  using the black box feedback method.
2.5 Fitting a posterior feedback distribution After obtaining feedback model samples by resampling the feedback model inputs, we estimate the feedback  distribution.
In practice, we can restrict the calculation to the vocabulary of the top-retrieved documents, instead of the entire collection.
Our  approach is to take samples from this space and then fit a distribution to the samples using maximum likelihood.
In such cases, the mode or mean of the feedback distribution often performs significantly better than the baseline (and in a smaller  proportion of cases, significantly worse).
Note that for this step we are re-using the existing retrieved documents and not performing additional queries.
For simplicity, we assume that queries and documents are  generated by multinomial distributions whose parameters are represented by unigram language models.
We repeat this process B times to obtain a set of B feedback language models, to which we then fit a Dirichlet distribution.
We selected three simple methods that cover complimentary assumptions about the query.
(3) We can then choose the language model at the mean or the mode of the posterior as the final enhanced feedback model.
In Sections 2.1-2.5 we describe a general method for  estimating a probability distribution over the set of possible language models.
Each variant corresponds to a different assumption about which aspects of the original query may be important.
The language model that would have been chosen by the baseline expansion is at the center of each map.
To accomplish this, we apply a widely-used simulation technique called bootstrap sampling ([7], p. 474) on the input parameters, namely, the set of top-retrieved documents.
This assumes that only one aspect of the query, namely, that represented by the term, is most important.
After generating a variant of the original query, we combine it with the original query using a weight αSUB so that we do not stray too ‘far".
We call this distribution over  possible feedback models the feedback model distribution.
3 iterations are enough - so that it is practical to apply at query-time when computational overhead must be small.
No-expansion Use only the original query.
For  example, a model may be weighted by its prediction confidence, estimated as a function of the variability of the posterior around the model.
Our goal in this section is to estimate a useful approximation to the feedback model distribution.
In our application, top-retrieved documents can be seen as a kind of noisy training set for relevance.
We assume that the multinomial feedback  models {ˆθ1, .
2.2 Resampling document models We would like an approximation to the posterior  distribution of the feedback model LF .
Leave-one-out A single term is left out of the original query.
In this respect, our approach resembles bagging [4], an ensemble approach which  generates multiple versions of a predictor by making bootstrap copies of the training set, and then averages the (numerical) predictors.
The term scores are weighted by the inverse of the variance of the term in the enhanced feedback model"s Dirichlet  distribution.
In Sections 2.6 and 2.7 we summarize how different query samples are used to generate multiple  feedback models, which are then combined.
Thus, a document is more likely to be chosen the higher it is in the ranking.
Each diagram is centered on the language model that would have been chosen by the baseline expansion.
Each point in our sample space is a language model, which typically has several thousand dimensions.
Each score is given by that term"s probability in the Dirichlet distribution.
Because of this potential complexity, we do not  attempt to derive a posterior distribution in closed form, but instead use simulation.
Our specific query method is given in Section 3.
First, we want to improve the quality of individual  feedback models by smoothing out variation when the baseline feedback model is unstable.
To help analyze the behavior of our method we used a Self-Organizing Map (via the SOM-PAK package [9]), to ‘flatten" and visualize the high-dimensional density function2 .
Also, while the distribution is usually close to the baseline feedback model, for some topics they are a significant distance apart (as measured by  JensenShannon divergence), as in Subfigure 2c.
2.6 Query variants We use the following methods for generating variants of the original query.
, ˆθB} were generated by a latent Dirichlet  distribution with parameters {α1, .
For a specific framework for experiments, we use the  language modeling (LM) approach for information retrieval [15].
2.3 Justification for a sampling approach The rationale for our sampling approach has two parts.
Typically B is in the range of 20 to 50 samples, with performance being relatively stable in this range.
This is a form of deterministic sampling.
We denote the set of k top-retrieved  documents from collection C in response to Q by DQ(k, C).
We assume a simple Dirichlet prior over the {α1, .
In this study, we set αSUB = 0.5.
The similarity function is  JensenShannon divergence.
A single peak (mode) is evident in some examples, but more complex structure appears in others.
We make no other assumptions about f(D, Q).
The density maps for three TREC topics are shown in Figure 2 above.
(We found the mode to give slightly better performance.)
The dark areas represent regions of high similarity between language models.
The light areas  represent regions of low similarity - the ‘valleys" between  clusters.
To estimate the {α1, .
We leave this for future study.
