In this section we present results confirming the usefulness of estimating a feedback model distribution from weighted resampling of top-ranked documents, and of combining the feedback models obtained from different small changes in the original query.
Also shown are the actual number of queries hurt by feedback (n−) for each method and collection.
Because we found that the baseline (Indri) expansion method performed better using a stopword list with the feedback model, all experiments used a stoplist of 419 common  English words.
We also examined the effect of the document sampling method on retrieval effectiveness, using two different  strategies.
When query variants are turned off and the  original query is used by itself with document sampling, there is little net change in average precision, a small decrease in P10 for 3 out of the 4 topic sets, but a significant drop in robustness for all topic sets.
If we turn off the stopword list, however, we obtain results such as those shown in Table 5 where four of the top ten baseline feedback terms for TREC topic 60 (said, but, their, not) are stopwords using the  BaseFB method.
Here, by ‘helped" we mean obtaining a higher average precision as a result of feedback.
Improvement shown for  BaseFB and RS-FB is relative to using no expansion.
This suggests that uniform weighting acts to increase variance in retrieval  results: when initial average precision is high, there are many relevant documents in the top k and uniform sampling may give a more representative relevance model than focusing on the highly-ranked items.
The x-axis gives the change in MAP over using baseline expansion with α = 0.5.
One obvious way to improve the worst-case performance of feedback is simply to use a smaller fixed α interpolation parameter, such as α = 0.3, placing less weight on the  (possibly risky) feedback model and more on the original query.
The reason for this effect appears to be the interaction of the term selection score with the top-m term cutoff.
Queries for which  initial average precision was negligible (≤ 0.01) were ignored, giving the remaining query count in column N. of −1.0, when all queries are hurt by the feedback method, to +1.0 when all queries are helped.
Indri"s method attempts to address the stopword  problem by applying an initial step based on Ponte [14] to  select less-common terms that have high log-odds of being in the top-ranked documents compared to the whole  collection.
For example, the baseline expanded query for topic 404 is: #weight(0.5 #combine(ireland peace talks) 0.5 #weight(0.10 ireland 0.08 peace 0.08 northern ...) 3.3 Expansion performance We measure our feedback algorithm"s effectiveness by two main criteria: precision, and robustness.
Collection DS + QV DS + No QV TREC 1&2 AvgP 0.2406 0.2547 (+5.86%) P10 0.5263 0.5362 (+1.88%) RI 0.7087 0.6515 (-0.0572) TREC 7 AvgP 0.2169 0.2200 (+1.43%) P10 0.4480 0.4300 (-4.02%) RI 0.5652 0.2609 (-0.3043) TREC 8 AvgP 0.2268 0.2257 (-0.49%) P10 0.4340 0.4200 (-3.23%) RI 0.4545 0.4091 (-0.0454) wt10g AvgP 0.1946 0.1865 (-4.16%) P10 0.2960 0.2680 (-9.46%) RI 0.1429 0.0220 (-0.1209) Table 3: Comparison of resampling feedback using  document sampling (DS) with (QV) and without (No QV)  combining feedback models from multiple query variants.
In the figure, the curve for each topic set interpolates between trade-off points, beginning at x=0, where α = 0.5, and continuing in the direction of the arrow as α decreases and the original query is given more and more weight.
In practice, however, because most term selection methods resemble a tf · idf type of weighting, terms with low idf but very high tf can sometimes be selected as expansion term candidates.
Finally, the term weights r(v) used in the expanded query are calculated based on the relevance model r(v) = X D p(q|D)p(v|D) p(v) p(D) (5) The quantity p(q|D) is the probability score assigned to the document in the initial retrieval.
(a) TREC 1&2 (upper curve); TREC 8 (lower curve) (b) TREC 7 (upper curve); wt10g (lower curve) Figure 3: The trade-off between robustness and average  precision for different corpora.
Indri"s expansion method first calculates a log-odds ratio o(v) for each potential expansion term v given by o(v) = X D log p(v|D) p(v|C) (4) over all documents D containing v, in collection C. Then, the expansion term candidates are sorted by descending o(v), and the top m are chosen.
Collection N Base-FB RS-FB n− RI n− RI TREC 1&2 103 26 +0.495 15 +0.709 TREC 7 46 14 +0.391 10 +0.565 TREC 8 44 12 +0.455 12 +0.455 wt10g 91 48 -0.055 39 +0.143 Combined 284 100 +0.296 76 +0.465 Table 2: Comparison of robustness index (RI) for baseline feedback (Base-FB) vs. resampling feedback (RS-FB).
Baseline FB p(wi|R) Resampling FB p(wi|R) said 0.055 court 0.026 court 0.055 pay 0.018 pay 0.034 federal 0.012 but 0.026 education 0.011 employees 0.024 teachers 0.010 their 0.024 employees 0.010 not 0.023 case 0.010 federal 0.021 their 0.009 workers 0.020 appeals 0.008 education 0.020 union 0.007 Table 5: Feedback term quality when a stoplist is not used.
First, the average precision of RS-FB was comparable to Base-FB, achieving an average gain of 17.6% compared to using no expansion across the four collections.
For each query, we obtained a set of B feedback models using the Indri baseline.
The results in Table 3 suggest that the model combination based on query variants may be largely account for the improved robustness.
In the example of  Table 5, resampling feedback leaves only one stopword (their) in the top ten.
Ideally, a robust feedback method would never perform worse than using the original query, while often performing better using the expansion.
While the presence and even  proportion of particular stopwords is fairly stable across different document samples, their relative position in the top-m list is not, as sets of documents with varying numbers of  better, lower-frequency term candidates are examined for each sample.
The ‘uniform weighting" strategy ignored the relevance scores from the initial retrieval and gave each document in the top k the same probability of selection.
Is it the use of document re-sampling, the use of multiple query variants, or some other factor?
Results from applying both the baseline  expansion method (Base-FB) and resampling expansion (RS-FB) are shown in Table 1.
3.6 The effect of resampling on expansion term quality Ideally, a retrieval model should not require a stopword list when estimating a model of relevance: a robust  statistical model should down-weight stopwords automatically depending on context.
We denote the Indri baseline feedback method as Base-FB.
As a result, while some number of stopwords may appear in each sampled document set, any given stopword tends to fall below the cutoff for multiple samples, leading to its classification as a high-variance, low-weight feature. 
On the other hand, when initial precision is low, there are few relevant documents in the bottom ranks and uniform sampling mixes in more of the non-relevant documents.
For a set of queries Q, the RI measure is defined as: RI(Q) = n+ − n− |Q| (6) where n+ is the number of queries helped by the feedback method and n− is the number of queries hurt.
Each curve through uncircled points shows the RI/MAP tradeoff using the  simple small-α strategy (see text) as α decreases from 0.5 to zero in the direction of the arrow.
The RS-FB feedback method obtained higher robustness than Base-FB on three of the four topic sets, with only slightly worse performance on TREC-8.
Compared to Base-FB, the RS-FB method achieves a  noticable reduction in the number of queries significantly hurt by expansion (i.e.
In a TREC evaluation using the GOV2 corpus [6], the method was one of the  topperforming runs, achieving a 19.8% gain in MAP compared to using unexpanded queries.
The relevance-score weighting strategy performs better overall, with significantly higher RI and P10 scores on 3 of the 4 topic sets.
We used the methods described in Section 2 to estimate an enhanced feedback model from the Dirichlet posterior distribution for each query variant, and to combine the feedback models from all the query variants.
(The top 100 expansion terms were selected to generate this example.)
Figure 4: Histogram showing improved robustness of  resampling feedback (RS-FB) over baseline feedback (Base-FB) for all datasets combined.
This method first selects terms using a log-odds calculation  described by Ponte [14], but assigns final term weights using Lavrenko"s relevance model[10].
However, an interesting side-effect of our  resampling approach is that it tends to remove many stopwords from the feedback model, making a stoplist less critical.
QV indicates that query variants were used in both runs.
3.4 Retrieval robustness We use the term robustness to mean the worst-case  average precision performance of a feedback algorithm.
This happens, for example, even with the Relevance Model approach that is part of our baseline feedback.
Finally, both Base-FB and RS-FB also consistently improved recall over using no expansion, with Base-FB achieving  better recall than RS-FB for all topic sets.
Using resampling feedback, however, appears to mitigate Collection QV + Uniform QV + Relevance-score weighting weighting TREC 1&2 AvgP 0.2545 0.2406 (-5.46%) P10 0.5369 0.5263 (-1.97%) RI 0.6212 0.7087 (+14.09%) TREC 7 AvgP 0.2174 0.2169 (-0.23%) P10 0.4320 0.4480 (+3.70%) RI 0.4783 0.5652 (+18.17%) TREC 8 AvgP 0.2267 0.2268 (+0.04%) P10 0.4120 0.4340 (+5.34%) RI 0.4545 0.4545 (+0.00%) wt10g AvgP 0.1808 0.1946 (+7.63%) P10 0.2680 0.2960 (+10.45%) RI 0.0220 0.1099 (+399.5%) Table 4: Comparison of uniform and relevance-weighted document sampling.
Each feedback model was obtained from a random sample of the top k documents taken with replacement.
By default we used the top 50 documents for feedback and the top 20 expansion terms, with the feedback interpolation parameter α = 0.5 unless otherwise stated.
Queries are binned by % change in AP compared to the unexpanded query.
Each feedback model contained 20 terms.
This figure shows that resampling feedback gives a somewhat better trade-off than the small-α approach for 3 of the 4 collections.
Also, the RS-FB method achieved  consistent improvements in P10 over Base-FB for every topic set, with an average improvement of 6.89% over Base-FB for all 350 topics.
The difference in average precision between the methods, however, is less marked.
For example, the initial query for topic 404 is: #combine(ireland peace talks) We performed Krovetz stemming for all experiments.
On the query side, we used leave-one-out (LOO) sampling to create the query variants.
We also include recall at 1,000 documents.
3.2 Baseline feedback method For our baseline expansion method, we use an algorithm included in Indri 1.0 as the default expansion method.
We therefore compared the  precision/robustness trade-off between our resampling feedback algorithm, and the simple small-α method.
Stopwords can harm feedback if  selected as feedback terms, because they are typically poor discriminators and waste valuable term slots.
To ensure as strong a baseline as possible, we use a stoplist for all  experiments reported here.
For comparison, the performance of resampling feedback at α = 0.5 is shown for each collection as the circled point.
This relevance model is then combined with the original query using linear interpolation, weighted by a parameter α.
The Indri baseline expansion gain was 17.25%.
We chose the Indri method because it gives a consistently strong baseline, is based on a language modeling approach, and is simple to experiment with.
In this section, we examine average  precision and precision in the top 10 documents (P10).
In contrast, the ‘relevance-score weighting" strategy chose documents with probability proportional to their relevance scores.
Much beyond this level, the additional benefits of more samples decrease as the initial score distribution is more closely fit and the processing time increases.
We chose these for their varied content and document properties.
3.5 Effect of query and document sampling methods Given our algorithm"s improved robustness seen in  Section 3.4, an important question is what component of our system is responsible.
Single-term query sampling had consistently worse performance across all collections and so our results here  focus on LOO sampling.
To generate the baseline queries passed to Indri, we wrapped the query terms with Indri"s #combine operator.
The percentage change compared to uniform sampling is shown in parentheses.
Collection NoExp Base-FB RS-FB TREC 1&2 AvgP 0.1818 0.2419 (+33.04%) 0.2406 (+32.24%) P10 0.4443 0.4913 (+10.57%) 0.5363 (+17.83%) Recall 15084/37393 19172/37393 15396/37393 TREC 7 AvgP 0.1890 0.2175 (+15.07%) 0.2169 (+14.75%) P10 0.4200 0.4320 (+2.85%) 0.4480 (+6.67%) Recall 2179/4674 2608/4674 2487/4674 TREC 8 AvgP 0.2031 0.2361 (+16.25%) 0.2268 (+11.70%) P10 0.3960 0.4160 (+5.05%) 0.4340 (+9.59%) Recall 2144/4728 2642/4728 2485/4728 wt10g AvgP 0.1741 0.1829 (+5.06%) 0.1946 (+11.78%) P10 0.2760 0.2630 (-4.71%) 0.2960 (+7.24%) Recall 3361/5980 3725/5980 3664/5980 Table 1: Comparison of baseline (Base-FB) feedback and feedback using re-sampling (RS-FB).
We call our method ‘resampling expansion" and denote it as RS-FB here.
Since we are also  reducing the potential gains when the feedback model is ‘right", however, we would expect some trade-off between average precision and robustness.
3.1 General method We evaluated performance on a total of 350 queries  derived from four sets of TREC topics: 51-200 (TREC-1&2), 351-400 (TREC-7), 401-450 (TREC-8), and 451-550 (wt10g, TREC-9&10).
In this way, documents that were more highly ranked were more likely to be selected.
We used 30 samples for our  experiments.
The number of samples has some effect on precision when less than 10, but performance stabilizes at around 15 to 20 samples.
As expected, robustness  continuously increases as we move along the curve, but mean  average precision generally drops as the gains from feedback are eliminated.
Our queries were derived from the words in the title field of the TREC topics.
In this study, it achieves an average gain in MAP of 17.25% over the four collections.
A more detailed view showing the distribution over  relative changes in AP is given by the histogram in Figure 4.
We observed similar feedback term behavior across many other topics.
the effect of stopwords automatically.
To evaluate robustness in this study, we use a very  simple measure called the robustness index (RI)3 .
Table 2 gives the Robustness Index scores for Base-FB and RS-FB.
For space reasons we only summarize our findings on  sample size here.
Robustness, and the tradeoff between precision and robustness, is analyzed in Section 3.4.
The value of RI ranges from a minimum 3 This is sometimes also called the reliability of improvement index and was used in Sakai et al.
Indexing and retrieval was performed using the Indri system in the Lemur toolkit [12] [1].
Nevertheless, this does not overcome the stopword problem completely, especially as the number of feedback terms grows.
In two cases, the RI measure drops by more than 50%.
The lowest P10 gain over Base-FB was +3.82% for TREC-7 and the highest was +11.95% for wt10g.
Circled points represent the tradeoffs obtained by resampling feedback for α = 0.5.
Higher and to the right is better.
The results are summarized in Figure 3.
For example, wt10g documents are Web pages with a wide variety of subjects and styles while TREC-1&2 documents are more homogeneous news articles.
The RI measure does not take into account the magnitude or distribution of the amount of change across the set Q.
However, it is easy to understand as a general indication of robustness.
We observe several trends in this table.
We use Dirichlet  smoothing of p(v|D) with μ = 1000.
Results are shown in Table 4.
where AP is hurt by 25% or more), while preserving positive gains in AP.
Feedback terms for TREC topic 60: merit pay vs seniority.
The  yaxis gives the Robustness Index (RI).
We call this the ‘small-α" strategy.
Phrases were not used.
For these experiments, B = 30 and k = 50.
This is discussed further in Section 3.6.
