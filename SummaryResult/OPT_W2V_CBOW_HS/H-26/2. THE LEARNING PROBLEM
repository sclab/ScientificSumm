In the case of learning a ranked retrieval function, X  denotes a space of queries, and Y the space of (possibly weak) rankings over some corpus of documents C = {d1, .
We can define average precision loss as ∆map(y, ˆy) = 1 − MAP(rank(y), rank(ˆy)), where rank(y) is a vector of the rank values of each  document in C. For example, for a corpus of two documents, {d1, d2}, with d1 having higher rank than d2, rank(y ) = (1, 0).
A learning method which optimizes for accuracy would choose h2 since that results in a higher accuracy score, but this yields a suboptimal MAP score. 
Using our notation, ROCArea can be defined as ROC(p, ˆp) = 1 rel · (|C| − rel) X i:pi=1 X j:pj =0 1[ˆpi>ˆpj ], where p is the true (weak) ranking, ˆp is the predicted  ranking, and 1[b] is the indicator function conditioned on b. Doc ID 1 2 3 4 5 6 7 8 p 1 0 0 0 0 1 1 0 rank(h1(x)) 8 7 6 5 4 3 2 1 rank(h2(x)) 1 2 3 4 5 6 7 8 Table 1: Toy Example and Models Suppose we have a hypothesis space with only two  hypothesis functions, h1 and h2, as shown in Table 1.
Hypothesis MAP ROCArea h1(x) 0.59 0.47 h2(x) 0.51 0.53 Table 2: Performance of Toy Models Table 2 shows the MAP and ROCArea scores of h1 and h2.
For instance, with h1(q), a threshold  between documents 1 and 2 gives 4 errors (documents 6-9  incorrectly classified as non-relevant), yielding an accuracy of 0.64.
Following the standard machine learning setup, our goal is to learn a function h : X → Y between an input space X (all possible queries) and output space Y (rankings over a corpus).
Here, a learning method which optimizes for  ROCArea would choose h2 since that results in a higher  ROCArea score, but this yields a suboptimal MAP score.
These two hypotheses predict a ranking for query x over a corpus of eight documents.
Doc ID 1 2 3 4 5 6 7 8 9 10 11 p 1 0 0 0 0 1 1 1 1 0 0 rank(h1(x)) 11 10 9 8 7 6 5 4 3 2 1 rank(h2(x)) 1 2 3 4 5 6 7 8 9 10 11 Table 3: Toy Example and Models We consider again a hypothesis space with two  hypotheses.
The average precision score is defined as MAP(p, ˆp) = 1 rel X j:pj =1 Prec@j, where rel = |{i : pi = 1}| is the number of relevant  documents, and Prec@j is the percentage of relevant documents in the top j documents in predicted ranking ˆy.
We restrict ourselves to the supervised learning scenario, where input/output pairs (x, y) are  available for training and are assumed to come from some fixed distribution P(x, y).
h1(q) 0.70 0.64 h2(q) 0.64 0.73 Table 4: Performance of Toy Models Table 4 shows the MAP and best accuracy scores of h1(q) and h2(q).
In order to quantify the quality of a prediction, ˆy = h(x), we will consider a loss function ∆ : Y × Y → .
Similarly, with h2(q), a threshold between documents 5 and 6 gives 3 errors (documents 10-11 incorrectly  classified as relevant, and document 1 as non-relevant), yielding an accuracy of 0.73.
∆(y, ˆy) quantifies the penalty for making prediction ˆy if the correct output is y.
We assume true rankings have two rank values, where relevant documents have rank value 1 and non-relevant  documents rank value 0.
Models which optimize for accuracy are not directly  concerned with the ranking.
The best accuracy refers to the highest  achievable accuracy on that ranking when considering all  possible thresholds.
, n}, the performance of h on S can be measured by the empirical risk, R∆ S (h) = 1 n nX i=1 ∆(yi, h(xi)).
Table 3 shows the predictions of the two hypotheses on a single query x. Hypothesis MAP Best Acc.
The goal is to find a function h such that the risk (i.e., expected loss), R∆ P (h) = Z X×Y ∆(y, h(x))dP(x, y), is minimized.
We further assume that all predicted rankings are complete rankings (no ties).
But given a finite set of training pairs, S = {(xi, yi) ∈ X × Y : i = 1, .
Instead, they learn a threshold such that documents scoring higher than the threshold can be classified as relevant and documents scoring lower as  nonrelevant.
In contrast, MAP assigns greater penalties to misorderings higher up in the predicted ranking.
Let p = rank(y) and ˆp = rank(ˆy).
MAP is the mean of the average precision scores of a group of queries.
2.2 MAP vs Accuracy Using a very similar example, we now demonstrate how optimizing for accuracy might result in suboptimal MAP.
While optimizing for these measures might achieve good MAP performance, we use two simple examples to show it can also be suboptimal in terms of MAP.
The loss function allows us to  incorporate specific performance measures, which we will exploit 1 http://svmrank.yisongyue.com for optimizing MAP.
2.1 MAP vs ROCArea Most learning algorithms optimize for accuracy or  ROCArea.
ROCArea assigns equal penalty to each misordering of a relevant/non-relevant pair.
Of course, P(x, y) is unknown.
