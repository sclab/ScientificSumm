In other words, the  algorithm maximizes H for each non-relevant document, d¯x j , Algorithm 2 Finding the Most Violated Constraint (argmax H) for Algorithm 1 with ∆map 1: Input: w, Cx , C¯x 2: sort Cx and C¯x in descending order of wT φ(x, d) 3: sx i ← wT φ(x, dx i ), i = 1, .
Observation 1 implies that in the ranking which  maximizes H, the relevant documents will be sorted by wT φ(x, d), and the non-relevant documents will also be sorted likewise.
The difference in the value of H for these two rankings is exactly δj(i, i + 1).
If the relevant documents are sorted by wT φ(x, d) in descending order, and the non-relevant  documents are likewise sorted by wT φ(x, d), then the  interleaving of the two sorted lists which satisfies the constraints will maximize H for that constrained set of rankings.
If the discriminant value for an incorrect ranking y is greater than for the true ranking yi (e.g., F(xi, y; w) > F(xi, yi; w)), then corresponding slack variable, ξi, must be at least ∆(yi, y) for that constraint to be satisfied.
Every ranking which satisfies the same set of constraints will have the same ∆map.
Let ξ∗ (w) be the optimal solution of the slack variables for OP 1 for a given weight vector w. Then 1 n Pn i=1 ξi is an upper bound on the empirical risk R∆ S (w).
We first prove that δj(·, ·) is monotonically decreasing in j. Lemma 1.
(see [19] for proof) Proposition 1 shows that OP 1 learns a ranking function that optimizes an upper bound on MAP error on the  training set.
Conceptually H starts with a perfect ranking ¯y, and adds the change in H when each successive non-relevant document slides up the ranking.
Given query x, we can derive a prediction by finding the ranking y that maximizes the discriminant function: h(x; w) = argmax y∈Y F(x, y; w).
, n do 5: H(y; w) ≡ ∆(yi, y) + wT Ψ(xi, y) − wT Ψ(xi, yi) 6: compute ˆy = argmaxy∈Y H(y; w) 7: compute ξi = max{0, maxy∈Wi H(y; w)} 8: if H(ˆy; w) > ξi + then 9: Wi ← Wi ∪ {ˆy} 10: w ← optimize (3) over W = S i Wi 11: end if 12: end for 13: until no Wi has changed during iteration parameter that controls this tradeoff and can be tuned to achieve good performance in different training tasks.
Let ¯R = maxi maxy Ψ(xi, yi) − Ψ(xi, y) , ¯∆ = maxi maxy ∆(yi, y), and for any > 0, Algorithm 1 terminates after adding at most max  2n ¯∆ , 8C ¯∆ ¯R2 2 ff constraints to the working set W. (see [19] for proof) However, within the inner loop of this algorithm we have to compute argmaxy∈Y H(y; w), where H(y; w) = ∆(yi, y) + wT Ψ(xi, y) − wT Ψ(xi, yi), or equivalently, argmax y∈Y ∆(yi, y) + wT Ψ(xi, y), since wT Ψ(xi, yi) is constant with respect to y.
, |C¯x | do 6: optj ← argmaxk δj(k, |Cx | + 1) 7: end for 8: encode ˆy according to (7) 9: return ˆy without considering the positions of the other non-relevant documents, and thus ignores the constraints of (9).
In other words, we will show that both j + 1 j + k + 1 − j j + k ≤ j j + k − j − 1 j + k − 1 (11) and −2 · (sx k − s¯x j+1) ≤ −2 · (sx k − s¯x j ) (12) are true for the aforementioned values of j and k. It is easy to see that (11) is true by observing that for any two positive integers 1 ≤ a < b, a + 1 b + 1 − a b ≤ a b − a − 1 b − 1 , and choosing a = j and b = j + k. The second inequality (12) holds because Algorithm 2 first sorts d¯x in descending order of s¯x , implying s¯x j+1 ≤ s¯x j .
(2) The combined feature function we use is Ψ(x, y) = 1 |Cx| · |C¯x| X i:di∈Cx X j:dj ∈C¯x [yij (φ(x, di) − φ(x, dj))] , where φ : X × C → N is a feature mapping function from a query/document pair to a point in N dimensional space2 .
For any y ∈ Y, yij = +1 if di is ranked ahead of dj, and yij = −1 if dj is ranked ahead of di, and yij = 0 if di and dj have equal rank.
We define δj(i1, i2), with i1 < i2, as the change in H from when the highest ranked relevant document ranked after d¯x j is dx i1 to when it is dx i2 .
The first part is the sort by wT φ(x, d), which  requires O(n log n) time, where n = |Cx | + |C¯x |.
We can recover the ranking as yij = 8 >>>< >>>: 0 if i = j sign(si − sj) if di, dj equal relevance sign(oj − i − 0.5) if di = dx i , dj = d¯x j sign(j − oi + 0.5) if di = d¯x i , dj = dx j .
We use the structural SVM formulation, presented in  Optimization Problem 1, to learn a w ∈ RN .
Using Lemma 1, we know that δj+1(i, optj) ≤ δj(i, optj) < 0, which implies that for any 1 ≤ i < optj, δj+1(i, |Cx | + 1) − δj+1(optj, |Cx | + 1) < 0.
We can then reformulate the argmax H problem as argmax H = argmax o1,...,o|C¯x| |C¯x | X k=1 δk(ok, |Cx | + 1) (8) s.t.
For any i1 < i2, we can then define δj(i1, i2) as δj(i1, i2) = i2−1 X k=i1 δj(k, k + 1), (6) or equivalently, δj(i1, i2) = i2−1 X k=i1 » 1 |Cx| „ j j + k − j − 1 j + k − 1 « − 2 · (sx k − s¯x j )  .
For i2 = i1 + 1, we have δj(i, i + 1) = 1 |Cx| „ j j + i − j − 1 j + i − 1 « − 2 · (sx i − s¯x j ), (5) where si = wT φ(x, di).
In order for the solution to be feasible, the jth non-relevant document must be ranked after the first j − 1 non-relevant documents, thus satisfying opt1 ≤ opt2 ≤ .
The first term in (5) is the change in ∆map when the ith relevant document has j non-relevant documents ranked before it, as opposed to j −1.
In Algorithm 2, the computed values of optj satisfy (10), implying that the solution returned by Algorithm 2 is feasible and thus optimal.
Let Cx and C¯x  denote the set of relevant and non-relevant documents of C for query x, respectively.
Note that wT Ψ(x, y) is exactly our discriminant function F(x, y; w) (see equation (2)).
Though closely related to the classification procedure, this has the substantial complication that we must contend with the  additional ∆(yi, y) term.
We will show that each term in the summation of δj+1(i1, i2) is no greater than the corresponding term in δj(i1, i2), or δj+1(k, k + 1) ≤ δj(k, k + 1) for k = i1, .
We will prove that optj ≤ optj+1 holds for any 1 ≤ j < |C¯x |, thus implying (10).
∀i, ∀y ∈ Y \ yi : wT Ψ(xi, yi) ≥ wT Ψ(xi, y) + ∆(yi, y) − ξi (4) The objective function to be minimized (3) is a tradeoff between model complexity, w 2 , and a hinge loss relaxation of MAP loss, P ξi.
The main algorithmic contribution of this paper is an efficient method for solving argmax H for ∆map.
This is primarily because ROCArea decomposes nicely into a sum of scores computed independently on each relative  ordering of a relevant/non-relevant document pair.
Though in the worst case this is O(n2 ), the number of  relevant documents, |Cx |, is often very small (e.g., constant with respect to n), in which case the running time for the second part is simply O(n).
Since Algorithm 2 computes optj as optj = argmax k δj(k, |Cx | + 1), (13) then by definition of δj (6), for any 1 ≤ i < optj, δj(i, optj) = δj(i, |Cx | + 1) − δj(optj, |Cx | + 1) < 0.
Then for each successive non-relevant document, the algorithm modifies the solution by sliding that  document up the ranking to locally maximize H while keeping the positions of the other non-relevant documents constant.
The algorithm starts with no constraints, and iteratively finds for each example (xi, yi) the output ˆy associated with the most violated constraint.
For the rest of our discussion, we assume that the relevant documents and non-relevant documents are both sorted by descending wT φ(x, d).
Once training is complete, making predictions on query x  using the resulting hypothesis h(x|w) requires only sorting by wT φ(x, d).
(1) We assume F to be linear in some combined feature  representation of inputs and outputs Ψ(x, y) ∈ RN , i.e., F(x, y; w) = wT Ψ(x, y).
Thus we see that each term in δj+1 is no greater than the corresponding term in δj, which completes the proof.
For convenience, we also refer to relevant documents as {dx 1 , .
3.1 Structural SVMs The above formulation is very similar to learning a  straightforward linear model while training on the pairwise  difference of relevant/non-relevant document pairings.
For most real-world datasets, Algorithm 2 is dominated by the sort and has complexity O(n log n).
(7) We can now reformulate H into a new objective function, H (o1, .
3.2.1 Proof of Correctness Algorithm 2 is greedy in the sense that it finds the best position of each non-relevant document independently from the other non-relevant documents.
As is also discussed in [13], this is attained by sorting the documents by wT φ(x, d) in descending order.
For each (xi, yi) in the training set, a set of constraints of the form in equation (4) is added to the optimization problem.
Recall that the true ranking is a weak ranking with two rank values (relevant and non-relevant).
One useful property of ∆map is that it is invariant to  swapping two documents with equal relevance.
The second part computes each optj, which requires O(|Cx | · |C¯x |) time.
By first sorting the relevant and non-relevant documents, the problem is simplified to finding the optimal interleaving of two sorted lists.
If the corresponding constraint is violated by more than we introduce ˆy into the working set Wi of active constraints for example i, and re-solve (3) using the updated W. It can be shown that Algorithm 1"s outer loop is guaranteed to halt within a polynomial number of iterations for any desired precision .
For any 1 ≤ i1 < i2 ≤ |Cx | + 1 and 1 ≤ j < |C¯x |, it must be the case that δj+1(i1, i2) ≤ δj(i1, i2).
We will show that each part of δj+1(k, k + 1) is no greater than the corresponding part in δj(k, k + 1).
The result of Lemma 1 leads directly to our main  correctness result: Theorem 2.
Intuitively, Ψ is a  summation over the vector differences of all relevant/non-relevant document pairings.
For example, if documents da and db are both relevant, then swapping the positions of da and db in any ranking does not affect ∆map.
Unfortunately there is a problem: a constraint is required for every possible wrong output y, and the  number of possible wrong outputs is exponential in the size of C. Fortunately, we may employ Algorithm 1 to solve OP 1.
, o|C¯x||w) = H(¯y|w) + |C¯x | X k=1 δk(ok, |Cx | + 1), where ¯y is the true (weak) ranking.
Conceptually, Algorithm 2 starts with a perfect ranking.
Previously, it was not clear how to incorporate non-linear multivariate loss  functions such as MAP loss directly into global optimization problems such as SVM training.
Algorithm 1 is guaranteed to halt in a polynomial  number of iterations [19], and each iteration runs Algorithm 2.
, o|C¯x| encode the positions of the non-relevant documents, where dx oj is the highest ranked relevant  document ranked after the jth non-relevant document.
Algorithm 1 is a cutting plane algorithm, iteratively  introducing constraints until we have solved the original problem within a desired tolerance [19].
The second term is the change in the discriminant score, wT Ψ(x, y), when yij changes from +1 to −1.
Recall from (6) that both δj(i1, i2) and δj+1(i1, i2) are summations of i2 − i1 terms.
Therefore, it must be the case that optj ≤ optj+1, which completes the proof.
Due to Observation 1, this encoding uniquely identifies a complete ranking.
Consider rankings which are constrained by fixing the relevance at each position in the ranking (e.g., the 3rd document in the ranking must be relevant).
solving equation (1)) given query x reduces to picking each yij to maximize wT Ψ(x, y).
We focus on functions which are parametrized by a weight vector w, and thus wish to find w to minimize the empirical risk, R∆ S (w) ≡ R∆ S (h(·; w)).
During prediction, our model chooses the ranking which maximizes the  discriminant (1).
Each term in δj(k, k +1) and δj+1(k, k +1) can be further decomposed into two parts (see (5)).
dx |Cx|} = Cx , and non-relevant documents as {d¯x 1 , .
3.2 Finding the Most Violated Constraint Using OP 1 and optimizing to ROCArea loss (∆roc), the problem of finding the most violated constraint, or solving argmaxy∈Y H(y, w) (henceforth argmax H), is addressed in [13].
Algorithm 1 Cutting plane algorithm for solving OP 1 within tolerance .
Since we assume predicted rankings to be complete rankings, yij is either +1 or −1 (never 0).
As is usual in SVM training, C is a 2 For example, one dimension might be the number of times the query words appear in the document.
The bottom ranking differs from the top only where d¯x j slides up one rank.
Therefore, it suffices to prove that Algorithm 2 satisfies (10).
Many SVM-based approaches optimize over these pairwise  differences (e.g., [5, 10, 13, 4]), although these methods do not optimize for MAP during training.
We will discuss later the choices of φ we used for our experiments.
Without the ability to efficiently find the most violated constraint (i.e., solve argmaxy∈Y H(y, w)), the constraint generation procedure is not tractable.
However, this reshuﬄing will affect the discriminant score, wT Ψ(x, y).
We consider only matrices which correspond to valid rankings (i.e, obeying antisymmetry and transitivity).
Given a learned weight vector w, predicting a ranking (i.e.
By extension, ∆map is invariant to any arbitrary  permutation of the relevant documents amongst themselves and of the non-relevant documents amongst themselves.
(9) Algorithm 2 describes the algorithm used to solve  equation (8).
(10) If the solution is feasible, the it clearly solves (8).
We represent rankings as a matrix of pairwise orderings, Y ⊂ {−1, 0, +1}|C|×|C| .
Then δj+1(optj+1, |Cx | + 1) < δj+1(optj, |Cx | + 1), which contradicts (13).
, |C¯x | 5: for j = 1, .
Our approach is to learn a discriminant function F : X × Y → over input-output pairs.
Therefore, the sum of slacks, P ξi, upper bounds the MAP loss.
Unlike ROCArea, however, MAP does not decompose linearly in the examples and requires a  substantially extended algorithm, which we describe in this section.
Solving argmax H for ∆map is more difficult.
To improve performance, it is advisable to use the standard C implementation4 of SVMstruct . 
We build upon the approach used by [13] for  optimizing ROCArea.
, (xn, yn), C, 2: Wi ← ∅ for all i = 1, .
Figure 1: Example for δj(i, i + 1) Figure 1 gives a conceptual example for δj(i, i + 1).
, |Cx | 4: s¯x i ← wT φ(x, d¯x i ), i = 1, .
(Structural SVM) min w,ξ≥0 1 2 w 2 + C n nX i=1 ξi (3) s.t.
, n 3: repeat 4: for i = 1, .
We now present a method based on structural SVMs [19] to address this problem.
This leads us to Observation 1.
Suppose for contradiction that optj+1 < optj.
3.2.2 Running Time The running time of Algorithm 2 can be split into two parts.
MAP, on the other hand, does not decompose in the same way as  ROCArea.
Optimization Problem 1.
We developed our software using a Python interface3 to SVMstruct , since the Python language greatly simplified the coding process.
, i2 − 1.
1: Input: (x1, y1), .
Observation 1.
≤ opt|C¯x|.
≤ o|C¯x|.
, d¯x j , dx i , dx i+1, .
, dx i , d¯x j , dx i+1, .
Theorem 1.
This is stated formally in Proposition 1.
Virtually all well-performing models were trained in a  reasonable amount of time (usually less than one hour).
d¯x |C¯x|} = C¯x .
Proposition 1.
Let o1, .
o1 ≤ .
