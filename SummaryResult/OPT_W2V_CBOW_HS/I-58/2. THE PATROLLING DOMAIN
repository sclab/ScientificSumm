m. The security agent"s set of pure strategies consists of possible routes of d houses to patrol (in an order).
The security agent can choose a mixed strategy so that the robber will be unsure of exactly where the security agent may  patrol, but the robber will know the mixed strategy the security agent has chosen.
With this knowledge, the robber must choose a single house to rob.
With this structure it is possible to model many different types of robbers who have differing motivations; for example, one robber may have a lower cost of getting caught than another, or may value the goods in the various houses differently.
The security agent"s set of possible pure strategies (patrol routes) is denoted by X and includes all d-tuples i =< w1, w2, ..., wd > with w1 .
If the house chosen by the robber is not on the security agent"s route, then the robber successfully robs it.
We assume that the  robber generally takes a long time to rob a house.
Otherwise, if it is on the security agent"s route, then the earlier the house is on the route, the easier it is for the security agent to catch the robber before he finishes robbing it.
The robber"s set of possible pure strategies (houses to rob) is denoted by Q and includes all integers j = 1 .
m. The payoffs (security agent, robber) for pure strategies i, j are: • −vl,x, vl,q, for j = l /∈ i.
Instead, they must choose a policy by which they patrol various routes at different times, taking into account factors such as the likelihood of crime in different areas, possible targets for crime, and the security agents" own resources (number of security agents, amount of available time, fuel, etc.).
• vl,q: value of the goods in house l to the robber.
We model the payoffs for this game with the following variables: • vl,x: value of the goods in house l to the security agent.
For example, the robber can observe over time how often the security agent patrols each area.
It is usually beneficial for this policy to be nondeterministic so that robbers cannot safely rob certain locations, knowing that they will be safe from the security agents [14].
The most basic version of our game consists of two players: the security agent (the leader) and the robber (the follower) in a world consisting of m houses, 1 .
• cx: reward to the security agent of catching the robber.
• cq: cost to the robber of getting caught.
• pl: probability that the security agent can catch the robber at the lth house in the patrol (pl < pl ⇐⇒ l < l).
If the distribution of different robber types is known or inferred from historical data, then the game can be modeled as a Bayesian game [6]. 
In most security patrolling domains, the security agents (like UAVs [1] or security robots [16]) cannot feasibly patrol all areas all the time.
m where no two elements are equal (the agent is not allowed to return to the same house).
To demonstrate the utility of our algorithm, we use a simplified version of such a domain, expressed as a game.
• plcx +(1−pl)(−vl,x), −plcq +(1−pl)(vl,q), for j = l ∈ i.
wd = 1 .
