While that work provides epsilon error-bounds based on the k-uniform strategies, their solution concept is still that of a Nash equilibrium, and they do not provide efficient algorithms for obtaining such k-uniform strategies.
Second, it provides  strategies which are simple to understand, represent, and implement.
However most of this work is focused on either limiting energy consumption involved in patrolling [7] or optimizing on criteria like the length of the path traveled [4, 13], without reasoning about any explicit model of an adversary[14].
Specifically, we deal with situations where the adversaries" actions and payoffs are known but the exact  adversary type is unknown to the security agent.
First, ASAP searches for the highest reward strategy, rather than a Bayes-Nash  equilibrium, allowing it to find feasible strategies that exploit the  natural first-mover advantage of the game.
Less attention has been paid to finding the optimal strategy to commit to in a Bayesian game (the Stackelberg scenario [15]).
Finally the patrolling problem which motivated our work has  recently received growing attention from the multiagent community due to its wide range of applications [4, 13].
This contrasts with ASAP, where our emphasis is on a highly efficient heuristic approach that is not focused on equilibrium solutions.
This paper focuses on security for agents patrolling in hostile  environments.
Acknowledgments : This research is supported by the United States Department of Homeland Security through Center for Risk and Economic Analysis of Terrorism Events (CREATE).
In these environments, intentional threats are caused by adversaries about whom the security patrolling agents have  incomplete information.
The Gala toolkit is one method for defining such games [9] without requiring the game to be represented in normal form via the Harsanyi transformation [8]; Gala"s guarantees are focused on fully competitive games.
Bayesian games have been a popular choice to model such incomplete information games [3].
However, the complexity of this problem was shown to be NP-hard in the general case [5], which also provides algorithms for this problem in the non-Bayesian case.
Agents acting in the real world quite frequently have such incomplete information about other agents.
Much work has been done on finding optimal Bayes-Nash equilbria for The Sixth Intl.
Our k-uniform strategies are similar to the k-uniform strategies of [12].
on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 317 Figure 3: Reward for ASAP using multisets of 10, 30, and 80 elements subclasses of Bayesian games, finding single Bayes-Nash  equilibria for general Bayesian games [10] or approximate Bayes-Nash equilibria [18].
Therefore, we present a heuristic called ASAP, with three key advantages towards addressing this problem.
We provide an efficient Mixed Integer Linear Program (MILP) implementation for ASAP, along with experimental results illustrating significant speedups and higher rewards over other approaches.
Sarit Kraus is also affiliated with UMIACS. 
It is also supported by the  Defense Advanced Research Projects Agency (DARPA), through the  Department of the Interior, NBC, Acquisition Services Division, under Contract No.
Third, it operates directly on the compact, Bayesian game  representation, without requiring conversion to normal form.
Joint Conf.
