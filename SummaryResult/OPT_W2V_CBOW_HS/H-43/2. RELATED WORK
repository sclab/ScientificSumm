Though our approach also uses latent space to  represent web pages (documents), we consider the link structure as well as the content of web pages.
Instead of using two categories, PageRank [17] uses a single category for the recursive notion, an authority is a web page with many incoming links from authorities.
Our  approach consider the content and the directed linkage between topics of source and destination web pages in one step, which implies the topic combines the information of web page as authorities and as hubs in a single set of factors.
The following are some work on combining documents and links, but the methods are loosely related to our approach.
Cohn and Hofmann [6] construct the latent space from both  content and link information, using content analysis based on  probabilistic LSI (PLSI) [10] and link analysis based on PHITS [5].
Achlioptas et al[2] decompose the web into hub and authority attributes then combine them with content.
Oh et al [16] use the Naive Bayesian frame to combine link information with content. 
Using recursive notion, a hub is a web page with many outgoing links to authorities, while an authority is a web page with many incoming links from hubs.
As the result of matrix factorization, the factor forms a factor graph, a miniature of the original graph, preserving the major structure of the original graph.
[3] use co-citation information in their classification model.
The algorithm incorporates link structure and the co-citation patterns.
The content information is only used for weighing the links by the textual similarity of both ends of the links.
The model was applied to web page classification, where web pages are  entities and hyperlinks are treated as relationships.
In our approach, the link is constructed with the linkage between the factor of the source web page and the factor of the  destination web page, instead of the destination web page itself.
In the link analysis approach, the framework of hubs and  authorities (HITS) [12] puts web page into two categories, hubs and authorities.
The major difference between the approach of [6] (PLSI+PHITS) and our approach is in the part of link analysis.
The similarity between documents could be defined by the dot products of the corresponding vectors of documents in the latent space.
The framework combines the hub and authority information of web pages.
In PLSI+PHITS, the link is constructed with the linkage from the topic of the source web page to the destination web page.
Analysis tasks, such as  classification, could be performed on the latent space.
The commonly used singular value decomposition (SVD) method ensures that the data points in the latent space can optimally reconstruct the original documents.
The factor of the destination web page contains information of its  outgoing links.
[23] uses the undirected graph regularization framework for document classification.
In the model, the outgoing links of the destination web page have no effect on the source web page.
The experiments of [21] show that using terms from the linked  document improves the classification accuracy.
The latent space implicitly  captures a large portion of information of documents, therefore it is called the latent semantic space.
In the content analysis part, our approach is closely related to Latent Semantic Indexing (LSI) [8].
RMNs apply  conditional random fields to define a set of potential functions on cliques of random variables, where the link structure provides hints to form the cliques.
In the algorithm, all links are treated as undirected edge of the link graph.
Joachims et al.
In turn, such information is passed to the factor of the source web page.
Chakrabarti et al.
In other words, the overall link structure is not utilized in PHITS.
[9] propose a clustering algorithm for web document  clustering.
Taskar et al.
[25] and [24] propose a directed graph regularization framework for semi-supervised learning.
But it is difficult to combine the content information into that framework.
[19] propose relational Markov networks (RMNs) for entity classification, by describing a conditional distribution of entity classes given entity attributes and relationships.
Zhou et al.
Zhang et al.
[11] combine text kernels and co-citation kernels for  classification.
LSI maps documents into a lower dimensional latent space.
However the model does not give an off-the-shelf  solution, because the success highly depends on the arts of designing the potential functions.
He et al.
On the other hand, the inference for RMNs is intractable and requires belief propagation.
