Because the two factorizations share a common base, the discovered bases (latent factors) explain both content information and link structures, and are then used in further information management tasks such as classification.
However, this approach  relies only on the content of web pages and ignores the structure of links among them.
In the following discussion, we use the task of web page  classification as an illustrating example, while the techniques we develop in later sections are applicable equally well to many other tasks in information retrieval and data mining.
Link structures provide invaluable information about properties of the documents as well as relationships among them.
Instead,  similar to LSI, the factors are learned from the data.
In the formulation, we perform factor analysis based on matrix factorization: solution to the first component is based on  factorizing the term-document matrix derived from content features;  solution to the second component is based on factorizing the adjacency matrix derived from links.
Comparing to data in  traditional information management, in addition to content, these data on the Web also contain links: e.g., hyperlinks from a student"s homepage pointing to the homepage of her advisor, paper citations, sources of a news article, comments of one blogger on posts from another blogger, and so on.
For example, in the WebKB dataset, the link structure  provides additional insights about the relationship among documents (e.g., links often pointing from a student to her advisor or from a faculty member to his projects).
v1 v2 v3 v4 v5 v6 v7 v8 Figure 1: An example of link structure In this paper, we propose a simple technique for analyzing inter-connected documents, such as web pages, using factor  analysis[18].
Some examples of such data include organizational and personal web pages (e.g, the WebKB benchmark data set, which contains university web pages), research papers (e.g., data in CiteSeer), online news articles, and customer-generated media (e.g., blogs).
By doing this, we search for a unified set of latent factors that best explains both content and link structures simultaneously and seamlessly.
From this point of view, the traditional classification methods that ignore the link structure may not be suitable.
This is a key difference between traditional factor analysis and our model.
However, link and content information have different properties.
The second component captures the  information contained in the underlying link structure, such as links from homepages of students to those of faculty members.
In Figure 1, for example, web pages v6 and v7 co-cite web page v8, implying that v6 and v7 are similar to each other.
In the proposed technique, both content information and link structures are seamlessly combined through a single set of  latent factors.
To improve the performance of web page classification,  therefore, both link structure and content information should be taken into consideration.
The first  component captures the content information.
Since some links among these documents imply the inter-dependence among the documents, the usual i.i.d.
Some work, such as [3], incorporates link information using  cocitation similarity, but this may not fully capture the global link structure.
On the other hand, there exist some studies, as we will discuss in detail in related work, that consider link information and content information separately and then combine them.
The advantage of this approach is that many off-the-shelf classification tools can be directly applied to the problem.
It is worth noting that we do not explicitly define the semantic of a factor a priori.
Section 3 presents the proposed approach to analyze the web page based on the combined information of links and content.
In our model, we connect two  components through a set of shared factors, that is, the latent factors in the second component (for contents) are tied to the factors in the first component (for links).
On the other hand, a few studies, for example [25], rely solely on link structures.
In turns, v4 and v5 should be similar to each other, since v4 and v5 cite similar web pages v6 and v7, respectively.
We argue that such an approach ignores the inherent consistency between link and  content information and therefore fails to combine the two seamlessly.
A  factor can be loosely considered as a type of documents (e.g., those homepages belonging to students).
For the classification problem of web pages, a simple approach is to treat web pages as independent documents.
Traditional factor analysis models the variables associated with entities through the factors.
This component has a form similar to that of the latent topics in the Latent Semantic Indexing (LSI) [8] in traditional information retrieval.
To achieve this goal, a simple approach is to convert one type of information to the other.
For example, in the Cora dataset, the content of a research article abstract largely determines the category of the article.
For example, a link is an actual piece of  evidence that represents an asymmetric relationship whereas the  content similarity is usually defined conceptually for every pair of  documents in a symmetric way.
In document classification, Kurland and Lee [14] convert content similarity among documents into weights of links.
Therefore, directly converting one type of information to the other usually degrades the quality of  information.
That is, documents are decomposed into latent topics/factors, which in turn are  represented as term vectors.
With the advance of the World Wide Web, more and more  hypertext documents become available on the Web.
However, in analysis of link structures, we need to model the relationship of two ends of links, i.e., edges between vertex pairs.
Our model contains two components.
It is however a very rare case that content  information can be ignorable.
Therefore, the model should involve factors of both vertices of the edge.
[13] concatenate outlink features with the content features of the blog.
This paper is organized as follows.
But using  cocitation similarity, the similarity between v4 and v5 is zero without considering other information.
Performing information management tasks on such structured data raises many new research challenges.
(independent and identical distributed) assumption of documents does not hold any more.
Section 4 extends the basic framework and a few variants for fine tune.
For example, in spam blog classification, Kolari et al.
Section 2 reviews related work.
Section 6 discusses the details of this approach and Section 7 concludes. 
Section 5 shows the experiment results.
