In addition, in our case, the user community that accesses a certain data file may also be very dispersed from a network point of view and thus cannot take advantage of any of the caching schemes.
From our perspective, although APPOINT employs some of the techniques used in peer-to-peer systems, it is also closely related to current Web caching architectures.
With APPOINT, we want to improve  existing client-server systems in terms of performance by using idle networking resources among active clients.
Cache misses are especially common in the type of large data-based services on which we are working.
The goal in this  approach is to enable remote users, typically only equipped with standard Web browsers, to access the company"s  spatial database server and retrieve information in the form of pictorial maps from them.
Although the advantage of this solution is the minimization of both  hardware and software resources on the client site, the resulting product has severe limitations in terms of available  functionality and response time (each user action results in a new bitmap being transferred to the client).
Work described in [9] examines a client-server  architecture for viewing large images that operates over a  lowbandwidth network connection.
Other systems followed these popular systems, each addressing a different flavor of sharing over the Internet.
Collaborative Web caching systems, the most relevant of these for our research, focus on creating  either a hierarchical, hash-based, central directory-based, or multicast-based caching schemes.
For our peer-to-peer transfer approach (APPOINT),  Napster is the forefather where a directory service is centralized on a server and users exchange music files that they have stored on their local disks.
In fact, APPOINT can work in  tandem with collaborative Web caching if they are deployed together.
In this case, while the server holds the full representation of the large  image, only a limited amount of data needs to be transferred to the client to enable it to display a currently requested view into the image.
It presents a technique based on wavelet transformations that allows the  minimization of the amount of data needed to be transferred over the network between the server and the client.
There has been a substantial amount of research on  remote access to spatial data.
We do not compete with these approaches.
Other related work has been reported in [16] where a client-server architecture is described that is designed to  provide end users with access to a server.
It is assumed that this data server manages vast databases that are impractical to be stored on individual clients.
Finally, none of the Web caching methods address the symmetric issue of large data uploads. 
Some of these systems have focused on anonymity while  others have focused on persistence of storage.
The solution presented by most of these vendors is based on performing all the calculations on the server side and transferring only bitmaps that  represent results of user queries and commands.
Hence, the point where the server is reached can be used to take a central decision but then the actual service request can be forwarded to a set of active clients, i.e., the  down8 load and upload operations.
We want to expand our  research, in the future, to address this issue.
It creates a pure  peer-topeer collaborative Web cache among the Web browser caches of the machines in a local-area network.
Also, other  approaches, like SETI@Home [21], made other resources, such as idle CPUs, work together over the Internet to solve large scale computational problems.
Both the client and the server keep a common mask that indicates what parts of the image are available on the client and what needs to be requested.
Except for this  recent peer-to-peer approach, Web caching is mostly a  wellstudied topic in the realm of server/proxy level caching [8, 11, 14, 17].
Hence, other issues like anonymity, decentralization, and persistence of storage were less important in our decisions.
One specific approach has been adopted by numerous Web-based mapping services (MapQuest [5], MapsOnUs [6], etc.).
Unfortunately, it suffers from scalability issues, i.e., floods of messages between peers in order to map connectivity in the system are required.
Confirming the authenticity of the indirectly delivered data sets is not yet addressed with APPOINT.
Our application domain, where the data is already freely available to the public, forms a prime candidate for such a peer-to-peer approach.
Our goal is different than these approaches.
On the client side, the image is  reconstructed into a pyramid representation to speed up zooming and panning operations.
This also allows dropping unnecessary parts of the image from the main memory on the server.
Hence, these policies would lead to the immediate  replacement of our relatively large data files even though they may be used frequently.
We try to address the situation where a request arrives at a server, meaning all the caches report a miss.
This work blends raster data management (stored in pyramids [22]) with vector data stored in quadtrees [19, 20].
Most of the Web caching schemes that are in use today employ a replacement policy that gives a  priority to replacing the largest sized items over smaller-sized ones.
Many peer-to-peer storage systems have also recently emerged.
Gnutella is a pure (decentralized) peer-to-peer file exchange system.
PAST [18], Eternity Service [7], CFS [10], and OceanStore [15] are some peer-to-peer storage systems.
Squirrel [13] forms the middle ground.
