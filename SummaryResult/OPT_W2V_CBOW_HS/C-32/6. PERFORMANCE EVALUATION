The overall time in the Reader group improves by 30% and is due to peer fetch that allows a client to access an  invalidated object at the cost of a local fetch avoiding the delay of fetching from the server.
In the three client group, Buddy and Base incur almost the same commit cost and therefore the entire performance benefit of Buddy is due to peer fetch avoiding direct fetches.
Our experiments run in a single server system and therefore, the commit time overhead of version cache management at the client does not contribute in the results presented in the section below.
The server was configured with a 50MB cache (of which 6MB were used for the modified object buffer), the client had a 40MB cache.
In hot caches we expect BuddyCache to provide two complementary benefits, both of which reduce the latency of access to shared modified  objects.
In peer fetch, the cost of the redirection includes, in addition to the local network request cost, the CPU processing latency of crossing the redirector and crossing the helper, the latter including the time to process the help request and the time to copy, and unswizzle the requested page.
6.4.2 Invalidation Misses To evaluate the performance benefits provided by  BuddyCache due to avoided invalidation misses, we compared the hot cache performance of the Base system with two  different Buddy system configurations.
The avoided server interactions correspond to different types of client cache misses.
Figure 12 shows the relative latency improvement  provided by BuddyCache in Buddy Reader and Buddy Writer configurations (computed as the overall time difference  between BuddyReader and Base relative to Base, and Buddy Writer and Base relative to Base) in a hot cache experiment as a function of increasing network latency, for fixed server load.
Table 1 shows the latency for the commit and server fetch requests in the Base and Buddy system for 3 client and 5 client groups in a fast local area network.
they depend on the total number of clients in the system.
This allows us to study the effect our techniques have on cold cache and cache consistency misses and isolate as much as possible the effect of cache capacity misses.
The results show that in a 40 ms network Buddy  system reduces significantly the overall time compared to the Base system, providing a 39% improvement in a three client group, 46% improvement in the five client group and 56% improvement in the ten client case.
6.1 Analysis The performance benefits of peer fetch and peer update are due to avoided server interactions.
In a 40 ms network, the overall time in the Writer group improves by 62% compared to Base.
We evaluate how the redirection cost affects this benefit by comparing and analyzing the performance of an application running in a storage system with  BuddyCache and without (called Base).
The commit cost increases with the number of clients since commits are processed sequentially.
The extra client commit time processing includes a version cache lookup operation for each object read by the  transaction at commit request preparation time, and a version cache insert operation for each object updated by a  transaction at commit reply processing time, but only if the  updated page is missing some earlier invalidations or updates.
The solo  commit mechanism introduces extra processing at the server at transaction validation time, and extra processing at the client at transaction commit time and at update or  invalidation processing time.
To keep the length of our experiments reasonable, we use small caches.
Let tfetch(S), tredirect(S), tcommit(S), and tcompute(S) be the time it takes a client to, respectively, fetch from server, peer fetch, commit a transaction and compute in a transaction, in a system S, where S is either a system with BuddyCache (BC) or without (Base).
Consider the sequence of r client misses on page P that arrive at the redirector in BCR between two consequent invalidations of page P. The first miss goes to the server, and the r − 1 subsequent misses are redirected.
One of the Buddy system configurations represents a collaborating peer group  modifying shared objects (Writer group), the other represents a group where the peers share a read-only interest in the  modified objects (Reader group) and the writer resides outside the BuddyCache group.
The numbers were 35 0 5 0 100 150 200 250 Base Buddy Base Buddy Base Buddy 3 Clients 5 Clients 10 Clients [ms] CPU Commit Server Fetch Peer Fetch Figure 7: Breakdown for cold read-only 40ms RTT 0 5 0 100 150 200 250 300 350 400 Base Buddy Base Buddy Base Buddy 3 Clients 5 Clients 10 Clients [ms] CPU Commit Server Fetch Peer Fetch Figure 8: Breakdown for cold read-only 80ms RTT obtained by averaging the overall time of each client in the group.
Buddy system with one group  contain36 0 5 0 100 150 200 250 300 Base Buddy Reader Buddy Writer [ms] CPU Commit Server Fetch Peer Fetch Figure 11: Breakdown for hot read-write 80ms RTT ing a single reader and another group containing two readers and one writer models the Writer group.
In OO7 benchmark, read-only transactions use the T1 traversal that performs a depth-first traversal of entire composite part graph.
34 Latency [ms] Base Buddy 3 group 5 group 3 group 5 group Fetch 1.3 1.4 2.4 2.6 Commit 2.5 5.5 2.4 5.7 Table 1: Commit and Server fetch Operation Latency [ms] PeerFetch 1.8 - 5.5 −AlertHelper 0.3 - 4.6 −CopyUnswizzle 0.24 −CrossRedirector 0.16 Table 2: Peer fetch 6.3 Basic Costs This section analyzes the basic cost of the requests in the Buddy system during the OO7 runs.
Figure 9 shows the relative latency improvement provided by BuddyCache (computed as the overall measured time difference between Buddy and Base relative to Base) as a -10% 0% 10% 20% 30% 40% 50% 60% 70% 1 5 1 0 2 0 3 0 4 0 5 0 6 0 7 0 8 0 9 0 100 Latency [ms] 3 Clients 3 Clients (Perf model) 5 Clients 5 Clients (Perf model) 10 Clients 10 FEs (perf model) Figure 9: Cold miss benefit 0 2 0 4 0 6 0 8 0 100 120 140 Base Buddy Reader Buddy Writer [ms] CPU Commit Server Fetch Peer Fetch Figure 10: Breakdown for hot read-write 40ms RTT function of network latency, with a fixed server load.
Figure 9 includes both the measured improvement and the improvement derived using the analytical model.Remarkably, the analytical results predict the measured improvement very closely, albeit being somewhat higher than the  empirical values.
The difference is -10% 0% 10% 20% 30% 40% 50% 60% 70% 1 5 1 0 2 0 3 0 4 0 5 0 6 0 7 0 8 0 9 0 100 Latency [ms] Benefits[%] Buddy Reader Buddy Reader (perf model) Buddy Writer Buddy Writer (perf model) Figure 12: Invalidation miss benefit minimal in the "writer group", and somewhat higher in the "reader group" (consistent with the results in the cold cache experiments).
The number of misses avoided by peer fetch depends on k, the number of clients in the BuddyCache, and on the client co-interest in the shared data.
The client cache misses are determined by several  variables, including the workload and the cache configuration.
In the five and ten client group the server fetch cost for individual client decreases because with more clients faulting in a fixed size shared module into BuddyCache, each client needs to perform less server fetches.
We evaluate this latency overhead indirectly by comparing the measured  latency of the Buddy system server fetch or commit request with the measured latency of the corresponding request in the Base system.
To gauge these client side overheads in a multiple server  system, we instrumented the version cache implementation to run with a workload trace that included reordered  invalidations and timed the basic operations.
For a request redirected to the server (server fetch) the extra cost of redirection includes a local request from the client to  redirector on the way to and from the server.
We run the experiments with 1-10 clients in Base, and one or two 1-10 client groups in Buddy.
CrossRedirector, includes the CPU latency of crossing the redirector plus a local network round-trip and is measured by timing a round-trip null request issued by a client to the redirector.
We expect peer fetch to improve the latency cost for client cold cache misses by fetching objects from nearby cache.
The invalidation time cost are  comparable, but since invalidations and updates are processed in the background this cost is less important for the overall performance.
Peer update can also avoid invalidation misses due to false-sharing, introduced when multiple peers update  different objects on the same page concurrently.
It is important that the extra commit time costs are kept to a minimum since client is synchronously waiting for the commit completion.
We do not  analyze this benefit (demonstrated by earlier work [2]) because our benchmarks do not allow us to control object layout, and also because this benefit can be derived given the cache hit rate and workload contention.
The measurements show that the redirection cost of crossing the redirector in not very high even in a local area network.
Let Tinval(Base), Tinval(BCR) and Tinval(BCW ) be the time it takes to complete a single transaction in the Base, BCR and BCW systems.
To evaluate these two benefits we consider the  performance of an application running without BuddyCache with an application running BuddyCache in two configurations.
We are currently working on optimizing the version cache implementation to further reduce these costs.
Tinval(Base) = missinv ∗ tfetch(Base) +tcompute + tcommit(Base) (3) Tinval(BCR) = missinv ∗ ( 1 r ∗ tfetch(BCR) +(1 − 1 r ) ∗ tredirect(BCR)) +tcompute + tcommit(BCR) (4) Tinval(BCW ) = tcompute + tcommit(BCW ) (5) In the experiments described below, we measure the  parameters N, r, missinv, tfetch(S), tredirect(S), tcommit(S), and tcompute(S).
Write transactions use the T2b traversal that is identical to T1 except that it modifies all the atomic parts in a single  composite.
The measurements show that in the worst case, when a large number of invalidations arrive out of order, and about half of the objects modified by T2a (200 objects) reside on reordered pages, the cost of updating the version cache is 0.6 ms.
In groups where peers share a read-only interest in the modified objects, peer fetch allows a client to access a modified object as soon as a collaborating peer has it, which avoids the delay of server fetch without the high cost imposed by the update-only protocols.
We expect a typical BuddyCache configuration not to be cache limited and therefore focus on workloads where the objects in the client working set fit in the cache.
We derive the times by timing the execution of the systems in the local area network environment and  substituting 40 ms and 80 ms delays for the requests crossing the redirector and the server to estimate the performance in the wide-area-network.
Higher network latency increases the relative performance  advantage provided by peer fetch relative to direct fetch but this benefit is offset by the increased commit times.
To study the benefit of avoiding invalidation misses, we consider hot cache performance in a workload with  modifications (with no cold misses).
The latter is an important  benefit because it shows that on workloads with updates, peer fetch allows an invalidation-based protocol to provide some of the benefits of update-based protocol.
6.3.2 Version Cache The solo commit allows a fast client modifying an object to commit independently of a slow peer.
This may happen when a  transaction accesses objects in multiple servers.
In a group containing only readers (BCR), during a steady state execution with uniform updates, a client transaction has missinv  invalidation misses.
To answer the question, we have implemented a  BuddyCache prototype for the OCC protocol and conducted  experiments to analyze the performance benefits and costs over a range of network latencies.
Since the goal of our study is to evaluate how effectively our  techniques deal with access to shared objects, in our study we limit client access to shared data only.
This is because the client in Buddy system is currently single threaded and therefore only starts processing a help request when blocked waiting for a fetch- or commit reply.
We then validate the model by comparing the derived values to the completion times and benefits measured directly in the  experiments.
Table 2 summarizes the latencies that allows us to break down the peer fetch costs.
We focus on a simple case where one client (writer) runs a workload with modifications, and the other clients (readers) run a read-only workload.
6.3.1 Redirection Fetch and commit requests in the BuddyCache cross the redirector, a cost not incurred in the Base system.
6.1.1 The Model The model considers how the time to complete an  execution with and without BuddyCache is affected by  invalidation misses and cold misses.
To make sure that the entire working set fits into the cache we use a single private module and choose a cache size of 40MB for each client.
The overall time includes time spent performing client computation, direct fetch requests, peer fetches, and commit requests.
6.4.1 Cold Misses To evaluate the performance gains from avoiding cold misses we compare the cold cache performance of OO7  benchmark running read-only workload in the Buddy and Base systems.
In Base, one writer and three readers access the server directly.
Both read-only and read-write transactions always work with data from the same module.
Peer update lets a client access an object modified by a nearby collaborating peer without the delay imposed by invalidation-only protocols.
The BuddyCache provides similar  performance improvements as with the 40ms network.
At typical Internet latencies 20ms-60ms the benefit increases with latency and levels off around 60ms with significant (up to 62% for ten clients) improvement.
The server side overheads are minimal and consist of a page version number update at commit time, and a version number comparison at transaction validation time.
Note that the performance benefit delivered by the peer fetch in the Reader group is approximately 50% less than the performance benefit delivered by peer update in the Writer group.
The objects in the database are clustered in 8K pages, which are also the unit of transfer in the fetch requests.
BuddyCache redirection supports the performance  benefits of avoiding communication with the servers but  introduces extra processing cost due to availability mechanisms and request forwarding.
To study the benefit of avoiding cold misses, we consider cold cache performance in a read-only workload (no  invalidation misses).
In Buddy system clients  connect to the redirector that connects to the database.
AlertHelper, includes the time for the helper to notice the request plus a network  roundtrip, and is measured by timing a round-trip null request issued from an auxiliary client to the helper client.
A client starts with a hot cache containing the working set of N pages.
6.4 Overall Performance This section examines the performance gains seen by an application running OO7 benchmark with a BuddyCache in a wide area network.
The Buddy system runs our implementation of BuddyCache prototype in Thor, supporting peer fetch, peer update, and solo commit, but not the failover.
The OO7 database is generated with modules for 3 clients, only one of which is used in our experiments as we explain above.
Assuming uniform access, a client invalidation miss has a chance of 1/r to be the first miss (resulting in server fetch), and a chance of (1 − 1/r) to be redirected.
The cost of the extra mechanism dominates BuddyCache benefit when network latency is low.
In each of the three systems, a single client runs a  readwrite workload (writer) and three other clients run read-only workload (readers).
In a large system with many groups, however, the server cache becomes less efficient.
The multi-user database allocates for each client a private module consisting of one tree of assembly objects, and adds an extra shared module that scales proportionally to the number of clients.
The version cache has an entry only when invalidations or updates arrive out of order.
As in cold cache case, the reason why the  simplified analytical model works well is because it captures the costs of network latency, the dominant performance cost. 
One, where a peer in the group modifies the objects, and another where the objects are modified by a peer outside the group.
Our analysis focuses on cold misses and invalidation misses, since the benefit of avoiding capacity misses can be derived from the cold misses.
Our workloads are based on the multi-user OO7  benchmark [8]; this benchmark is intended to capture the  characteristics of many different multi-user CAD/CAM/CASE  applications, but does not model any specific application.
We directly measured the time to copy and unswizzle the requested page at the helper, (CopyUnswizzle), and timed the crossing times using a null crossing request.
To evaluate the overheads of the peer fetch, we measure the peer fetch latency (PeerFetch) at the requesting client and break down its component costs.
Here again, the reported numbers are derived from the local area network experiment results.
A client starts with a cold cache and runs read-only workload until it  accesses all N pages while committing l transactions.
The fetch cost does not increase as much because the server cache reduces this cost.
The Base system runs Thor distributed object storage system [23] with clients connecting directly to the servers.
All the numbers were computed by averaging measured request latency over 1000 requests.
We consider two types of transaction workloads in our analysis, read-only and read-write.
until an object on the page is invalidated).
Each composite part contains a graph of atomic parts linked by connection objects; each atomic part has 3  outgoing connections.
The results show that the BuddyCache reduces  significantly the completion time compared to the Base system.
The peer update benefit dominates overhead in Writer configuration even in low-latency network (peer update  incurs minimal overhead) and offers significant 44-64%  improvement for entire latency range.
We obtain the  numbers by running 2000 transactions to filter out cold misses and then time the next 1000 transactions.
This benefit is due to peer update that avoids all misses due to updates.
In general they may vary with the server load, e.g.
Since each client accesses the entire shared set r = k. Let Tcold(Base) and Tcold(BC) be the time it takes to complete the l transactions in Base and BC.
The trend toward  increase in available network bandwidth decreases the cost of the update-only protocols.
Our analysis tries, as much as possible, to separate these variables so they can be controlled in the validation  experiments.
Clients running read-write transactions don"t  modify in every transaction, instead they have a 50% probability of running read-only transactions.
The database was stored by a server on a 40GB IBM 7200RPM hard drive, with a 8.5 average seek time and 40 MB/sec data transfer rates.
the client cache is large enough to hold N pages.
Figures 10 and 11 show the overall time to complete 1000 hot cache OO7 read-only transactions.
However, the trend toward increasingly large caches, that are updated when cached  objects are modified, makes invalidation-base protocols more attractive.
Unlike with cold misses, r ≤ k because the second  invalidation disables redirection for P until the next miss on P causes a server fetch.
We use OO7 because it is a standard benchmark for measuring object storage system performance.
The first of the misses fetches P from the server, and the subsequent r − 1 misses are  redirected.
For simplicity, our model assumes the fetch and commit times are constant.
The server, the clients and the redirectors ran on a 850MHz Intel Pentium III  processor based PC, 512MB of memory, and Linux Red Hat 6.2.
The AlertHelper latency which includes the elapsed time from the help request arrival until the start of help request processing is highly variable and therefore contributes to the high variability of the PeerFetch time.
As in cold cache experiments, here the analytical results  predict the measured improvement closely.
In a group containing the writer (BCW ), peer update eliminates all invalidation misses.
In Base system clients  connect directly to the database.
Figure 8 shows the overall time and cost break down in the 80 ms network.
Figures 7 and 8 show the overall time to complete 1000 cold cache transactions.
The main reason why the simplified model works well is it captures the dominant performance component, network latency cost.
The local network latency is fixed and less than 0.1 ms.
Consider an execution with cold misses.
In a specific BuddyCache  execution it is modeled by the variable r, defined as a number of fetches arriving at the redirector for a given version of page P (i.e.
This overhead is not inherent to the BuddyCache architecture and could be  mitigated by a multi-threaded implementation in a system with pre-emptive scheduling.
We compute the completion times derived using the above model and derive the benefits.
Consider k clients running concurrently accessing uniformly a shared set of N pages in BuddyCache (BC) and Base.
33 Tcold(Base) = N ∗ tfetch(Base) +(tcompute + tcommit(Base)) ∗ l (1) Tcold(BC) = N ∗ 1 k ∗ tfetch(BC) + (1 − 1 k ) ∗ tredirect +(tcompute + tcommit(BC)) ∗ l (2) Consider next an execution with invalidation misses.
The OO7 database  contains a tree of assembly objects with leaves pointing to three composite parts chosen randomly from among 500 such  objects.
These can be cold misses, invalidation misses and capacity misses.
This simple configuration is sufficient to show the impact of  BuddyCache techniques.
Moreover, technology trends indicate that memory and storage capacity will continue to grow and therefore a typical  BuddyCache configuration is likely not to be cache limited.
A single transaction includes one traversal and there is no sleep time between transactions.
The figure includes both the measured improvement and the improvement derived using the analytical model.
We  assume there are no capacity misses, i.e.
Buddy system with one group containing a single writer and another group  running three readers models the Reader group.
In BC, r cold misses for page P reach the redirector.
This difference is similar in 80ms network.
In our implementation of OO7, the private module size is about 38MB.
They were connected by a 100Mb/s Ethernet.
This section presents a simple analytical performance model for this benefit.
The OO7 benchmark generates database  modules of predefined size.
We use two systems in our experiments.
Technology trends indicate that both benefits will remain important in the foreseeable future.
We use a medium database that has 200 atomic parts per composite part.
Is the cure worse then the  disease?
6.2 Experimental Setup Before presenting our results we describe our experimental setup.
The experiments ran in Utah experimental testbed emulab.net [1].
