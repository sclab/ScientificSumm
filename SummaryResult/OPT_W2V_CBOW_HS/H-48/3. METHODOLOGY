In  contrast, the controlled variable in this framework is the query term that has been removed from the documents relevant to that query, as determined by the removal strategy, S. Query terms are removed one by one, in a manner and order  determined by S, so that collections differ only with respect to the one term that has been removed (or masked) in the documents relevant to that query.
We systematically remove query terms from known relevant documents,  creating alternate versions of a test collection that differ only in how many or which query terms have been removed from the documents relevant to a particular query.
Query terms are removed from documents one by one, so the differences in IR system performance can be  measured with respect to missing terms.
Systematically removing query terms from known relevant documents introduces a controlled amount of query-document term mismatch by which we can  evaluate the degree to which particular QE techniques are able to retrieve conceptually relevant documents, despite a lack of actual term overlap.
This removal process affects only the relevant documents in the search collection.
The evaluation framework presented in this paper consists of three elements: a test collection, C; a strategy for selecting which query terms to remove from the relevant documents in that collection, S; and a metric by which to compare  performance of the IR systems, m. The test collection, C, consists of a document collection, queries, and relevance judgments.
Although, on the surface, we are changing the  distribution of terms between the relevant and non-relevant  documents sets by removing query terms from the relevant  documents, doing so does not change the conceptual relevancy of these documents.
The strategy, S, determines the order and manner in which query terms are removed from the relevant documents in C. This evaluation framework is not metric-specific; any metric (MAP, P@10, recall, etc.)
The incremental additive removal of query terms from relevant documents allows the evaluation to show the  degree to which IR system performance degrades as more and more query terms are missing, thereby increasing the degree of query-document term mismatch.
Removing terms  individually allows for a clear comparison of the contribution of QE in the absence of each individual query term. 
In fact, using this framework stretches the value of existing test collections in that one  collection becomes several when query terms are removed from relevant documents, thereby increasing the amount of  information that can be gained from evaluating on a particular collection.
Removing a query term from  relevant documents simply masks the presence of that query term in those documents.
Notice that, for a given query, only relevant documents are modified.
In order to accurately measure IR system performance in the presence of query-term mismatch, we need to be able to adjust the degree of term mismatch in a test corpus in a principled manner.
Based on the purpose of the evaluation and the retrieval algorithm being used, it might make more sense to choose a removal order for S based on query term IDF or perhaps based on a measure of query term probability in the document collection.
In other evaluations of QE effectiveness, the controlled variable is simply whether or not queries have been  expanded or not, compared in terms of some metric.
It does not in any way change the conceptual relevancy of the documents.
Introducing query-document term mismatch into the test collection in this manner allows us to manipulate the degree of term  mismatch between relevant documents and queries in a  controlled manner.
In the most extreme case (i.e., when the length of the query is less than or equal to the number of query terms removed from the relevant documents), there will be no term overlap between a query and its relevant documents.
Possible orders for removal could be based on metrics such as IDF or the global probability of a term in a document collection.
Our approach is to introduce  querydocument term mismatch into a corpus in a controlled  manner and then measure the performance of IR systems as the degree of term mismatch changes.
Non-relevant  documents are left unchanged, even in the case that they contain query terms.
Two  decisions must be made when choosing a removal strategy S. The first is the order in which S removes terms from the relevant documents.
It is in this way that we can explicitly measure the degree to which an IR system overcomes query-document term mismatch.
The choice of a query term removal strategy is relatively flexible; the only restriction in choosing a strategy S is that query terms must be removed one at a time.
Although test collections are difficult to come by, it should be noted that this evaluation framework can be used on any available test collection.
It must be  determined if S will remove the terms individually (i.e., remove just one different term each time) or additively (i.e., remove one term first, then that term in addition to another, and so on).
The queries themselves remain  unaltered.
Once an order for removal has been decided, a manner for term removal/masking must be decided.
can be used to measure IR system performance.
