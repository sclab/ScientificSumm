We provide a framework for the analysis of the trade-off between static caching of query answers and posting lists; • Static caching of terms can be more effective than  dynamic caching with, for example, LRU.
We provide algorithms based on the Knapsack problem for  selecting the posting lists to put in a static cache, and we show improvements over previous work, achieving a hit ratio over 90%; • Changes of the query distribution over time have little impact on static caching.
The primary use of a cache memory is to speedup  computation by exploiting frequently or recently used data,  although reducing the workload to back-end servers is also a major goal.
In this paper we explore the trade-offs in the design of each cache level, showing that the problem is the same and only a few parameters change.
Broker Static caching posting lists Dynamic/Static cached answers Local query processor Disk Next caching level Local network access Remote network access Figure 1: One caching level in a distributed search architecture.
Millions of queries are submitted daily to Web search  engines, and users have high expectations of the quality and speed of the answers.
As posting lists have variable size, caching them dynamically is not very efficient, due to the complexity in terms of  efficiency and space, and the skewed distribution of the query stream, as shown later.
In such a setting, to achieve a fast response time and to increase the query throughput, using a cache is crucial.
Static caching of posting lists poses even more challenges: when deciding which terms to cache one faces the trade-off between frequently queried terms and terms with small posting lists that are space efficient.
Sections 5 and 6 introduce algorithms for caching posting lists, and a theoretical framework for the analysis of static caching, respectively.
As the searchable Web becomes larger and larger, with more than 20 billion pages to index,  evaluating a single query requires processing large amounts of data.
A static cache is based on historical information and is periodically updated.
In general, we assume that each level of caching in a distributed search architecture is similar to that shown in Figure 1.
Returning an answer to a query that already exists in the cache is more efficient than computing the answer using cached posting lists.
The decision of what to cache is either off-line (static) or online (dynamic).
Section 7 discusses the impact of changes in the query distribution on static caching, and Section 8 provides concluding remarks. 
We use a query log spanning a whole year to explore the limitations of dynamically caching query answers or posting lists for query terms.
Finally, before deciding to adopt a static caching policy the query stream should be analyzed to verify that its  characteristics do not change rapidly over time.
Caching terms: As the engine evaluates a particular query, it may decide to store in memory the posting lists of the involved query terms.
Section 4 discusses the limitations of  dynamic caching.
Caching of posting lists has additional challenges.
More concretely, our main conclusions are that: • Caching query answers results in lower hit ratios  compared to caching of posting lists for query terms, but it is faster because there is no need for query  evaluation.
Such online decisions are based on a cache policy, and several different policies have been studied in the past.
For a search engine, there are two possible ways to use a cache memory: Caching answers: As the engine returns answers to a  particular query, it may decide to store these answers to resolve future queries.
Often the whole set of  posting lists does not fit in memory, and consequently, the engine has to select a small set to keep in memory and speed up query processing.
Caching can be applied at different levels with increasing response latencies or processing requirements.
When a new request arrives, the cache system decides whether to evict some entry from the cache in the case of a cache miss.
A dynamic cache replaces entries according to the sequence of requests.
For example, the different levels may correspond to the main memory, the disk, or resources in a local or a wide area network.
The remainder of this paper is organized as follows.
On the other hand, previously unseen queries occur more often than previously unseen terms,  implying a higher miss rate for cached answers.
Sections 2 and 3 summarize related work and characterize the data sets we use.
