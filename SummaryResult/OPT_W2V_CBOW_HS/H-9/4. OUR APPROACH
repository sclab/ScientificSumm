Based on the similarity scores, we rank all the documents in H. The top ranked documents provide us a working set to learn the aspects that users are usually interested in.
Each aspect is labeled with a  representative query.
All the clusters obtained are related to the input query q from different perspectives, and they represent the possible aspects of interests about query q of users.
This center query can be regarded as the most representative one for the whole cluster, and thus provides a label for the cluster naturally.
4.2.1 Star Clustering Given Hq, star clustering starts with constructing a  pairwise similarity graph on this collection based on the vector space model in information retrieval [18].
Each document in H corresponds to a past query, and thus the top ranked documents correspond to q"s related past queries.
4.3 Categorizing Search Results In order to organize the search results according to users" interests, we use the learned aspects from the related past queries to categorize the search results.
Based on the pseudo-documents in each discovered aspect Ci, we build a centroid prototype pi by taking the average of all the vectors of the documents in Ci: pi = 1 |Ci|   l∈Ci vl.
These clusters form a cover of the similarity graph.
Given the top m Web pages returned by a search engine for q: {s1, ..., sm}, we group them into different aspects using a categorization algorithm.
Then, for each pair of documents di and dj (i = j), their similarity is computed as the cosine score of their corresponding vectors vi and vj , that is sim(di, dj ) = cos(vi, vj) = vi · vj |vi| · |vj | .
Specifically, we use the following formula to calculate the similarity between query q and pseudo-document Qi:   w∈q ¡ Qi c(w, q) × IDF(w) × (k1 + 1) × c(w, Qi) k1((1 − b) + b |Qi| avdl ) + c(w, Qi) where k1 and b are OKAPI parameters set empirically, c(w, Qi) and c(w, q) are the count of word w in Qi and q respectively, IDF(w) is the inverse document frequency of word w, and avdl is the average document length in our history  collection.
All the aspects are finally ranked according to the number of search results they have.
A good feature of the star clustering algorithm is that it outputs a center for each cluster.
4.2 Learning Aspects by Clustering Given a query q, we use Hq = {d1, ..., dn} to represent the top ranked pseudo-documents from the history collection H. These pseudo-documents contain the aspects that users are interested in.
Each Qi  corresponds to a unique query and is enriched with clickthrough information as discussed in Section 3.
To know what the users are really interested in given this query, we first retrieve its past similar queries in our preprocessed history data collection.
After the similarity graph Gσ is built, the star clustering algorithm clusters the documents using a greedy algorithm as follows: 1.
We then assign sj to the aspect with which it has the highest cosine similarity score.
Given an input query, the general procedure of our approach is: 1.
4.1 Finding Related Past Queries Given a query q, a search engine will return a ranked list of Web pages.
Categorize and organize the search results of the input query according to the aspects learned above.
A good property of the star clustering in our setting is that it can suggest a good label for each cluster naturally.
In the past query  collection Hq, each document corresponds to a query.
In this subsection, we propose to use a clustering method to discover these aspects.
There is only one parameter σ in the star clustering algorithm.
In this paper, we use an algorithm based on graph partition: the star clustering algorithm [2].
To find q"s related queries in H, a natural way is to use a text retrieval  algorithm.
Learn aspects from the information in the working set.
Each document di is a vertex of Gσ.
These aspects correspond to users" interests given the input query.
Formally, for each of the n pseudo-documents {d1, ..., dn} in the collection Hq, we compute a TF-IDF vector.
Specifically, for any search result sj, we build a TF-IDF vector.
A similarity graph Gσ can then be constructed as follows using a similarity threshold parameter σ.
A big σ enforces that the connected documents have high similarities, and thus the clusters tend to be small.
Within each aspect, the search results are ranked according to their original search engine ranking. 
The centroid-based method computes the cosine similarity between the vector representation of sj and each centroid prototype pi.
All the information forms a working set.
All these pi"s are used to categorize the search results.
Then the clusters are formed by dense subgraphs that are star-shaped.
Our approach is to organize search results by aspects learned from search engine logs.
On the other hand, a small σ will make the clusters big and less coherent.
Any clustering algorithm could be applied here.
Form a cluster C containing u and all its neighbors that are not marked as center.
Here we use the OKAPI method [17], one of the state-of-the-art retrieval methods.
From those unmarked vertices, find the one which has the highest degree and let it be u.
We describe the star clustering algorithm below.
Formally, assume we have N pseudo-documents in our history data set: H = {Q1, Q2, ..., QN }.
In principle, any categorization algorithm can be used here.
Get its related information from search engine logs.
Here we use a simple centroid-based method for  categorization.
Associate every vertex in Gσ with a flag, initialized as unmarked.
Repeat from step 2 until all the vertices in Gσ are marked.
We now give a detailed presentation of each step.
We will study the impact of this parameter in our experiments.
Each cluster is star-shaped, which consists a single center and several satellites.
If sim(di, dj) > σ, there would be an edge connecting the corresponding two vertices.
Naturally, more sophisticated methods such as SVM [21] may be expected to achieve even better  performance.
Mark the flag of u as center.
Mark all the selected neighbors as satellites.
