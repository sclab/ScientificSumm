Since we want to test whether our algorithm can learn from the past queries, we also filter out those test cases whose queries can not retrieve at least 100 pseudo-documents from our history collection.
For each test case, i.e., each session, we count the number of different clicks and filter out those test cases with fewer than 4 clicks under the assumption that a query with more clicks is more likely to be an informational query.
On average, we have 6.23 and 5.89 clicks for each test case in the two test sets respectively. 
Using clicks as judgments, we can then compare different algorithms for  organizing search results to see how well these algorithms can help users reach the clicked URLs.
Since it is infeasible to ask the original user who submitted a query to judge the results for the query, we follow the work [11] and opt to use the clicks associated with the query in a session to approximate relevant documents.
To test our algorithm, we separate the whole data set into two parts according to the time: the first 2/3 data is used to simulate the historical data that a search engine  accumulated, and we use the last 1/3 to simulate future queries.
After cleaning, we get 169,057 unique queries in our history data collection totally.
For each test set, we use every session as a test case.
Different test cases may have the same queries but possibly different clicks.)
The average length of these pseudo-documents is 68 words and the total data size of our history collection is 129MB.
We construct our test data from the last 1/3 data.
Each session  contains a single query and several clicks.
Organizing search results into different aspects is expected to help informational queries.
According to the time, we separate this data into two test sets equally for cross-validation to set parameters.
There are 8,144,000 queries, 3,441,000 distinct queries, and 4,649,000 distinct URLs in the raw data.
In the history collection, we clean the data by only  keeping those frequent, well-formatted, English queries (queries which only contain characters ‘a", ‘b", ..., ‘z", and space, and appear more than 5 times).
We build the pseudo-documents for all these queries as described in Section 3.
On average, each query has 3.5 distinct clicks.
It thus makes sense to focus on the informational queries in our evaluation.
Finally, we obtain 172 and 177 test cases in the first and second test sets respectively.
(Note that we do not aggregate sessions for test cases.
We construct our data set based on the MSN search log data set released by the Microsoft Live Labs in 2006 [14].
In total, this log data spans 31 days from 05/01/2006 to 05/31/2006.
