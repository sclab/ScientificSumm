Hence, we will also use RankNet as a generic ranker to explore the contribution of implicit feedback for different ranking alternatives. 
The approach above assumes that there are no interactions between the underlying features producing the original web search ranking and the implicit feedback features.
While there has been significant work on merging multiple rankings, we adapt a simple and robust approach of ignoring the original rankers" scores, and instead simply merge the rank orders.
FEEDBACK We consider two complementary approaches to ranking with implicit feedback: (1) treating implicit feedback as independent evidence for ranking results, and (2) integrating implicit feedback features directly into the ranking algorithm.
3.2 Ranking with Implicit Feedback Features Modern web search engines rank results based on a large number of features, including content-based features (i.e., how closely a query matches the text or title or anchor text of the document), and query-independent page quality features (e.g., PageRank of the document or the domain).
We compute a merged score SM(d) for d by combining the ranks obtained from implicit feedback, Id with the original rank of d, Od:     ¡     ¢ £ + + + + = otherwise O dforexistsfeedbackimplicitif OI w wOIdS d dd I IddM 1 1 1 1 1 1 ),,,( where the weight wI is a heuristically tuned scaling factor representing the relative importance of the implicit feedback.
Hence, a natural approach is to incorporate implicit feedback features directly as features for the ranking algorithm.
For a given query q, the implicit score ISd is computed for each result d from available user interaction features, resulting in the implicit rank Id for each result.
The main reason for ignoring the original scores is that since the feature spaces and learning algorithms are different, the scores are not directly comparable, and re-normalization tends to remove the benefit of incorporating classifier scores.
3.1 Implicit Feedback as Independent Evidence The general approach is to re-rank the results obtained by a web search engine according to observed clickthrough and other user interactions for the query in previous search sessions.
In most cases, automatic (or  semiautomatic) methods are developed for tuning the specific ranking function that combines these feature values.
An attractive feature of RankNet is both train- and run-time efficiency - runtime ranking can be quickly computed and can scale to the web, and training can be done over thousands of queries and associated judged results.
We found that a simple rank merging heuristic combination works well, and is robust to variations in score values from original rankers.
3.3 Learning to Rank Web Search Results A key aspect of our approach is exploiting recent advances in machine learning, namely trainable ranking algorithms for web search and information retrieval (e.g., [5, 11] and classical results reviewed in [3]).
We now describe such a ranker that we used to learn over the combined feature sets including implicit feedback.
This model requires a ranking algorithm to be robust to missing values: more than 50% of queries to web search engines are unique, with no previous implicit feedback available.
Hence, an attractive choice to use is a supervised machine learning technique to learn a ranking function that best predicts relevance judgments.
Furthermore, RankNet can learn with many (differentiable) cost functions, and hence can automatically learn a ranking function from human-provided labels, an attractive alternative to heuristic feature combination techniques.
Each result is assigned a score according to expected relevance/user satisfaction based on previous interactions, resulting in some preference ordering based on user interactions alone.
One special case of this model arises when setting wI to a very large value, effectively forcing clicked results to be ranked higher than un-clicked results - an intuitive and effective heuristic that we will use as a baseline.
We now relax this assumption by integrating implicit feedback features directly into the ranking process.
At runtime, the search engine would fetch the implicit feedback features associated with each query-result URL pair.
Applying more sophisticated classifier and ranker combination algorithms may result in additional improvements, and is a promising direction for future work.
In our setting, explicit human relevance judgments (labels) are available for a set of web search queries and results.
While the specific training algorithms used by RankNet are beyond the scope of this paper, it is described in detail in [5] and includes extensive evaluation and comparison with other ranking methods.
The specific implicit feedback features are described in Section 4, and the algorithms for interpreting and incorporating implicit feedback are described in Section 5.
We experimented with a variety of merging functions on the development set of queries (and using a set of interactions from a different time period from final evaluation sets).
It is a neural net tuning algorithm that optimizes feature weights to best match explicitly provided pairwise user preferences.
The query results are ordered in by decreasing values of SM to produce the final ranking.
We describe the two general ranking approaches next.
We use a 2-layer implementation of RankNet in order to model non-linear relationships between features.
During training or tuning, the ranker can be tuned as before but with additional features.
RankNet is one such algorithm.
