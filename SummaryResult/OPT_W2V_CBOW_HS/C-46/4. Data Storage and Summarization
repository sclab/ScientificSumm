Instead, they rely on the proxies to maintain this metadata index-sensors periodically send the proxy information  summarizing the data contained in a contiguous sequence of records, as well as a handle indicating the location of these records in flash memory.
Changing the size of the data summary can be performed in an application-specific manner (e.g.
Thus, random  access is enabled at granularity of a summary-the start offset of each chunk of records represented by a summary is known to the proxy.
The state of the archive is captured in the metadata associated with the summaries, and is stored and maintained at the proxy.
The resolution of the summary depends on two  parametersthe interval over which summaries of the data are constructed and transmitted to the proxy, as well as the size of the  applicationspecific summary.
When older data is overwritten, a delete operation is performed, where an index entry is deleted from the interval skip graph at the proxy and the corresponding storage space in flash memory at the sensor is freed.
The former index enables quick lookups on all records seen during a certain interval, while the latter index enables quick lookups on all records matching a certain value.
Sensors produce time-series data streams, and therefore, temporal ordering of data is a natural and simple way of storing archived sensor data.
proxy proxy proxy record 3 record summary local archive in flash memory data summary start,end offset time interval sensor summary sent to proxy Insert summaries into interval skip graph Figure 7: Sensor Summarization 45 The read operation enables stored records to be retrieved in  order to answer queries.
All fields of the record need to be specified at creation time; thus, the size of the record is known a priori and the store simply allocates the the corresponding number of bytes at the tail to store the record.
The key intuition is that each sensor can independently identify the fraction of false hits and true hits for queries that access its local archive.
The mechanism works as follows: In addition to the summary of sensor data, each node sends metadata to the proxy containing the time interval corresponding to the summary, as well as the start and end offsets of the flash memory location where the raw data corresponding is stored (as shown in Figure 7).
4.2 Adaptive Summarization The data summaries serve as glue between the storage at the  remote sensor and the index at the proxy.
In general, the proxy can index the time interval representing a summary or the value range reported in the summary (or both).
In order to support a variety of applications, TSAR supports variable-length data fields; as a result, record sizes can vary from one record to another.
Due to the append-only nature of the store,  creation of records is simple and efficient.
In addition to  simplicity, a temporally ordered store is often suitable for many sensor data processing tasks since they involve time-series data processing.
If many queries result in false hits, then the sensor makes the granularity of each summary finer to reduce the number and overhead of false hits.
When a query matches a summary in the index, the sensor uses these offsets to access the relevant records on its local flash by sequentially  reading data from the start address until the end address.
These queries are then routed to the sensors, which locate the relevant data records in the local archive and respond back to the proxy.
Consequently, the local archival store is a collection of records, designed as an append-only circular buffer, where new records are appended to the tail of the buffer.
The second is an adaptive summarization technique that enables each sensor to adapt to changing data and query characteristics.
If most queries result in true hits, then the sensor  determines that the summary can be coarsened further to reduce update costs without adversely impacting the hit ratio.
As described in Section 2.4, there is a trade-off between the  energy used in sending summaries (and thus the frequency and  resolution of those summaries) and the cost of false hits during queries.
To enable such lookups, each sensor node in TSAR maintains an archival store of sensor data.
Each update from a sensor to the proxy includes three pieces of information: the summary, a time period corresponding to the summary, and the start and end offsets for the flash archive.
Raw  sensor data is stored in the data field of the record.
While we anticipate local storage capacity to be large, eventually there might be a need to overwrite older data, especially in high data rate applications.
A camera-based sensor, for instance, may store binary images in this data field.
The coarser and less frequent the summary information, the less energy required, while false query hits in turn waste energy on  requests for non-existent data.
Consequently, TSAR sensors do not maintain any index for the data stored in their archive.
This may be done via techniques such as multi-resolution storage of data [9], or just simply by overwriting older data.
Our archival store supports three operations on records: create, read, and delete.
In a traditional database system, efficient lookups are enabled by maintaining a structure such as a B-tree that indexes certain keys of the records.
Any  queryspecific operation can then be performed on this data.
Thus, no index needs to be maintained at the sensor, in line with our goal of simplifying sensor state management.
Since records are always written at the tail, the store need not maintain a free space list.
Currently, TSAR employs a simple  summarization scheme that computes the ratio of false and true hits and  decreases (increases) the interval between summaries whenever this ratio increases (decreases) beyond a threshold.
Timestamp Calibration Parameters Opaque DataData/Event Attributes size Figure 6: Single storage record Sensor data has very distinct characteristics that inform our  design of the TSAR archival store.
Having described the proxy-level index structure, we turn to the mechanisms at the sensor tier.
Each record has a metadata field which includes a timestamp, sensor settings, calibration parameters, etc.
The first is a local archival store at each sensor node that is optimized for resource-constrained devices.
Within this collection, records are accessed sequentially.
The create operation simply creates a new record and appends it to the tail of the store.
While the implementation of such an archival store is straightforward on resource-rich devices that can run a database, sensors are often power and resource-constrained.
4.1 Local Storage at Sensors Interval skip graphs provide an efficient mechanism to lookup sensor nodes containing data relevant to a query.
The data field is opaque and application-specific-the storage system does not know or care about interpreting this field.
Since writes are immutable, the size of a record does not change once it is created.
Consequently, the sensor archiving subsystem in TSAR is explicitly designed to exploit characteristics of sensor data in a resource-constrained  setting.
However, this can be quite  complex for a small sensor node with limited resources.
Our focus in this paper is on the interval over which the summary is constructed.
TSAR employs an adaptive summarization technique that  balances the cost of sending updates against the cost of false positives.
TSAR implements two key  mechanisms at the sensor tier.
The format of each data record is shown in Figure 6.
using wavelet compression techniques as in [9]) and is beyond the scope of this paper.
Examples include signal processing operations such as FFT, wavelet transforms, clustering, similarity matching, and target detection.
The rest of this section describes these mechanisms in detail.
