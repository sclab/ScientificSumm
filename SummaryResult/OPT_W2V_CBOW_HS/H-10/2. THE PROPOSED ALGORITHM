Due to the simplicity and effectiveness of the regularized linear classifier that we have introduced in section 2.2.1, we choose it to be our local label predictor, such that for each document xi, the following criterion is minimized Ji = 1 ni xj ∈Ni wT i xj − qj 2 + λi wi 2 , (7) where ni = |Ni| is the cardinality of Ni, and qj is the  cluster membership of xj.
qT q = 1 (20) Using the Lagrangian method, we can derive that the  optimal solution q corresponds to the smallest eigenvector of the matrix M = (P − I)T (P − I) + λL, and the cluster  assignment of xi can be determined by the sign of qi, i.e.
The basic idea is that, for each document vector xi (1 i n), we train a local label predictor based on its k-nearest neighborhood Ni, and then use it to predict the label of xi.
2 We believe that the text data are also sampled from some low dimensional manifold, since it is impossible for them to for ∀x ∈ M, q(x) returns the cluster membership of x.
To avoid such a problem, we can add a regularization term and minimize the following criterion J = 1 n n i=1 (wT xi − yi)2 + λ w 2 , (5) where λ is a regularization parameter.
The smoothness of q over M can be calculated by the following Dirichlet integral [2] D[q] = 1 2 M q(x) 2 dM, (13) where the gradient q is a vector in the tangent space T Mx, and the integral is taken with respect to the standard  measure on M. If we restrict the scale of q by q, q M = 1 (where ·, · M is the inner product induced on M), then it turns out that finding the smoothest function  minimizing D[q] reduces to finding the eigenfunctions of the Laplace Beltrami operator L, which is defined as Lq −div q, (14) where div is the divergence of a vector field.
The term-frequency vector xi of  document di is defined as xi = [xi1, xi2, · · · , xim]T , xik = tik log n idfk , where tik is the term frequency of wk ∈ W, n is the size of the document corpus, idfk is the number of documents that contain word wk.
2.3 Global Regularization In data clustering, we usually require that the cluster  assignments of the data points should be sufficiently smooth with respect to the underlying data manifold, which implies (1) the nearby points tend to have the same cluster  assignments; (2) the points on the same structure (e.g.
In the next subsection, we will introduce a global regularization criterion and combine it with Jl, which aims to find a good clustering result in a local-global way.
First we assume that all the documents belong to C classes indexed by L = {1, 2, · · · , C}.
Then (wc∗ i )T xi returns the predicted confidence of xi  belonging to class c. Hence the local prediction error for class c can be defined as J c l = n i=1 (wc∗ i ) T xi − qc i 2 , (22) And the total local prediction error becomes Jl = C c=1 J c l = C c=1 n i=1 (wc∗ i ) T xi − qc i 2 .
QT Q = I, (27) From the Ky Fan theorem [28], we know the optimal solution of the above problem is Q∗ = [q∗ 1, q∗ 2, · · · , q∗ C ]R, (28) where q∗ k (1 k C) is the eigenvector corresponds to the k-th smallest eigenvalue of matrix (P − I)T (P − I) + λL, and R is an arbitrary C × C matrix.
Therefore, in this multi-class case, for each document xi (1 i n), we will construct C locally linear regularized label predictors whose normal vectors are wc∗ i = Xi XT i Xi + λiniIi −1 qc i (1 c C), (21) where Xi = [xi1, xi2, · · · , xini ] with xik being the k-th  neighbor of xi, and qc i = [qc i1, qc i2, · · · , qc ini ]T with qc ik = qc (xik).
A natural way for making the problem solvable is to remove the constraint and relax qi to be  continuous, then the objective that we aims to minimize becomes J = Pq − q 2 + λqT Lq = qT (P − I)T (P − I)q + λqT Lq = qT (P − I)T (P − I) + λL q, (19) and we further add a constraint qT q = 1 to restrict the scale of q.
(13) corresponds to  minimizing Jg = qT Lq = n i=1 (qi − qj)2 wij, (15) where q = [q1, q2, · · · , qn]T with qi = q(xi), L is the graph Laplacian with its (i, j)-th entry Lij =    di − wii, if i = j −wij, if xi and xj are adjacent 0, otherwise, (16) where di = j wij is the degree of xi, wij is the similarity between xi and xj.
By taking ∂J /∂w = 0, we get the solution w∗ = n i=1 xixT i −1 n i=1 xiyi , (2) which can further be written in its matrix form as w∗ = XXT −1 Xy, (3) where X = [x1, x2, · · · , xn] is an m × n document matrix, y = [y1, y2, · · · , yn]T is the label vector.
(25) Then the criterion to be minimized for CLGR in multi-class case becomes J = Jl + λJg = C c=1 Pqc − qc 2 + λ(qc )T Lqc = C c=1 (qc )T (P − I)T (P − I) + λL qc = trace QT (P − I)T (P − I) + λL Q , (26) where Q = [q1 , q2 , · · · , qc ] is an n × c matrix, and trace(·) returns the trace of a matrix.
Let αi = xT i Xi XT i Xi + λiniIi −1 , then Pij = αi j, if xj ∈ Ni 0, otherwise , (12) where Pij is the (i, j)-th entry of P, and αi j represents the j-th entry of αi .
Since w∗ i = XiXT i + λiniI −1 Xiqi, then XiXT i + λiniI w∗ i = Xiqi =⇒ XiXT i w∗ i + λiniw∗ i = Xiqi =⇒ w∗ i = (λini)−1 Xi qi − XT i w∗ i .
This is an attractive expression since we can determine the cluster assignment of u by using the inner-products between the points in {u ∪ Ni}, which suggests that such a local regularizer can easily be kernelized [21] as long as we define a proper kernel function.
For example, in the two-class classification  scenario1 (in which we exactly know the label of each  document), a linear classifier with least square fit aims to learn a column vector w such that the squared cost J = 1 n (wT xi − yi)2 (1) is minimized, where yi ∈ {+1, −1} is the label of xi.
Therefore w∗ i = Xiβ = Xi XT i Xi + λiniIi −1 qi 2 Using theorem 1, we only need to compute the inverse of an ni × ni matrix for every document to train a local label predictor.
Furthermore, we also normalize each xi (1 i n) to have a unit length, so that each document is represented by a normalized TF-IDF vector.
In this way, xi is also called the  TFIDF representation of document di.
2.4 Clustering with Local and Global Regularization Combining the contents we have introduced in section 2.2 and section 2.3 we can derive the clustering criterion is minq J = Jl + λJg = Pq − q 2 + λqT Lq s.t.
(6), we can get Jl = n i=1 w∗T i xi − qi 2 = n i=1 xT i Xi XT i Xi + λiniIi −1 qi − qi 2 = Pq − q 2 , (11) where q = [q1, q2, · · · , qn]T , and the P is an n × n matrix constructing in the following way.
(24) Similarly we can define the global smoothness regularizer in multi-class case as Jg = C c=1 n i=1 (qc i − qc j )2 wij = C c=1 (qc )T Lqc .
Without the loss of generality, we assume that the data points reside (roughly) on a low-dimensional manifold M2 , and q is the cluster assignment function defined on M, i.e.
2.2.3 Combining the Local Regularized Predictors After all the local predictors having been constructed, we will combine them together by minimizing Jl = n i=1 w∗T i xi − qi 2 , (10) which stands for the sum of the prediction errors for all the local predictors.
qc is the classification  function for class c (1 c C), such that qc (xi) returns the confidence that xi belongs to class c. Our goal is to obtain the value of qc (xi) (1 c C, 1 i n), and the cluster assignment of xi can be determined by {qc (xi)}C c=1 using some proper discretization methods that we will introduce later.
Let W = {w1, w2, · · · , wm} be the complete vocabulary set of the document corpus (which is preprocessed by the stopwords removal and words stemming operations).
(15) with exponential weights can effectively measure the smoothness of the data assignments with respect to the intrinsic data manifold.
we should compute the inverse of an m × m matrix for  every document vector, which is computationally prohibited.
Let β = (λini)−1 qi − XT i w∗ i , then w∗ i = Xiβ =⇒ λiniβ = qi − XT i w∗ i = qi − XT i Xiβ =⇒ qi = XT i Xi + λiniIi β =⇒ β = XT i Xi + λiniIi −1 qi.
(6), we can get the optimal solution is w∗ i = XiXT i + λiniI −1 Xiqi, (8) where Xi = [xi1, xi2, · · · , xini ], and we use xik to denote the k-th nearest neighbor of xi.
As in [20], we can treat the i-th row of Q as the  embedding of xi in a C-dimensional space, and apply some traditional clustering methods like kmeans to  clustering these embeddings into C clusters.
w∗ is  estimated using the whole training set.
Generally speaking, learning can be posed as a problem of function estimation, from which we can get a good  classification function that will assign labels to the training dataset and even the unseen testing dataset with some cost  minimized [24].
Moreover, for a new testing point u that falls into Ni, we can classify it by the sign of qu = w∗T i u = uT wi = uT Xi XT i Xi + λiniIi −1 qi.
Thus we adopt it as a global regularizer to punish the smoothness of the predicted data assignments.
Since the values of the entries in Q∗ is continuous, we need to further discretize Q∗ to get the cluster assignments of all the data points.
Till now we can write the criterion of clustering by  combining locally regularized linear label predictors Jl in an explicit mathematical form, and we can minimize it directly using some standard optimization techniques.
(12)) and rewrite Jl as Jl = C c=1 J c l = C c=1 Pqc − qc 2 .
Then the optimal solution that minimize J is given by w∗ = XXT + λnI −1 Xy, (6) where I is an m × m identity matrix.
2.1 Document Representation In our work, all the documents are represented by the weighted term-frequency vectors.
2.5 Multi-Class CLGR In the above we have introduced the basic framework of Clustering with Local and Global Regularization (CLGR) for the two-class clustering problem, and we will extending it to multi-class clustering in this subsection.
Then for a test document t, we can determine its label by l = sign(w∗T u), (4) where sign(·) is the sign function.
Generally, the graph can be viewed as the discretized form of manifold.
For example, a testing point will be classified by the local  classifier trained using the training points located in the vicinity 1 In the following discussions we all assume that the  documents coming from only two classes.
If xi and xj are adjacent3 , wij is usually computed in the following way wij = e − xi−xj 2 2σ2 , (17) where σ is a dataset dependent parameter.
Finally we will combine all those local predictors by minimizing the sum of their prediction errors.
(20), we also add the constraint that QT Q = I to restrict the scale of Q.
(8) can be rewritten as w∗ i = Xi XT i Xi + λiniIi −1 qi, (9) where Ii is an ni × ni identity matrix.
We can model the dataset as an weighted  undirected graph as in spectral clustering [22], where the graph nodes are just the data points, and the weights on the edges represent the similarities between pairwise points.
According to [24], this may not be a smart idea, since a unique w∗ may not be good enough for predicting the labels of the whole input space.
2.2.2 Constructing the Local Regularized Predictors Inspired by their success, we proposed to apply the local learning algorithms for clustering.
The problem here is that XiXT i is an m × m matrix with m ni, i.e.
Since the optimal Q∗ is not unique (because of the existence of an arbitrary matrix R), we can pursue an optimal R that will rotate Q∗ to an indication matrix4 .
3 In this paper, we define xi and xj to be adjacent if xi ∈ N(xj) or xj ∈ N(xi).
qi = [qi1, qi2, · · · , qini ]T with qik representing the cluster assignment of xik.
In this section, we will introduce our Clustering with Local and Global Regularization (CLGR) algorithm in detail.
First let"s see the how the documents are represented throughout this paper.
However, despite its empirical success, the regularized  linear classifier is on earth a global classifier, i.e.
It is proved that under certain conditions, such a form of wij to determine the weights on graph edges leads to the convergence of graph Laplacian to the Laplace Beltrami operator [3][18].
2.2.1 Motivation As we know that clustering is one type of learning  techniques, it aims to organize the dataset in a reasonable way.
(3) is that the matrix XXT may be singular and thus not invertable (e.g.
Then it can be shown that minimizing Eq.
In order to get better predictions, [6] proposed to train  classifiers locally and use them to classify the testing points.
xi will be classified as class one if qi > 0, otherwise it will be classified as class 2.
(11), we can define an n×n matrix P (see Eq.
In this subsection we will introduce how to construct those local predictors.
The generalizations of our method to multi-class cases will be discussed in section 2.5. of it.
(12), and λ is a regularization parameter to trade off Jl and Jg.
In this  subsection we will introduce the local regularization part in detail.
2.2 Local Regularization As its name suggests, CLGR is composed of two parts: local regularization and global regularization.
However, the discrete fill in the whole high-dimensional sample space.
qi ∈ {−1, +1}, (18) where P is defined as in Eq.
submanifold or cluster) tend to have the same cluster assignments [31].
However, the results may not be good enough since we only exploit the local informations of the dataset.
The same as in Eq.
It has been reported that the regularized linear classifier can achieve very good results on text classification problems [29].
And it has been shown that the manifold based methods can achieve good results on text classification tasks [31].
Fortunately, we have the following theorem: Theorem 1. w∗ i in Eq.
Then our optimization problem becomes minQ J = trace QT (P − I)T (P − I) + λL Q s.t.
The detailed algorithm can be referred to [26].
The detailed algorithm procedure for CLGR is summarized in table 1. 
Although this method seems slow and stupid, it is reported that it can get better performances than using a unique global classifier on certain tasks [6].
A natural problem in Eq.
constraint of pi makes the problem an NP hard integer  programming problem.
Then our objective becomes minq J = qT (P − I)T (P − I) + λL q s.t.
Then using Eq.
(23) As in Eq.
when m n).
There are mainly two approaches to achieve this goal: 1.
Combining Eq.
(10) with Eq.
In summary, using Eq.
