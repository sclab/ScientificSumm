The goal of the ranking problem, then, is to learn a function f such that, )()(,, ji ffji xxZ >∈∀ 708 Note that, as with learning a regression function, the result of this process is a function (f) that maps feature vectors to real values.
The only difference is the technique used to learn the function.
As with back-prop, RankNet attempts to minimize the value of a cost function by adjusting each weight in the network according to the gradient of the cost function with respect to that weight.
RankNet also allows the pairs in Z to be weighted with a confidence (posed as the probability that the pair satisfies the ordering induced by the ranking function).
By directly optimizing the ordering of objects, these methods are able to learn a function that does a better job of ranking than do regression techniques.
If we let xi represent features of page i, and yi be a value (say, the rank) for each page, we could learn a regression function that mapped each page"s features to their rank.
The difference is that, while a typical neural network cost function is based on the difference between the network output and the desired output, the RankNet cost function is based on the difference between a pair of network outputs.
All we really care about is the order of the pages, not the actual value assigned to them.
The classification problem is to learn a function f that maps yi=f(xi), for all i.
Let X={xi} be a collection of feature vectors (typically, a feature is any real valued number), and Y={yi} be a collection of associated classes, where yi is the class of the object described by feature vector xi.
We used RankNet [7], one of the aforementioned techniques for learning ranking functions, to learn our static rank function.
That is, for each pair of feature vectors <i,j> in the training set, RankNet computes the network outputs oi and oj.
In this paper, we used a probability of one for all pairs.
Since vector i is supposed to be ranked higher than vector j, the larger is oj-oi, the larger the cost.
When yi is real-valued as well, this is called regression.
This function can still be applied anywhere that a  regressionlearned function could be applied.
For these, let Z={<i,j>} be a collection of pairs of items, where item i should be assigned a higher value than item j.
Recent work on this ranking problem [7][13][18] directly attempts to optimize the ordering of the objects, rather than the value assigned to them.
Static ranking can be seen as a regression problem.
RankNet is a straightforward modification to the standard neural network back-prop algorithm.
In the next section, we will discuss the features used in our feature vectors, xi. 
Much work in machine learning has been done on the problems of classification and regression.
However, this over-constrains the problem we wish to solve.
