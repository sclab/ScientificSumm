In this paper, we study how we can build a Hidden-Web crawler2 that can automatically download pages from the Hidden Web, so that search engines can index them.
We believe that an effective Hidden-Web crawler can have a tremendous impact on how users search information on the Web: • Tapping into unexplored information: The Hidden-Web crawler will allow an average Web user to easily explore the vast amount of information that is mostly hidden at present.
Since a majority of Web users rely on search engines to discover pages, when pages are not indexed by search engines, they are unlikely to be viewed by many Web users.
According to many studies, the size of the Hidden Web increases rapidly as more organizations put their valuable content online through an easy-to-use Web interface [7].
In particular, a large part of the Web is hidden behind search forms and is reachable only when users type in a set of keywords, or queries, to the forms.
estimate that well over 100,000 Hidden-Web sites currently exist on the Web.
Our adaptive policy examines the pages returned from previous queries and adapts its query-selection policy  automatically based on them (Section 3).
• We investigate a number of crawling policies for the Hidden Web, including the optimal policy that can potentially download the maximum number of pages through the minimum number of interactions.
Given that the only entry to Hidden Web pages is through querying a search form, there are two core challenges to  implementing an effective Hidden Web crawler: (a) The crawler has to be able to understand and model a query interface, and (b) The crawler has to come up with meaningful queries to issue to the query interface.
In this paper, we provide a theoretical framework to investigate the Hidden-Web crawling problem and propose effective ways of generating queries automatically.
For example, PubMed hosts many high-quality papers on medical research that were selected from careful  peerreview processes, while the site of the US Patent and Trademarks Office1 makes existing patent documents available, helping  potential inventors examine prior art.
These pages are often referred to as the Hidden Web [17] or the Deep Web [7], because search engines typically cannot index the pages and do not return them in their results (thus, the pages are essentially hidden from a typical Web user).
100 Figure 1: A single-attribute search interface Hidden-Web crawler attempts to automate this process for  Hidden Web sites with textual content, thus minimizing the  associated costs and effort required.
The results from our experiments are very promising.
When the query forms have a free text input, however, an infinite  number of queries are possible, so we cannot exhaustively issue all  possible queries.
how a crawler can automatically generate queries so that it can discover and download the Hidden Web pages.
The first challenge was addressed by Raghavan and Garcia-Molina in [29], where a method for learning search  interfaces was presented.
Conventional crawlers rely on the hyperlinks on the Web to discover pages, so current search engines cannot index the Hidden-Web pages (due to the lack of links).
• Improving user experience: Even if a user is aware of a  number of Hidden-Web sites, the user still has to waste a significant amount of time and effort, visiting all of the potentially relevant sites, querying each of them and exploring the result.
According to a recent  article [5], several organizations have recognized the importance of bringing information of their Hidden Web sites onto the surface, and committed considerable resources towards this effort.
Here, we present a solution to the second challenge, i.e.
By making the Hidden-Web pages searchable at a central location, we can significantly reduce the user"s wasted time and effort in  searching the Hidden Web.
• Reducing potential bias: Due to the heavy reliance of many Web users on search engines for locating information, search engines influence how the users perceive the Web [28].
Recent studies show that a significant fraction of Web content cannot be reached by following links [7, 12].
Moreover, the content provided by many Hidden-Web sites is often of very high quality and can be extremely valuable to many users [7].
Clearly, when the search forms list all possible values for a query (e.g., through a drop-down list), the solution is straightforward.
Can the crawler automatically come up with meaningful queries without understanding the semantics of the search form?
• We evaluate various crawling policies through experiments on real Web sites.
In this case, what queries should we pick?
Our 1 US Patent Office: http://www.uspto.gov 2 Crawlers are the programs that traverse the Web automatically and download pages for search engines.
Our experiments will show the relative  advantages of various crawling policies and demonstrate their  potential.
Users do not necessarily perceive what actually exists on the Web, but what is indexed by search engines [28].
We exhaustively issue all possible queries, one query at a time.
We also evaluate our proposed solutions through experiments conducted on real Hidden-Web sites.
In one experiment, for example, our adaptive policy downloaded more than 90% of the pages within PubMed (that contains 14 million documents) after it issued fewer than 100 queries. 
In summary, this paper makes the following contributions: • We present a formal framework to study the problem of  HiddenWeb crawling.
• We propose a new adaptive policy that approximates the optimal policy.
Unless users go  directly to Hidden-Web sites and issue queries there, they cannot access the pages at the sites.
Unfortunately, we show that the optimal policy is NP-hard and cannot be implemented in practice (Section 2.2).
In [12], Chang et al.
(Section 2).
