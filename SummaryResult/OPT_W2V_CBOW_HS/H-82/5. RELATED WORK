The idea of automatically issuing queries to a database and  examining the results has been previously used in different contexts.
This work is  complementary to ours; once we detect search interfaces on the Web using the method in [15], we may use our proposed algorithms to download pages automatically from those Web sites.
The main focus of this work is to learn Hidden-Web query interfaces, not to  generate queries automatically.
In [22] Lawrence and Giles issue random queries to a number of Web Search Engines in order to estimate the fraction of the Web that has been indexed by each of them.
First, it provides a theoretical framework for analyzing the process of generating queries for a database and examining the results, which can help us better understand the  effectiveness of the methods presented in the previous work.
In [3] the authors study query-based techniques that can extract relational data from large text databases.
In  contrast, our main focus is to generate queries automatically without any human intervention.
Our vision is different from this body of work in that we intend to download and index the Hidden pages at a central location in advance, so that users can access all the information at their convenience from one single location. 
[15] propose a method to automatically detect whether a particular Web page contains a search form.
In a similar fashion, Bharat and Broder [8] issue random queries to a set of Search Engines in order to estimate the relative size and overlap of their indexes.
In a recent study, Raghavan and Garcia-Molina [29] present an architectural model for a Hidden Web crawler.
In [6], Barbosa and Freire experimentally evaluate methods for building multi-keyword queries that can return a large fraction of a document collection.
There exists a large body of work studying how to identify the most relevant database given a user query [20, 19, 14, 23, 18].
This body of work is often referred to as meta-searching or database selection problem over the Hidden Web.
Second, we apply our framework to the problem of Hidden Web crawling and demonstrate the efficiency of our algorithms.
Reference [4] reports methods to estimate what fraction of a text database can be eventually acquired by issuing queries to the database.
Our work differs from the  previous studies in two ways.
These approaches  essentially assume cooperative document databases that willingly share some of their metadata and/or documents to help a third-party search engine to index the documents.
In order to make documents in multiple textual databases  searchable at a central place, a number of harvesting approaches have 108 been proposed (e.g., OAI [21], DP9 [24]).
Again, these works study orthogonal issues and are complementary to our work.
For example, [19]  suggests the use of focused probing to classify databases into a topical category, so that given a query, a relevant database can be selected based on its topical category.
Our approach assumes  uncooperative databases that do not share their data publicly and whose documents are accessible only through search interfaces.
The potential queries are either provided manually by users or collected from the query interfaces.
For example, in [10, 11], Callan and Connel try to acquire an  accurate language model by collecting a uniform random sample from the database.
Cope et al.
