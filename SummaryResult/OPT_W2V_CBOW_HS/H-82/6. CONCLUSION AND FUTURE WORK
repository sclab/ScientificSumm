In this paper, we studied how we can build a Hidden Web crawler that can automatically query a Hidden Web site and download pages from it.
While generating queries for multi-attribute databases is clearly a more difficult problem, we may exploit the following observation to  address this problem: When a site supports multi-attribute queries, the site often returns pages that contain values for each of the query attributes.
We proposed three different query generation policies for the Hidden Web: a policy that picks queries at random from a list of keywords, a policy that picks queries based on their frequency in a generic text collection, and a policy which adaptively picks a good query based on the content of the pages downloaded from the Hidden Web site.
For example, in this paper we assumed that the crawler already knows all query  interfaces for Hidden-Web sites.
Therefore they cannot get to the Hidden Web pages which are only accessible through query interfaces.
In this case, a fully  automatic Hidden-Web crawler should know that the first result index page contains only a partial result and press the next button  automatically.
But how can the crawler discover the query interfaces?
The main challenge is to automatically segment the returned pages so that we can  identify the sections of the pages that present the values corresponding to each attribute.
Finally, some Hidden Web sites may contain an infinite number of Hidden Web pages which do not contribute much  significant content (e.g.
Given these results, we believe that our work provides a potential  mechanism to improve the search-engine coverage of the Web and the user experience of Web search.
For example, when an online bookstore supports queries on title, author and isbn, the pages returned from a query typically contain the title, author and ISBN of corresponding books.
Traditional crawlers normally follow links on the Web to  discover and download pages.
In this case the Hidden-Web crawler should be able to detect that the site does not have much more new content and stop downloading pages from the site.
Since many Web sites follow limited formatting styles in presenting multiple attributes - for example, most book titles are preceded by the label Title: - we believe we may learn page-segmentation rules automatically from a small set of training examples.
Experimental evaluation on 4 real Hidden Web sites shows that our policies have a great potential.
In addition, some Hidden-Web sites return their  results in batches of, say, 20 pages, so the user has to click on a next button in order to see more results.
Rowling", etc), we can apply the same idea that we used for the textual database: estimate the frequency of each  attribute value and pick the most promising one.
Thus, if we can analyze the returned pages and extract the values for each field (e.g, title = ‘Harry Potter", author = ‘J.K.
In particular, in certain cases the adaptive policy can download more than 90% of a Hidden Web site after issuing approximately 100 queries.
Page similarity detection algorithms may be useful for this purpose [9, 13]. 
Other Practical Issues In addition to the automatic query  generation problem, there are many practical issues to be addressed to build a fully automatic Hidden-Web crawler.
The method proposed in [15] may be a good starting point.
Multi-attribute Databases We are currently investigating how to extend our ideas to structured multi-attribute databases.
a calendar with links for every day).
6.1 Future Work We briefly discuss some future-research avenues.
