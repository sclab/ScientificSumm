Thus a counterexample is also a counterargument, one that states that a specific argument α is not always true, and the evidence  provided is the case c. Specifically, for a case c to be a  counterexample of an argument α, the following conditions have to be met: α.D c and α.S = c.S, i.e.
COUNTERARGUMENTS For our purposes an argument α generated by an agent A is  composed of a statement S and some evidence D supporting S as  correct.
In the example in Figure 1, if an agent generates the argument α = Ai, P, Walk, (Cars_passing = no) , an agent that thinks that the correct solution is Wait might answer with the counterargument β = Aj, P, Wait, (Cars_passing = no ∧ Traffic_light = red) ,  meaning that, although there are no cars passing, the traffic light is red, and the street cannot be crossed.
By exchanging arguments and counterarguments (including  counterexamples), agents can argue about the correct solution of a given problem, i.e.
In our framework, a counterargument  consists of a justified prediction Aj, P, S , D generated by an agent Aj with the intention to rebut an argument α generated by another agent Ai, that endorses a solution class S different from that of α.S for the problem at hand and justifies this with a justification D .
However, in order to do so, they need a specific interaction protocol, a preference relation between contradicting arguments, and a  decision policy to generate counterarguments (including  counterexamples).
In the  example depicted in Figure 1, an agent Ai may generate the argument α = Ai, P, Wait, (Traffic_light = red) , meaning that the agent Ai believes that the correct solution for P is Wait because the attribute Traffic_light equals red.
Using this information, we can define three types of arguments: justified predictions,  counterarguments, and counterexamples.
A justified prediction α is generated by an agent Ai to argue that Ai believes that the correct solution for a given problem P is α.S, and the evidence provided is the justification α.D.
the case must satisfy the justification α.D and the solution of c must be different than the predicted by α.
In the remainder of this section we will see how this  general definition of argument can be instantiated in specific kind of arguments that the agents can generate.
A counterargument β is an argument offered in opposition to another argument α.
A counterexample c is a case that contradicts an argument α.
In the context of MAC systems, agents argue about predictions for new problems and can provide two kinds of information: a) specific cases P, S , and b) justified predictions: A, P, S, D .
they can engage a joint deliberation process.
In the following sections we will present these elements. 
