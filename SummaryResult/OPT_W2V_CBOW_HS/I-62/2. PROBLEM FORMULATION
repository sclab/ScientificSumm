The approach here is to sum the difference between the real value of a state to the maximal Q-value of this state if resource res cannot be used for all states in a trajectory given by the  policy of task ta.
The  actions a ∈ A(s) applicable in a state are the  combination of all resource assignments that may be executed, according to the state s. In particular, a is simply an allocation of resources to the current tasks, and ata is the resource allocation to task ta.
Indeed, since an action a is a resource  assignment, Resc \ res(a) is the new set of available resources after the execution of action a.
resources to allocate) that should be executed on task ta, considering the global state s. In this case, an optimal  policy is one that maximizes the expected total reward for  accomplishing all tasks.
The global Q-value is the sum of the Q-values produced by each agent managing each task as shown in Line 11,  considering the global action a.
Furthermore, the action space of the agents takes into account only their available resources which is much less complex than a  standard action space, which is the combination of all possible resource allocation in a state for all agents.
An agent i uses as the value of a possible state transition s the Q-value for this agent which determines the maximal global Q-value for state s as in the original Q-decomposition approach.
As discussed by Singh and Cohn [10], this type of  algorithm has a number of desirable anytime characteristics: if an action has to be picked in state s before the algorithm has converged (while multiple competitive actions remains), the action with the highest lower bound is picked.
In this case, Bounded  RealTime Dynamic Programming (bounded-rtdp) permits to focuss the search on relevant states, and to prune the action space A by using lower and higher bound on the value of states.
The fact that the agents use a determined Q-value as the value of a state is an extension of the Sarsa on-policy algorithm [8] to Q-decomposition.
As the  components Lowta are computed considering s0; for example, if in a subsequent state only one task remains, the bound of SinghL will be higher than any of the Lowta components.
These value functions, which are also used for the upper bound, are computed considering that each task may use all available resources.
The arbitrator then chooses an action maximizing the sum of the agent Q-values for each global state s ∈ S. The next time state s is updated, an agent i considers the value as its respective contribution, or Q-value, to the global maximal Q-value.
It requires each agent to compute a value, from its perspective, for every action.
IN particular, if an agent i allocate resources Resi to the first set of tasks T ai, and agent i allocate resources Resi to second set of tasks T ai , the resulting policy may include actions which cannot be executed together.
In this case, two agents can plan for both set of tasks to determine the policy.
Indeed, in Lines 5 and 6 of the bounded-backup function, if QU (a, s) ≤ L(s) then action a may be pruned from the action space of s. In Line 13 of this function, a state can be labeled as solved if the  difference between the lower and upper bounds is lower than .
Singh and Cohn proposed that the algorithm terminates when only one competitive action remains for each state, or when the range of all competitive actions for any state are bounded by an indifference parameter .
For a non-consumable resource, since the resource is not considered in the state space, all other reachable states from sta consider that the resource res is still usable.
In particular, L(s) is the lower bound of state s, while U(s) is the upper bound of state s. Similarly, QL(a, s) is the Q-value of the lower bound of action a in state s, while QU (a, s) is the Q-value of the upper bound of action a in state s. Using these two bounds allow significantly  reducing the action space A.
h(s) is a heuristic which define an initial value for state s. This heuristic has to be admissible - The value given by the heuristic has to  overestimate (or underestimate) the optimal value V (s) when the objective function is maximized (or minimized).
In fact, Smith and Simmons [11] state that the  additional time to compute a Bellman backup for two bounds, instead of one, is no more than 10%, which is also what we obtained.
Summing the Lowta(sta) value functions for each ta ∈ T a does not violates the local and global resource  constraints.
Since |A|×|T a| |A|×|S|, the computation time to determine the upper bound of a state, which is done one time for each  visited state, is much less than the computation time required to compute a standard Bellman backup for a state, which is usually done many times for each visited state.
Since the upper bound for state s is known, it may be estimated The Sixth Intl.
The admissibility of these bounds has been proven by Singh and Cohn, such that, the upper bound always overestimates the optimal value of each state, while the lower bound always underestimates the  optimal value of each state.
The possible actions are limited by Lres and Gres.
Then, the optimal policy and Q-value of each agent is updated in Lines 16 and 17 to the sub-actions ai and specific Q-values Qi(ai, s) of each agent 1214 The Sixth Intl.
Furthermore, one may  compute the Q-Values Q(a, s) of each state action pair using the The Sixth Intl.
To determine the optimal policy π, Singh and Cohn implemented an algorithm very similar to bounded-rtdp, which uses the bounds to initialize L(s) and U(s).
An mdp in the context of a resource allocation problem with limited resources is defined as a tuple Res, T a, S, A, P, W, R, , where: • Res = res1, ..., res|Res| is a finite set of resource types available for a planning process.
When the global solution is computed, the lower bound is as follow: hL(s) = max(SinghL, max a∈A(s) ta∈T a Lowta(sta)) (6) We use the maximum of the SinghL bound and the sum of the lower bound components Lowta, thus  marginalrevenue ≥ SinghL.
The marginal revenue of a resource res for a task ta in a state sta is defined as following: mrta(sta) = max ata∈A(sta) Qta(ata, sta)− max ata∈A(sta) Qta(ata|res /∈ ata, sta) (5) The concept of marginal revenue of a resource is used in Algorithm 4 to allocate the resources a priori among the tasks which enables to define the lower bound value of a state.
To consider only possible actions, our upper bound, named maxU is introduced: hU (s) = max a∈A(s) ta∈T a Qta(ata, sta) (4) where Qta(ata, sta) is the Q-value of task ta for state sta, and action ata computed using a standard lrtdp approach.
The optimal value of a state, V (s), is given by: V (s) = R(s) + max a∈A(s) γ s ∈S Pa(s |s)V (s ) (1) where the remaining consumable resources in state s are Resc \ res(a), where res(a) are the consumable resources used by action a.
Indeed, rather than allowing each agent to choose the successor action, each agent i uses the action ai executed by the arbitrator in the successor state si: Qi(ai, si) = Ri(si) + γ si∈Si Pai (si|si)Qi(ai, si) (3) where the remaining consumable resources in state si are Resci \ resi(ai) for a resource allocation problem.
The  allocation a priori of the resources is made using marginal revenue, which is a highly used concept in microeconomics [7], and has recently been used for coordination of a Decentralized mdp [2].
The marginal revenue is multiplied by this term to indicate that, the more a task has a high residual value, the more its marginal revenue is going to be high.
A Markov Decision Process (mdp) framework is used to model our stochastic resource  allocation problem.
The policy is subjected to the local resource constraints res(π(s)) ≤ Lres∀ s ∈ S , and ∀ res ∈ Res.
1: Function Qdec-backup(s) 2: V (s) ← 0 3: for all i ∈ Ag do 4: for all ai ∈ Ai(s) do 5: Qi(ai, s) ← Ri(s) + γ si ∈Si Pai (si|s)Qi (ai, s ) {where Qi (ai, s ) = hi(s ) when s is not yet visited, and s has Resci \ resi(ai) remaining consumable resources for each agent i} 6: end for 7: end for 8: for all a ∈ A(s) do 9: Q(a, s) ← 0 10: for all i ∈ Ag do 11: Q(a, s) ← Q(a, s) + Qi(ai, s) 12: end for 13: if Q(a, s) > V (s) then 14: V (s) ← Q(a, s) 15: for all i ∈ Ag do 16: πi(s) ← ai 17: Qi (ai, s) ← Qi(ai, s) 18: end for 19: end if 20: end for A standard Bellman backup has a complexity of O(|A| × |SAg|), where |SAg| is the number of joint states for all agents excluding the resources, and |A| is the number of joint  actions.
on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1213 following equation: Q(a, s) = R(s) + γ s ∈S Pa(s |s) max a ∈A(s ) Q(a , s ) (2) where the optimal value of a state is V (s) = max a∈A(s) Q(a, s).
1: Function bounded-rtdp(S) 2: returns a value function V 3: repeat 4: s ← s0 5: visited ← null 6: repeat 7: visited.push(s) 8: bounded-backup(s) 9: Resc ← Resc \ {π(s)} 10: s ← s.pickNextState(Resc) 11: until s is a goal 12: while visited = null do 13: s ← visited.pop() 14: bounded-backup(s) 15: end while 16: until s0 is solved or |A(s)| = 1 ∀ s ∈ S reachable from s0 17: return V Algorithm 3 The bounded Bellman backup.
1: Function revenue-bound(S) 2: returns a lower bound LowT a 3: for all ta ∈ T a do 4: Vta ←lrtdp(Sta) 5: valueta ← 0 6: end for 7: s ← s0 8: repeat 9: res ← Select a resource type res ∈ Res 10: for all ta ∈ T a do 11: if res is consumable then 12: mrta(sta) ← Vta(sta) − Vta(sta(Res \ res)) 13: else 14: mrta(sta) ← 0 15: repeat 16: mrta(sta) ← mrta(sta) +  Vta(sta)max (ata∈A(sta)|res/∈ata) Qta(ata, sta) 17: sta ← sta.pickNextState(Resc) 18: until sta is a goal 19: s ← s0 20: end if 21: mrrvta(sta) ← mrta(sta) × Vta(sta)−valueta R(sgta ) 22: end for 23: ta ← Task ta ∈ T a which maximize mrrvta(sta) 24: Resta ← Resta {res} 25: temp ← ∅ 26: if res is consumable then 27: temp ← res 28: end if 29: valueta ← valueta + ((Vta(sta) − valueta)× max ata∈A(sta,res) Qta(ata,sta(temp)) Vta(sta) ) 30: until all resource types res ∈ Res are assigned 31: for all ta ∈ T a do 32: Lowta ←lrtdp(Sta, Resta) 33: end for 34: return LowT a putes valueta.
1: Function bounded-backup(s) 2: for all a ∈ A(s) do 3: QU (a, s) ← R(s) + γ s ∈S Pa(s |s)U(s ) 4: QL(a, s) ← R(s) + γ s ∈S Pa(s |s)L(s ) {where L(s ) ← hL(s ) and U(s ) ← hU (s ) when s is not yet visited and s has Resc \ res(a) remaining consumable resources} 5: if QU (a, s) ≤ L(s) then 6: A(s) ← A(s) \ res(a) 7: end if 8: end for 9: L(s) ← max a∈A(s) QL(a, s) 10: U(s) ← max a∈A(s) QU (a, s) 11: π(s) ← arg max a∈A(s) QL(a, s) 12: if |U(s) − L(s)| < then 13: s ← solved 14: end if how far the lower bound is from the optimal.
Firstly, if the resources are already shared among the agents, and the actions made by an agent does not influence the state of another agent, the globally optimal policy can be computed by planning separately for each agent.
When executing an action a in state s, the specific states of the tasks change stochastically, and the remaining  resource are determined with the resource available in s,  subtracted from the resources used by action a, if the resource is consumable.
However, since the resource are shared, the state space and action space is greatly reduced for each task, reducing greatly the calculus compared to the value functions computed in Line 4 which is done for both SinghL and revenue-bound.
For our problem, a reward of 1 × wta is given when the state of a task (sta) is in an achieved state, and 0 in all other cases.
First of all, a value function is computed for all tasks to realize, using a standard rtdp approach.
Since the  available consumable resources are represented in the state space, this condition is verified by itself.
The primary assumption underlying  Qdecomposition is that the overall reward function R can be additively decomposed into separate rewards Ri for each  distinct agent i ∈ Ag, where |Ag| is the number of agents.
For its part, hU (s) defines an upper bound on the value of s such that the optimal value of s is lower than hU (s).
The resources are allocated in the order of its specialization.
The main difference of complexity between SinghL and revenue-bound is in Line 32 where a value for each task has to be computed with the shared resource.
In particular, the SinghL bound may The Sixth Intl.
Proof: The local resource constraints are satisfied because the upper bound is computed using all global possible  actions a.
Each lrtdp trial is the result of simulating the policy π while updating the values V (s) using a Bellman backup (Equation 1) over the states s that are visited.
In brief, for each visited states s ∈ S, each agent computes its respective Q-values with respect to the global state s. So the state space is the joint state space of all agents.
Thus, hL(s) is a realizable policy, and an admissible lower bound. 
It has been proven that lrtdp, given an admissible initial  heuristic on the value of states cannot be trapped in loops, and eventually yields optimal values [4].
• S is a finite set of states with s ∈ S. A state s is a tuple T a, res1, ..., res|Resc| , which is the  characteristic of each unaccomplished task ta ∈ T a in the environment, and the available consumable resources.
In other words, valueta is assigned to valueta + (the residual value × the value ratio of resource type res).
The global constraint only  applies for consumable resource types (Resc) and the  local constraints always apply to consumable and  nonconsumable resource types.
The next sections propose to tighten the bounds of Singh-rtdp to permit a more effective pruning of the action space.
• A is a finite set of actions (or assignments).
This term is multiplied by max ata∈A(sta) Qta(ata,sta(res)) Vta(sta) , which is the ratio of the efficiency of resource type res.
To coordinate with each other, each agent i reports its action values Qi(ai, si) for each state si ∈ Si to an arbitrator at each learning  iteration.
For our resource allocation problem described briefly in this section, Q-decomposition can be applied to generate an optimal solution.
The global resource constraint is res(tra) ≤ Gres∀ tra ∈ T RA ,and ∀ res ∈ Resc where res(tra) is a function which returns the resources used by trajectory tra.
In the beginning of the  algorithm, no resources are allocated to a specific task, thus the valueta variable is initialized to 0 for all ta ∈ T a.
In Line 5 of the Qdec-backup function, each agent managing a task  computes its respective Q-value.
An agent considers as a possible state transition only the possible states of the set of tasks it manages.
The lower bound of state s is hL(s) = ta∈T a Lowta(sta), where Lowta(sta) is a value function for each task ta ∈ T a, such that the resources have been allocated a priori.
On the one hand, hL(s) defines a lower bound on the value of s such that the optimal value of s is higher than hL(s).
A second type of resource allocation problem is where the resources are already shared among the agents, but the actions made by an agent may influence the reward obtained by at least another agent.
In Line 21, the marginal revenue is updated in  function of the resources already allocated to each task.
Furthermore, if a new task dynamically arrives in the environment, it can be accommodated by redefining the lower and  upper bounds which exist at the time of its arrival.
bounded-rtdp is similar to rtdp except there are two distinct initial heuristics for unvisited states s ∈ S; hL(s) and hU (s).
2.1 Resource Allocation as a MDPs In our problem, the transition function and the reward function are both known.
In particular, πta(s) is the action (i.e.
Allocating the resources in this order improves the quality of the resulting lower bound.
A consumable  resource type is one where the amount of available resource is decreased when it is used.
In Line 24, the resource type res is allocated to the group of resources Resta of task ta.
For a consumable resource, the Q-value consider only resource res in the state space, while for a non-consumable resource, no resources are available.
This variable is the estimated value of each task ta ∈ T a.
Indeed, since the problem is stochastic, the  optimal value is lower than for the deterministic version.
Then, our own method proposes tighter bounds, thus allowing a more effective pruning of the action space.
Each resource type may have a local resource constraint Lres on the number that may be used in a single step, and a global resource constraint Gres on the number that may be used in total.
When the execution goes back to the bounded-rtdp function, the next state in Line 10 has a fixed number of consumable resources available Resc, determined in Line 9.
In this problem, a state represents a  conjunction of the particular state of each task, and the available resources.
If the  difference between the lower and upper bound is too high, one can choose to use another greedy algorithm of one"s choice, which outputs a fast and near optimal solution.
Also, the checkSolved(s, ) procedure can be omitted because the bounds can provide the labeling of a state as solved.
A simple resource allocation problem is one where there are the following two tasks to realize: ta1 = {wash the dishes}, and ta2 = {clean the floor}.
In brief, pickNextState(res) selects a none-solved state s reachable under the current policy which has the highest Bellman error (|U(s) − L(s)|).
On the other hand, a  nonconsumable resource type is one where the amount of  available resource is unchanged when it is used.
On the other hand, the Q-decomposition Bellman backup has a complexity of O((|Ag| × |Ai| × |Si)|) + (|A| × |Ag|)), where |Si| is the number of states for an agent i, excluding the resources and |Ai| is the number of actions for an agent i.
To result these conflicts, we use Q-decomposition  proposed by Russell and Zimdars [9] in the context of  reinforcement learning.
In this case, when an action of an agent i cannot be executed simultaneously with an action of another agent i , the global action is simply discarded from the action space A(s).
Singh and Cohn [10] proved that an algorithm that uses  admissible lower and upper bounds to prune the action space is assured of converging to an optimal solution.
The Line 12 computes the marginal revenue of a consumable resource res for each task ta ∈ T a.
The two  possible system trajectories are (s, a), (s ) and (s, a), (s ) .
That is, Qi(ai, si) is the value of a state such that it maximizes maxa∈A(s) i∈Ag Qi(ai, si).
Thus, Vta(sta)−valueta R(sgta ) is the residual expected value that remains to be achieved, knowing current allocation to task ta, and normalized by the reward of realizing the tasks.
In this section, a bounded version of rtdp  (boundedrtdp) is presented in Algorithm 2 to prune the action space of sub-optimal actions.
1216 The Sixth Intl.
In a standard Bellman backup, |A| is multiplied by |SAg|, which is much more complex than multiplying |A| by |Ag| with the Q-decomposition Bellman backup.
When all resources are allocated, the lower bound components Lowta of each task are  computed in Line 32.
However, there are interaction between the resource of Res1 and Res2, so that certain combination of resource cannot be assigned.
The values of the bounds are computed in Lines 3 and 4 of the bounded-backup function.
A solution of an mdp is a policy π mapping states s into actions a ∈ A(s).
Since resource allocation in a stochastic environment is NP-Complete, heuristics should be employed.
Q-decomposition which decomposes a  planning problem to many agents to reduce the computational complexity associated to the state and/or action spaces is now introduced.
bounded-rtdp  labels states for which |U(s) − L(s)| < as solved and the convergence is reached when s0 is solved or when only one competitive action remains for each state.
Indeed, an optimal Bellman backup can be applied in a state as in Algorithm 1.
Doing this produces a higher value for each task, than the one obtained when planning for all tasks globally with the shared limited resources.
For example, state s is entered, which may transit to s or to s , according to action a.
2.5 Reducing the Upper Bound SinghU includes actions which may not be possible to execute because of resource constraints, which overestimates the upper bound.
Thus, for a stochastic resource allocation problem, the marginal revenue of a resource is the additional expected value it  involves.
This stopping criteria is more effective since it is similar to the one used by Smith and Simmons [11] and McMahan et al.
Also, in some cases, this form of agent decomposition allows the local Q-functions to be expressed by a much reduced state and action space.
Only the values of the state transitions change.
A computer has to compute the  optimal allocation of these resources to cleaner robots to realize their tasks.
2.4 Singh and Cohn"s Bounds Singh and Cohn [10] defined lower and upper bounds to prune the action space.
The resources may be constrained by the amount that may be used simultaneously (local constraint), and in total (global constraint).
The convergence is  accomplished by means of a labeling procedure called  checkSolved(s, ).
Furthermore, the higher is the number of resources allocated to realize a task, the higher is the expectation of realizing the task.
Here, Qi (ai, s ) determines the optimal Q-value of agent i in state s .
The first part of the equation to compute valueta represents the expected residual value for task ta.
A system trajectory tra is a possible sequence of state-action pairs, until a goal state is reached under the optimal policy π.
To realize the tasks, two type of resources are assumed: res1 = {brush}, and res2 = {detergent}.
Computing these two Q-values is made simultaneously as the state transitions are the same for both Q-values.
Then, using these task-value functions, a lower bound hL, and upper bound hU can be defined.
Since |SAg| is combinatorial with the  number of tasks, so |Si| |S|.
For this reason, when the specific states of the tasks change, or when the number of available resources changes, the value of this state may change.
However, when the resources are available to all agents, no Q-decomposition is possible.
The relative reward of the state of a task rsta is the product of a real number sta by the weight factor wta.
Furthermore, the time is not considered in the model description, but it may also include a time horizon by using a finite horizon mdp.
• T a is a finite set of tasks with ta ∈ T a to be  accomplished.
When each task has its own set of resources, each task may be solved independently.
If the resources are already shared among the agents, the number of resource type for each agent will usually be lower than the set of all  available resource types for all agents.
Finally, in Lines 12 to 15, a backup is made in a backward fashion on all visited state of a trajectory, when this trajectory has been made.
In Line 4 of the algorithm, a value function is  computed for all tasks in the environment using a standard lrtdp [4] approach.
Thus, the computation time of the upper bound is negligible.
In other words, the more a resource is  efficient on a small group of tasks, the more it is allocated early.
Proof: Lowta(sta) is computed with the resource being shared.
Line 14 simply allocate the  current value with respect to the highest global Q-value, as in a standard Bellman backup.
In some scenarios, it may happens that the missiles can be classified in two types: Those requiring a set of resources Res1 and those requiring a set of resources Res2.
The global constraint is defined according to all system trajectories tra ∈ T RA.
The only difference between bounded-rtdp, and the rtdp version of Singh and Cohn is in the stopping  criteria.
Then, the arbitrator functionalities are in Lines 8 to 20.
lrtdp is a simple dynamic programming  algorithm that involves a sequence of trial runs, each starting in the initial state s0 and ending in a goal or a solved state.
Another example of this type comes from our problem of interest, explained in Section 3, which is a naval platform which must counter incoming missiles (i.e.
Then, in Line 9, a resource type res (consumable or non-consumable) is selected to be allocated.
on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 2.6 Increasing the Lower Bound The idea to increase SinghL is to allocate the resources a priori among the tasks.
For readability, the upper bound by Singh and Cohn is named SinghU, and the lower bound is named SinghL.
For example, an admissible heuristic for a stochastic shortest path problem is the solution of a deterministic shortest path problem.
This can happen depending on the type of missiles, their range, and so on.
Some of the gain in complexity to use Q-decomposition resides in the si∈Si Pai (si|s) part of the equation.
A goal state is a sink state where an agent stays forever.
All resource types are allocated in this manner until Res is empty.
Russell and Zimdars [9] demonstrated that local Sarsa converges to the optimum.
Since the number of states is  exponential with the number of tasks, using Q-decomposition should reduce the planning time significantly.
These two tasks are  either in the realized state, or not realized state.
Computing the maxU bound in a state has a  complexity of O(|A| × |T a|), and O(|T a|) for SinghU.
Then, a task ta is selected in Line 23 with the highest marginal revenue, adjusted with residual value.
Furthermore, the  communication cost between the agents and the arbitrator is null since this approach does not consider a geographically separated problem.
When the initial state is labelled as solved, the algorithm has converged.
For example, a brush is a non-consumable resource, while the detergent is a consumable resource.
• W = [wta] is the relative weight (criticality) of each task.
The lower bound of Equation 6 is  admissible.
However, all the agents are penalized when an agent consumes oil because of the pollution it generates.
In particular, hL(s) = max ta∈T a Vta(sta), and hU (s) = ta∈T a Vta(sta).
However, hU (s) still overestimates V (s) because the global resource constraint is not enforced.
In this way, an ideal compromise can be found for the agents to reach a global optimum.
Also, |A| is combinatorial with the number of resource types.
This pruning enables to speed up the convergence of lrtdp.
on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1215 Algorithm 2 The bounded-rtdp algorithm.
Thus, having to compute two Q-values instead of one does not augment the complexity of the  approach.
These agents desire to maximize their specific reward by consuming the right amount of oil.
The upper bound defined by Equation 4 is admissible.
Indeed, as the resources are shared, the tasks  cannot overuse them.
Also, S contains a non empty set sg ⊆ S of goal states.
mdps have been widely adopted by researchers today to model a stochastic process.
A standard Bellman backup has a complexity of O(|A| × |S|).
tasks) by using its resources (i.e.
The Line 5  initializes the valueta variable.
Indeed, each task may use all consumable resources for its own purpose.
This heuristic proved to obtain good results, but other ones may be tried, for example Monte-Carlo  simulation.
In this paper, the bounds defined by Singh and Cohn and implemented using bounded-rtdp define the Singh-rtdp approach.
In brief, marginal revenue is the extra revenue that an additional unit of product will bring to a firm.
Here, a domain expert may  separate all available resources in many types or parts to be allocated.
Russell and Zimdars called this approach local Sarsa.
In other words, the model is Markovian as the history has not to be considered in the state space.
This procedure tries to label as solved each traversed state in the current trajectory.
sta is the specific state of task ta.
The next sections describe two separate methods to define hL(s) and hU (s).
R(sgta ) is the reward to realize task ta.
First of all, the method of Singh and Cohn [10] is briefly described.
• A discount (preference) factor γ, which is a real  number between 0 and 1.
All consumable and non-consumable resource types are allocated to each task.
• State rewards R = [rs] : ta∈T a rsta ← sta × wta.
Thus, the Q-decomposition Bellman backup is much less complex than a standard Bellman backup.
Their approach is pretty  straightforward.
on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1217 be higher when a little number of tasks remain.
2.2 Q-decomposition for Resource Allocation There can be many types of resource allocation problems.
• Transition probabilities Pa(s |s) for s ∈ S and a ∈ A(s).
This is due to the fact that mdps provide a well-studied and simple, yet very  expressive model of the world.
Indeed, our model may consider  consumable and non-consumable resource types.
For instance, a group of agents which manages the oil consummated by a country falls in this group.
2.3 Bounded-RTDP Bonet and Geffner [4] proposed lrtdp as an improvement to rtdp [1].
on Autonomous Agents and Multi-Agent Systems (AAMAS 07) for action a. Algorithm 1 The Q-decomposition Bellman Backup.
bounded-rtdp is now introduced.
Afterwards, Line 29  recomAlgorithm 4 The marginal revenue lower bound algorithm.
In these circumstances, |Ai| |A|.
Joint Conf.
Joint Conf.
Joint Conf.
Joint Conf.
Joint Conf.
That is, R = i∈Ag Ri.
This strategy has been proven as efficient [11] [6].
Adapted from [4] and [10].
weapons, movements).
brtdp [6].
Theorem 2.2.
Theorem 2.1.
