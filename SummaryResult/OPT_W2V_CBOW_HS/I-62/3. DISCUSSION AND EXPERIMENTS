Indeed, an optimal allocation is one where the resources are distributed in the best way to all tasks, and our lower bound heuristically does that. 
Q-decomposition permits to diminish the planning time significantly in our problem settings, and seems a very efficient approach when a group of agents have to allocate resources which are only available to themselves, but the  actions made by an agent may influence the reward obtained by at least another agent.
For the local constraints, at most 1 resource of each type can be  allocated to execute tasks in a specific state.
This way, the value of each unvisited state is assured to overestimate its real value since the value of achieving a task ta is the highest the planner may get for ta.
In terms of experiments, notice that the lrtdp lrtdp-up and approaches for resource allocation, which doe not prune the action space, are much more complex.
task by using a heuristic to distribute the resources.
The set of task T ai, managed by agent i, can be accomplished with the set of resources Resi, while the second set of task T ai , managed by agent Agi , can be accomplished with the set of resources Resi .
The number of resource type has been fixed to 5, where there are 3  consumable resource types and 2 non-consumable resources types.
When a missile is not countered, it transits to another state, which may be  preferred or not to the current state, where the most preferred state for a task is when it is countered.
From these results, we conclude that the  difference of efficiency between mr-rtdp and Singh-rtdp is more attributable to the marginal-revenue lower bound that to the maxU upper bound.
0.01 0.1 1 10 100 1000 10000 1 2 3 4 5 6 7 8 Timeinseconds Number of tasks LRTDP LRTDP-up Singh-RTDP MR-RTDP Figure 2: Efficiency of MR-RTDP compared to SINGH-RTDP.
In our problem, |Sta| = 4, thus each task can be in four distinct states.
The Singh-rtdp approach diminished the planning time by using a lower and upper bound to prune the action space.
tasks) by using its  resources (i.e.
Furthermore, for consumable resources, the total amount of available  consumable resource is between 1 and 2 for each type.
The state transitions are all stochastic  because when a missile is in a given state, it may always transit in many possible states.
To compute the lower bound of revenue-bound, all available resources have to be separated in many types or parts to be allocated.
The global constraint is generated randomly at the start of a scenario for each consumable resource type.
Nevertheless, even if the lrtdp approach uses a simple heuristic, still a huge part of the state space is not visited when computing the optimal policy.
To implement Qdec-lrtdp, we divided the set of tasks in two equal parts.
Indeed, when the number of task to execute is high, the lower bounds by Singh-rtdp takes the values of a single task.
Resi had one consumable resource type and one non-consumable resource type, while Resi had two consumable resource types and one non-consumable resource type.
There are also local and global resource  constraints on the amount that may be used.
There are two types of states; firstly, states where actions modify the transition probabilities; and then, there are goal states.
For our problem, we allocated each resource of each type in the order of of its specialization like we said when describing the revenue-bound function.
mr-rtdp  further reduce the planning time by providing very tight  initial bounds.
Indeed, the time reduction is quite significant compared to lrtdp-up, which demonstrates the efficiency of using bounds to prune the action space.
The effectiveness of each resource is modified randomly by ±15% at the start of a scenario.
A simple heuristic has been used where the value of an unvisited state is assigned as the value of a goal state such that all tasks are achieved.
on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 0.01 0.1 1 10 100 1000 10000 100000 1 2 3 4 5 6 7 8 9 10 11 12 13 Timeinseconds Number of tasks LRTDP QDEC-LRTDP Figure 1: Efficiency of Q-decomposition LRTDP and LRTDP.
Lets summarize these approaches here: • Qdec-lrtdp: The backups are computed using the Qdec-backup function (Algorithm 1), but in a lrtdp context.
For the experiments, 100 randomly resource allocation problems were generated for each approach, and possible number of tasks.
We also implemented mr-rtdp with the SinghL bound, and this was slightly more efficient than Singh-rtdp.
• lrtdp-up: The upper bound of maxU is used for lrtdp.
Furthermore, we implemented mr-rtdp with the SinghU bound, and this was slightly less efficient than with the maxU bound.
In particular, Singh-rtdp needed 231 seconds in average to solve problem with six tasks and mr-rtdp required 76 seconds.
On the other hand, the lower bound of mr-rtdp takes into account the value of all 1218 The Sixth Intl.
In particular, each resource type has a probability to counter a missile between 45% and 65% depending on the state of the task.
• Singh-rtdp: The SinghL and SinghU bounds are used for bounded-rtdp.
This constraint is also present on a real naval platform because of sensor and launcher constraints and engagement policies.
• mr-rtdp: The revenue-bound and maxU bounds are used for bounded-rtdp.
Since this heuristic is pretty straightforward, the advantages of using better heuristics are more evident.
For instance, it took an average of 1512 seconds to plan for the lrtdp-up approach with six tasks (see Figure 1).
When the number of tasks is odd, one more task was assigned to T ai .
These  constraints are managed by the arbitrator as described in  Section 2.2.
The domain of the experiments is a naval platform which must counter incoming missiles (i.e.
In particular the updates made in the  checkSolved function are also made using the the  Qdecbackup function.
There are constraint between the group of resource Resi and Resi such that some assignments are not possible.
For this problem a standard lrtdp approach has been implemented.
The approaches described in this paper are compared in Figures 1 and 2.
Joint Conf.
weapons, movements).
