This simple adaptation strategy allows us to improve application performance in several situations typical for the Grid: • If an application is started on fewer processors than its degree of parallelism allows, it will automatically expand to more  processors (as soon as there are extra resources available).
• If an application is running on an appropriate set of resources but after a while some of the resources (processors and/or  network links) become overloaded and slow down the  computation, the overloaded resources will be removed.
Adaptation strategy • If the application degree of parallelism is changing during the computation, the number of nodes the application is running on will be automatically adjusted.
Efficiency is defined as the average utilization of the processors, that is, the fraction of time the processors spend doing useful work rather than being idle or communicating with other processors (10).
• If some of the original resources chosen by the user are  inappropriate for the application, for example the bandwidth to one of the clusters is too small, the inappropriate resources will be removed.
This causes small inaccuracies in the calculations of the coordinator, but does not influence the performance of adaptation.
Typically, the efficiency drops as new processors are added to the computation.
However, it would be more efficient to ask for the fastest processors among the available ones.
Processors run the benchmark at such frequency so as not to exceed the specified overhead.
The  processor speed could then be measured by counting the tasks  processed by this processor within one monitoring period.
Enabling opportunistic migration requires, again, the ability to specify to the scheduler what  better resources are (faster, with a certain minimal bandwidth) and receiving notifications when such resources become available.
In order to monitor the application performance and guide the adaptation, we added an extra process to the computation which we call  adaptation coordinator.
The lower bound on minimal required bandwidth is tightened each time a cluster with high inter-cluster overhead is removed.
The number of nodes that are removed again depends on the weighted average efficiency.
Further improvements are possible, but require extra  functionality from the grid scheduler and/or integration with monitoring services such as NWS (22).
We  introduce a new application performance metric: weighted average  efficiency which describes the application performance on a  heterogeneous set of resources.
In that case, we simply remove the whole cluster instead of computing node badness and removing the worst nodes.
The badness of clusters is computed similarly to the badness of processors: cluster badnessi = α ∗ 1 speedi + β ∗ ic overheadi The speed of a cluster is the sum of processor speeds normalized to the speed of the fastest cluster.
The runtimes of the Barnes-Hut application, scenarios 0-5 add nodes faster nodes available if compute weighted average efficiency E wa wait & collect statistics rank nodes remove worst nodes waE Ewa Y N N Y above if below if Emin maxE Figure 1.
The badness of a processor is determined by the following formula: proc badnessi = α ∗ 1 speedi + β ∗ ic overheadi + γ ∗ inW orstCluster(i) The processor is considered bad if it has low speed ( 1 speed is big) and high inter-cluster overhead (ic overhead).
wa efficiency = 1 n ∗ n i=0 speedi ∗ (1 − overheadi ) The useful work done by a processor (1 − overheadi) is weighted by multiplying it by the speed of this processor  relative to the fastest processor.
Scenario 0 a b c Scenario 1 Scenario 2 Scenario 3 Scenario 4 Scenario 5 without monitoring and adaptation (runtime 1) with monitoring and adaptation (runtime 2) with monitoring but no adaptation (runtime 3) Figure 2.
This could be done, for example, by passing a benchmark to the grid scheduler, so that it can measure processor speeds in an application specific way.
The number of requested processors depends on the current efficiency: the higher the efficiency, the more processors are requested.
To calculate weighted average efficiency, we need to know the relative speeds of the processors, which depend on the  application and the problem size used.
Removing processors located in a single cluster is desirable since it decreases the amount of wide-area communication.
During this process the coordinator learns the application requirements by remembering the characteristics of the removed processors.
This means, however, that we cannot use these resources even if the cause of the  performance problem disappears, e.g.
Since it is impractical to run the 122 whole application on each processor separately, we use  applicationspecific benchmarks.
These requirements are then used to guide the adding of new processors.
We are currently discussing with them the possibility of providing the functionalities required by us, aiming to extend our adaptivity  strat124 egy to support opportunistic migration and to improve the initial resource selection. 
The coordinator starts removing processors when the weighted average efficiency drops below Emin.
For heterogeneous environments with different processor speeds, we extended the notion of efficiency and introduced weighted  average efficiency.
Additionally, when one of the clusters has an exceptionally high inter-cluster overhead (larger than 0.25), we conclude that the  bandwidth on the link between this cluster and the Internet backbone is insufficient for the application.
Currently we are using the following values: α = 1, β = 100 and γ = 10, based on the observation that ic overhead > 0.2 indicates bandwidth problems and processors with speed < 0.05 do not contribute to the computation.
After deciding which nodes are removed, the  coordinator sends a message to these nodes and the nodes leave the computation.
In the future, we plan to combine  benchmarking with monitoring the load of the processor which would allow us to avoid running the benchmark if no change in processor load is detected.
In our current implementation, the application  programmer specifies the length of the benchmark (by specifying its  problem size) and the maximal overhead it is allowed to cause.
In the heterogeneous world, it is hardly beneficial to add  processors if the efficiency is lower than 50% unless the added processor is faster than some of the currently used processors.
The more often it is run, the faster changes in processor speed are detected.
Adding faster processors might be beneficial regardless of the efficiency.
The developers of the KOALA metascheduler (15) have recently started a project whose goal is to provide support for adaptive applications.
Occasionally, the coordinator may miss data at the end of a monitoring period, so it has to use data from the previous monitoring period for these processors.
If an application runs with efficiency between Emin and Emax, the adaptation component will not undertake any action, even if  better resources become available.
In this section we will explain how we use application  malleability to find a suitable set of resources for a given application and to adapt to changing conditions in the grid environment.
Also, during application execution, we can learn some  application requirements and pass them to the scheduler.
The lower the  efficiency, the more nodes are removed.
efficiency = 1 n ∗ n i=0 (1 − overheadi ) where n is the number of processors and overheadi is the  fraction of time the ith processor spends being idle or communicating.
The bandwidth  between each pair of clusters is estimated during the computation by measuring data transfer times, and the bandwidth to the removed cluster is set as a minimum.
After removing the overloaded resources, the weighted average efficiency will increase to above the Emax threshold and the adaptation  coordinator will try to add new resources.
3.1 Weighted average efficiency In traditional parallel computing, a standard metric describing the performance of a parallel application is efficiency.
Typically, it would be enough to measure the speed of one processor per site, since clusters and supercomputers are usually  homogeneous.
Note that the benchmarking overhead could be avoided  completely for more regular applications, for example, for  masterworker applications with tasks of equal or similar size.
Conversely, if an application is started on more processors than it can  efficiently use, a part of the processors will be released.
Efficiency indicates the benefit of using multiple processors.
After each monitoring period, the processors compute their overhead over this period as the percentage of the time they spent being idle or communicating in this period.
If necessary, the adaptation component will try to add other resources.
Therefore, adding processors when  efficiency is smaller or equal to 50% will only decrease the system utilization without significant performance gains.
However, it is less accurate than using an  application specific benchmark.
Benchmarks have to be re-run periodically because the speed of a processor might change if it becomes overloaded by another  application (for time-shared machines).
Therefore, slower processors are modeled as fast ones that spend a large fraction of the time being idle.
Currently we use the same application with a small problem size as a benchmark and we require the application programmer to specify this problem size.
One example is minimal bandwidth required by the application.
High  intercluster overhead indicates that the bandwidth to this processor"s cluster is insufficient.
This number is typically hard to find, but in (10) it was  theoretically proven that if the optimal number of processors is used, the efficiency is at least 50%.
For example, adding nodes to a  computation can be improved.
At the end of each monitoring period, the processors send the overhead statistics and processor speeds to the coordinator.
Existing grid schedulers such as GRAM from the Globus Toolkit (11) do not support such functionality.
It is especially important when migrating from resources that cause performance problems: we have to be careful not to add the resources we have just removed.
The longer the benchmark, the greater the accuracy of the measurement.
An alternative approach would be ranking the processors based on parameters such as clock speed and cache size.
This approach requires extra effort from the programmer to find the right problem size and possibly to produce input files for this problem size, which may be hard.
Efficiency of 30% or lower might indicate performance problems such as low bandwidth or overloaded processors.
Each  processor decides separately when it is time to send data.
Periodically, the coordinator computes the weighted average efficiency and other statistics, such as average inter-cluster overhead or overheads in each cluster.
• If during the computation a substantial part of the processors will crash, the adaptation component will try to add new  resources to replace the crashed processors.
In that case,  removing bad processors will be beneficial for the application.
The optimal number of processors is the number for which the ratio of efficiency to execution time is  maximized.
the bandwidth of a link might improve if the background traffic diminishes.
When it exceeds Emax, the  coordinator requests new processors from the scheduler.
Weighted average efficiency reflects the fact that adding slow processors yields less benefit than adding fast processors.
Such low efficiency might also indicate that we simply have too many  processors.
Therefore, achieving a high speedup (and thus a low execution time) and achieving a high system utilization are conflicting goals (10).
The coordinator always tries to remove the worst processors.
A heuristic formula is used to decide which processors have to be removed.
The thresholds we use are Emax = 50%, because we know that adding processors when  efficiency is lower does not make sense, and Emin = 30%.
There is a trade-off between the accuracy of speed measurements and the overhead it incurs.
In the future, we are planning to generate benchmarks  automatically by choosing a random subset of the task graph of the original application.
Such bounds can be passed to the scheduler to avoid adding inappropriate resources.
The coordinator uses statistics from  application processors to compute the weighted average efficiency.
Apart from total overhead, each processor also computes the overhead of inter-cluster and  intracluster communication.
3.3 Adaptation strategy The adaptation coordinator tries to keep the weighted average  efficiency between Emin and Emax.
We are currently not able to perform opportunistic migration - migrating to better resources when they are discovered.
In that case, removing some processors may not be beneficial but it will not harm the application.
If the efficiency falls above or below certain thresholds, the  coordinator decides on adding or removing processors.
The sizes of tasks can vary by many orders of magnitude.
The clocks of the processors are not synchronized with each other or with the clock of the coordinator.
Dashed lines indicate a part that is not supported yet, as will be explained below.
3.2 Application monitoring Each processor measures the time it spends communicating or  being idle.
Currently we use blacklisting - we simply do not  allow adding resources we removed before.
The ic overhead of a cluster is an average of processor inter-cluster overheads.
Figure 1 shows a schematic view of the adaptation strategy.
The adaptation coordinator periodically collects performance statistics from the application processors.
Currently, we add any nodes the  scheduler gives us.
Adding processors beyond this number yields little benefit.
Therefore, the application will be migrated from overloaded resources.
Alternatively, information from a grid monitoring system can be used.
This optimization will further reduce the benchmarking overhead.
The α, β and γ coefficients determine the relative importance of the terms.
Therefore, processors belonging to the worst cluster are preferred.
The computation is divided into monitoring periods.
This  approach is sometimes used for resource selection for sequential  applications (14).
Function inW orstCluster(i) returns 1 for processors belonging to the worst cluster and 0 otherwise.
Unfortunately, divide-and-conquer applications typically exhibit a very  irregular structure.
The fastest processor has speed = 1, for others holds: 0 < speed ≤ 1.
Those coefficients are established empirically.
123 0 2000 4000 6000 runtime(secs.)
