We propose three ways to relax Online SVMs: • Reduce the size of the optimization problem by only optimizing over the last p examples.
Thus, the margin found by an  optimization is not guaranteed to be one that maximizes the margin for the global data set of examples {x1, .
The learner may encounter an example that lies within the margins, but farther from the margins than M. Such an example means the hypothesis is no longer globally optimal for the data set, but it is considered good enough for continued use without immediate retraining.
However, as we saw in the previous section, the task of content-based spam detection is best achieved by SVMs with a high value of C. Setting C to a high value for this domain implies that minimizing  training loss is more important than maximizing the margin (see Figure 3).
Note that this is not equivalent to training a new SVM classifier from scratch on the p most recent examples,  because each successive optimization problem is seeded with the previous hypothesis w [8].
If we re-train on every example that is not well-classified, our hyperplane will be guaranteed to be optimal at every step.
As we will see in the experimental section, this parameter can be set as low as 1 with little impact on the quality of results, providing computational savings. 
set (w, b) := (w",b") If size(seenData) > p remove oldest example from seenData Add xi to seenData done Figure 4: Pseudo-code for Relaxed Online SVM.
This set of constraints reduces the number of free variables in the optimization problem, reducing computational cost.
, (xn, yn), C, m, p: Initialize w := 0, b := 0, seenData := { } For Each xi ∈ X do: Classify xi using f(xi) = sign(< w, xi > +b) If yif(xi) < m Find w , b with SMO on seenData, using w, b as seed hypothesis.
One of the main benefits of SVMs is that they find a  decision hyperplane that maximizes the margin between classes in the data space.
3.2 Reducing Number of Updates As noted before, the KKT conditions show that a well classified example will not change the hypothesis; thus it is not necessary to re-train when we encounter such an  example.
Here, each update still produces an optimal  hyperplane.
This hypothesis may contain values for features that do not occur anywhere in the p most recent examples, and these will not be changed.
An example xi is now considered well classified when yif(xi) > M, for some 0 ≤ M ≤ 1.
The number of re-training updates can be reduced by  relaxing the definition of well classified.
, xn)}, but rather one that satisfies a relaxed requirement that the margin be maximized over the examples { x(n−p+1), .
We can bound this expense by only considering the p most recent examples for optimization (see Figure 4 for pseudo-code).
In the experimental section, we show that this  version of Online SVMs, which updates only on actual errors, does not significantly degrade performance on content-based spam detection, but does significantly reduce cost.
3.1 Reducing Problem Size In the full Online SVMs, we re-optimize over the full set of seen data on every update, which becomes expensive as the number of seen data points grows.
The full margin maximization feature that they provide is unnecessary, and relaxing this requirement can reduce computational cost.
That is, a close first approximation may be good enough.
In this case, the original  softmargin SVM is computed by maximizing at example n: W (α) = nX i=1 αi − 1 2 nX i,j=1 αiαjyiyj < xi, xj >, subject to the previous constraints [23]: ∀i ∈ {1, .
This allows the hypothesis to remember rare (but informative) features that were learned further than p examples in the past.
Maximizing the margin is expensive, typically requiring quadratic training time in the number of training examples.
, xn}, subject to the fixed constraints on the hyperplane that were found in previous optimizations over examples {x1, .
In the extreme case, we can set M = 0, which creates a mistake driven Online SVM.
Experimental results reported in the  following section show that they equal or approach the  performance of full Online SVMs on content-based spam detection.
3.3 Reducing Iterations As an iterative solver, SMO makes repeated passes over the data set to optimize the objective function.
Successive iterations of this loop bring the hyperplane closer to an optimal value.
solver by allowing an approximate solution to the  optimization problem.
We introduce a parameter T to control the maximum number of iterations we allow.
Formally, the optimization problem is now defined most clearly in the dual form [23].
, n} : 0 ≤ αi ≤ C and nX i=1 αiyi = 0 To this, we add the additional lookback buffer constraint ∀j ∈ {1, .
• Reduce the number of training updates by only  training on actual errors.
Thus, while SVMs do create high performance spam  filters, applying them in practice is overkill.
Under the KKT conditions, an example xi is considered well-classified when yif(xi) > 1.
As we describe in the remainder of this subsection, all of these methods trade statistical robustness for reduced  computational cost.
, (n − p)} : αj = cj where cj is a constant, fixed as the last value found for αj while j > (n − p).
SMO has one main loop, which can alternate between passing over the entire data set, or the smaller active set of current  support vectors [22].
However, it is possible that these iterations provide less benefit than their expense justifies.
• Reduce the number of iterations in the iterative SVM Given: dataset X = (x1, y1), .
This update procedure is similar to that used by  variants of the Perceptron algorithm [18].
(For completeness, when p ≥ n, define (n − p) = 1.)
, x(n−p)}.
