It can be seen from Figure 3 that the ReDDE and UUM/HR algorithms are more effective (on the representative, relevant and nonrelevant testbeds) or as good as (on the Trec123-100Col testbed) the CORI resource selection algorithm.
DATABASE RECOMMENDATION All four testbeds described in Section 4 were used in the experiments to evaluate the resource selection effectiveness of the database recommendation system.
The resource descriptions were created using query-based sampling.
And let Bi and Ei denote the number of relevant documents in the ith ranked database of B or E. Then Rn is defined as follows: = = = k i i k i i k B E R 1 1 (17) Usually the goal is to search only a few databases, so our figures only show results for selecting up to 20 databases.
This suggests that the UUM/HR algorithm is more robust than the ReDDE algorithm.
The UUM/HR algorithm is more effective than the ReDDE algorithm on the representative and relevant testbeds and is about the same as the ReDDE algorithm on the  Trec123100Col and the nonrelevant testbeds.
Fifty queries (101-150) were used as training queries to build the relevant logistic model and to fit the exponential functions of the centralized document score curves for large ratio databases (details in Section 3.1).
The database size statistics were estimated by the sample-resample method [21].
Resource selection experiments on the four testbeds.
Collection Selected.
Resource selection algorithms of database recommendation systems are typically compared using the recall metric nR [1,17,18,21].
It can be noted that when selecting only a few databases on the Trec123-100Col or the nonrelevant testbeds, the ReDEE algorithm has a small advantage over the UUM/HR algorithm.
More training data or a more sophisticated model may help to solve this minor puzzle.
The experiments summarized in Figure 3 compared the effectiveness of the three resource selection algorithms, namely the CORI, ReDDE and UUM/HR.
We attribute this to two causes: i) The ReDDE algorithm was tuned on the Trec123-100Col testbed; and ii) Although the difference is small, this may suggest that our logistic model of estimating probabilities of relevance is not accurate enough.
Another 50 queries (51-100) were used as test data.
Collection Selected.
Let B denote a baseline ranking, which is often the RBR (relevance based ranking), and E as a ranking provided by a resource selection algorithm.
Collections Selected.
Collections Selected.
Relevant Testbed.
About 80 queries were sent to each database to download 300 unique documents.
Nonrelevant Testbed.
Representative Testbed.
Trec123-100Col Testbed.
The UUM/HR algorithm is described in Section 3.3.
Figure 3.
