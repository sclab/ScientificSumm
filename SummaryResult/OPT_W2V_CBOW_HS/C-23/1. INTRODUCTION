In this paper, we propose a dynamic co-allocation scheme based on co-allocation Grid data transfer architecture called  RecursiveAdjustment Co-Allocation scheme that reduces the idle time spent waiting for the slowest server and improves data transfer performance [24].
Another way is to use co-allocation technology [17] to download data.
Downloading large datasets from several replica locations may result in varied performance rates, because the replica sites may have different architectures, system loadings, and network connectivity.
The data grid infrastructure integrates data storage devices and data management services into the grid environment, which consists of scattered computing and storage resources, perhaps located in different countries/regions yet accessible to users [12].
Co-allocation of data transfers enables the clients to download data from multiple locations by establishing multiple connections in parallel.
This method selects the best servers to provide optimum transfer rates because bandwidth quality can vary unpredictably due to the sharing nature of the internet.
Several co-allocation strategies were provided in previous work [17].
Certain data-intensive scientific applications, such as high-energy physics, bioinformatics applications and virtual astrophysical observatories, entail huge amounts of data that require data file management systems to replicate files and manage data transfers and distributed data access.
In Section 4, an efficient replica selection service is proposed by us.
Our research approaches are outlined in Section 5, and experimental results and a performance evaluation of our scheme are presented in Section 6.
This can improve the performance compared to the single-server cases and alleviate the internet congestion problem [17].
Bandwidth quality is the most important factor affecting transfers between clients and servers since download speeds are limited by the bandwidth traffic congestion in the links connecting the servers to the clients.
Recently, large-scale, data-sharing scientific communities such as those described in [1, 5] used this technology to replicate their large datasets over several sites.
One way to improve download speeds is to determine the best replica locations using replica selection techniques [19].
Therefore, it is important to reduce the differences in finishing time among replica servers.
Replicating popular content in distributed servers is widely used in practice [14, 17, 19].
We also discuss combination cost and provide an effective scheme for reducing it.
Most Data Grid applications execute simultaneously and access large numbers of data files in the Grid environment.
Related background review and studies are presented in Section 2 and the co-allocation architecture and related work are introduced in Section 3.
Data Grids aggregate distributed resources for solving large-size dataset management problems.
Experimental results show that our approach is superior to previous methods and achieved the best overall performance.
An idle-time drawback remains since faster servers must wait for the slowest server to deliver its final block.
The remainder of this paper is organized as follows.
Section 7 concludes this research paper. 
