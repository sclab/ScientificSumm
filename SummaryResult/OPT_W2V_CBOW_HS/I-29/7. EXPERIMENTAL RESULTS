These classes,  summarized in Table 1, were designed to evaluate key aspects of a set of Coordinators distributed scheduling agents, such as their ability to handle unexpected execution results, chains of nle"s involving multiple agents, and effective scheduling of new activities that arise unexpectedly at some point  during the problem run.
Only simple tactics were adopted to explicitly address such uncertainty, such as the use of expected durations and quality for activities and a policy of excluding from consideration those activities with failure likelihood of >75%.
This evaluation involved over 2000 problem instances randomly generated by a scenario  generator that was configured to produce scenarios of varying Problem Class Description Agent Class Quality OD ‘Only Dynamics".
97.9% (390 probs) Actual task duration & quality vary according to distribution.
Even when such activities are successfully installed they tend to be prone to deadline failures -If a source-side activity(s) either fails or exceeds its expected duration the resulting longer duration of the target activity can violate its time window deadline. 
These results are particularly notable given that each agent"s STN-based scheduler does very little reasoning over the success probability of the  activity sequences it selects to execute.
The evaluation team employed an MDP-based solver capable of unrolling the entire search space for these problems, choosing for an agent at each execution decision point the activity most likely to produce maximum global quality.
The other half of  underperforming TT problems involve activities that depend on facilitation relationships in order to fit in their time windows (recall that facilitation increases quality and decreases  duration).
cTaems 99.0% (360 probs) model is augmented with new tasks dynamically during run.
As reported in Table 1, the year 1 prototype agent clearly compares favorably to the benchmark on all classes,  coming within 2% of the MDP optimal averaged over the  entire set of 2190 problems.
‘Agent Quality" is % of ‘optimal" durations within six experiment classes.
on Autonomous Agents and Multi-Agent Systems (AAMAS 07) tivities must be scheduled are so tight that any scheduled activity which executes with a longer duration than the  expected value, causes a deadline failure.
The agent turns in its lowest performance on the TT (Temporal Tightness) experiment classes, and an  examination of the agent trace logs reveals possible reasons.
The very respectable agent  performance can be at least partially credited to the fact that the flexible times representation employed by the scheduler affords it an important buffer against the uncertainty of  execution and exogenous events.
SYNC Problems contain range of 97.1% (360 probs) different Sync sum tasks NTA ‘New Task Arrival".
An initial version of the agent described in this paper was developed in collaboration with SRI International and subjected to the independently conducted Coordinators  programmatic evaluation.
In about half of the TT problems the year 1 agent under-performs on, the specified time windows within which an agent"s  ac490 The Sixth Intl.
Release - 94.9% (360 probs) Deadline windows preclude preferred high quality (longest duration) tasks from all being scheduled.
The  hardware configuration used by the evaluators instantiated and ran one agent per machine, dedicating a separate machine to the MASS simulator.
The limited facilitates reasoning performed by the year 1 scheduler sometimes causes failures to install a  heavily facilitated initial schedule.
This constitutes a case where more sophisticated reasoning over success  probability would benefit this agent.
Year 1 evaluation problems were  constrained to be small enough (3 -10 agents, 50 - 100 methods) such that comparison against an optimal centralized solver was feasible.
facilitates) CHAINS Activities chained together 99.5% (360 probs) via sequences of enables NLEs (1-4 chains/prob) TT ‘Temporal Tightness".
This established a challenging benchmark for the distributed agent systems to compare against.
OVERALL Avg: 98.1% (2190 probs) Std dev: 6.96 Table 1: Performance of year 1 agent over  Coordinators evaluation.
Frequent & 100% (360 probs) random (esp.
No NLEs.
Joint Conf.
INT ‘Interdependent".
