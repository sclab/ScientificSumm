When the large parts of inventory is compatible with the customer"s preferences as in the first test case, the performance of all techniques are nearly same (e.g., Scenario 1).
Table 3 shows these preferences and availability of those products in the inventory for first five scenarios.
The first seven scenarios are tested with the first dataset that  contains a total of 19 services and the last three scenarios are tested with the second dataset that contains 50 services.
In addition, the location of wine should be in Europe.
We take the average of these numbers in order to evaluate these systems fairly.
Table 3: Availability of wines in different test scenarios ID Preference of consumer Availability (out of 19) 1 Dry wine 15 2 Red and dry wine 8 3 Red, dry and moderate wine 4 4 Red and strong wine 2 5 Red or rose, and strong 3 7.1 Comparison of Learning Algorithms In comparison of learning algorithms, we use the five scenarios in Table 3.
• The customer wants to buy delicate white wine whose  producer in the category of Expensive Winery.
With these test cases, we are interested in finding the number of  iterations that are required for the producer to generate an acceptable offer for the consumer.
Their performance is comparable and they can handle all cases including Scenario 5.
• The customer wants to buy expensive red wine, which is  located around California Region or cheap white wine, which is located in around Texas Region.
Since the performance also depends on the initial request, we repeat our experiments with different initial  requests.
Moreover, the winery of the wine should not be expensive.
• The customer wants to buy moderate rose wine, which is  located around French Region.
Each test scenario contains a list of  preferences for the user and number of matches from the product list.
Table 4: Comparison of learning algorithms in terms of average number of interactions Run DCEA SCR RO CEA ID3 Scenario 1: 1.2 1.4 1.2 1.2 1.2 Scenario 2: 1.4 1.4 2.6 1.4 1.4 Scenario 3: 1.4 1.8 4.4 1.4 1.4 Scenario 4: 2.2 2.8 9.6 1.8 2 Scenario 5: 2 2.6 7.6 1.75+ No offer 1.8 Avg.
CEA gives the best results when it can generate an answer but  cannot handle the cases containing disjunctive preferences, such as the one in Scenario 5.
When the results are examined, considering semantic closeness increases the performance. 
There are two products meeting all these requirements.
• The customer wants to buy wine whose winery is located in California and whose grape is a type of white grape.
The second worst method is SCR since it only considers the customer"s most recent request and does not learn from previous requests.
As is customary, we test each  algorithm with the same initial requests.
As the number of compatible services drops, RO performs poorly as expected.
Furthermore, the winery of the wine should be an expensive winery.
We apply a variety of scenarios on this dataset in order to see the performance differences.
Note that these preferences are internal to the consumer and the producer tries to learn these during negotiation.
Table 5 gives the performance evaluation in terms of the number of interactions needed to reach a consensus.
The category of winery should be Moderate Winery.
Tversky"s metric gives the worst results since it does not consider the semantic similarity.
We evaluate the performance of the proposed systems in respect to learning technique they used, DCEA and ID3, by comparing them with the CEA, RO (for random offering), and SCR (offering based on current request only).
Consequently, for each case, we run the algorithms five times with several variations of the initial requests.
There are five available products.
Wu Palmer"s metric and RP similarity measure nearly give the same performance and better than others.
• The customer wants to buy wine whose color is red or rose and grape type is red grape.
In each  experiment, we count the number of iterations that were needed to reach an agreement.
There are only four products meeting these conditions.
ID3 and DCEA achieve the best results.
There are two available products.
of all cases: 1.64 2 5.08 1.51+No offer 1.56 7.2 Comparison of Similarity Metrics To compare the similarity metrics that were explained in  Section 5, we fix the learning algorithm to DCEA.
There is only one product meeting these requirements.
In addition to the scenarios shown in Table 3, we add following five new scenarios considering the hierarchical information.
The flavor should be delicate or moderate where the body should be medium or light.
Lin"s performance are better than Tversky but worse than others.
Table 4 compares the approaches using different learning  algorithm.
The sweetness degree is wished to be dry or off dry.
Here, first we use Tversky"s similarity measure.
