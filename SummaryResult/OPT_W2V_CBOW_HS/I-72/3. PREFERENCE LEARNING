Then, we generate new hypotheses as the number of all possible attributes by using the removed hypothesis.
The second set is the most specific set S. S contains only hypotheses that are known to identify the concept that is being learned.
For instance, if the example continues with a negative sample (Full, Strong, Red), we can specialize the previous rule such as {(?-Full), ?, (?-Red)}.
However, we do not eliminate the  hypotheses in G that do not cover this sample since G now contains a disjunction of many hypotheses, some of which will be conflicting with each other.
Each attribute of the hypothesis has two parts: inclusive list, which holds the list of valid values for that attribute and exclusive list, which is the list of values which cannot be taken for that feature.
Accordingly, the consumer generates a request where the color feature is set to red and other features are set to arbitrary values, e.g.
During the interactions, each request of the consumer can be  considered as a positive example and each counter offer generated by the producer and rejected by the consumer agent can be thought of as a negative example.
This stems from the possibility of using this rule in the  generation of other hypotheses.
To be able to learn  disjunctive concepts, they create new version spaces by examining the consistency between G and S. We deal with the problem of not supporting disjunctive concepts of CEA by extending our hypothesis language to include  disjunctive hypothesis in addition to the conjunctives and negation.
from the general set while having a positive sample such as (Full, Strong, White).
Applying this algorithm to our system with the intention of learning the consumer"s preferences is  appropriate since this algorithm also supports learning disjunctive  concepts in addition to conjunctive concepts.
The requests of the consumer and the counter offers of the  producer are represented as vectors, where each element in the vector corresponds to the value of a feature.
by three disjunctive hypotheses; each hypothesis being minimally  specialized.
The ID3 algorithm is used in the learning process with the  purpose of classification of offers.
This means that the consumer is willing to accept any wine offered by the producers as long as the color is red.
When we want to find whether the given service description is  acceptable, we start searching from the root node by examining the value of test attributes until reaching a leaf node.
on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Table 1: How DCEA works Type Sample The most The most general set specific set + (Full,Strong,White) {(?, ?, ?)}
The Sixth Intl.
Note that compared to the previous studies of disjunctive versions, our  approach uses only a single version space rather than multiple version space.
The decision tree has two types of nodes: leaf node in which the class labels of the instances are held and non-leaf nodes in which test attributes are held.
on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 1303 Algorithm 1 Disjunctive Candidate Elimination Algorithm 1: G ←the set of maximally general hypotheses in H 2: S ←the set of maximally specific hypotheses in H 3: For each training example, d 4: if d is a positive example then 5: Add d to S 6: if s in S can be combined with d to make one element then 7: Combine s and d into sd {sd is the rule covers s and d} 8: end if 9: end if 10: if d is a negative example then 11: For each hypothesis g in G does cover d 12: * Assume : g = (x1, x2, ..., xn) and d = (d1, d2, ..., dn) 13: - Remove g from G 14: - Add hypotheses g1, g2, gn where g1= (x1-d1, x2,..., xn), g2= (x1, x2-d2,..., xn),..., and gn= (x1, x2,..., xn-dn) 15: - Remove from G any hypothesis that is less general than another hypothesis in G 16: end if EXAMPLE 2.
The consumer may prefer red wine or rose wine but not white wine.
In this process, at each time one attribute value of negative sample is applied to the hypothesis in the general set.
Removing a specific hypothesis from G will result in loss of information, since other hypotheses are not guaranteed to cover it.
Only when all the values exist in the list, they will be replaced by ?.
Decision trees can learn from examples easily and classify new instances as positive or negative.
As the name suggests, it is a  generalization and contains all possible values unless the values have been identified not to represent the concept.
At the beginning of negotiation, the producer agent does not know the consumer"s preferences but will need to learn them  using information obtained from the dialogues between the producer and the consumer.
On the contrary, the winery may not be as important as the color for this customer, so the consumer may have a tendency to accept wines from any winery as long as the color is red.
If a hypothesis includes red in its exclusive list and ?
On the other hand, we need to learn disjunctive concepts in the negotiation of a service since consumer may have several  alternative wishes.
3.1 CEA CEA [10] is one of the inductive learning algorithms that learns concepts from observed examples.
The preferences denote the relative importance of the features of the services demanded by the consumer agents.
The first one is inductive learning.
When a positive sample comes, the most specific set S should be generalized in order to cover the new training instance.
As a  result, the most general hypotheses and the most special hypotheses cover all positive training samples but do not cover any negative ones.
After the first positive sample, S is generalized to also cover the instance.
The algorithm maintains two sets to model the concept to be learned.
}, {(Medium,Moderate,Red)}} {?, ?, (?-Rose)}} the offers involving the wine whose color is white or rose.
Positive means that the service description will possibly be accepted by the consumer agent whereas the negative implies that it will potentially be rejected by the consumer.
Consumer"s  requests are considered as positive training examples and all rejected counter-offers are thought as negative ones.
in its inclusive list, this means that color may take any value except red.
For instance, the color of the wine may be important so the  consumer insists on buying the wine whose color is red and rejects all 1302 The Sixth Intl.
The original CEA will generalize this as (Light, Delicate, ?
For each attribute in the negative sample, we add one of them at each time to the exclusive list of the removed hypothesis.
We only modify the most general hypotheses not to cover this  negative sample (lines 11-15).
This is necessary since no training data is  available before the interactions start.
Note that, exclusive list contains the values that the attribute cannot take.
The problem with this algorithm is that it is not an  incremental algorithm, which means all the training examples should exist before learning.
For this reason, we instead use the ID3 algorithm [13] and iteratively build decision trees to simulate incremental learning.
Interestingly, most of the time consumer preferences are disjunctive.
The negative  samples enforce the specialization of some hypotheses so that G does not cover any hypothesis accepting the negative samples as  positive.
Say, we are considering an agent that is buying wine.
To overcome this problem, the system keeps  consumer"s requests throughout the negotiation interaction as positive examples and all counter-offers rejected by the consumer as  negative examples.
CEA is known to perform poorly if the information to be learned is  disjunctive.
The requests of the consumers represent individual wine products whereas their preferences are constraints over service features.
When a negative sample comes, we do not change S as before.
The first set is the most general set G. G contains hypotheses about all the possible values that the concept may obtain.
After some time, some hypotheses in S can be merged and can construct one hypothesis (lines 6, 7).
We elaborate on  Candidate Elimination Algorithm (CEA) for Version Space [10].
However, in fact, we only know that the color can be red or white.
At each interaction between the producer and the consumer, both G and S are modified.
After each coming request, the decision tree is rebuilt.
When these sets are equal, the algorithm converges by means of reaching the target concept.
Thus, all possible hypotheses that do not cover the negative sample are generated (line 14).
For example, a consumer may have preference for red wine.
The most general set and the most specific set show the contents of G and S after the sample comes in.
To use CEA with such preferences, a solid modification is  necessary.
The third sample is positive and generalizes S even more.
),  meaning the color can take any value.
3.3 ID3 ID3 [13] is an algorithm that constructs decision trees in a  topdown fashion from the observed examples represented in a vector with attribute-value pairs.
Different from the original CEA, we try to specialize the G minimally.
For instance, body, flavor, color and so on are potential test attributes for wine service.
If any positive sample comes, we add the sample to the special set as before (line 4).
To tackle this problem, we propose to use incremental learning algorithms [6].
At the beginning of the algorithm, G is initialized to cover all possible concepts while S is initialized to be empty.
We modify the CEA algorithm to deal with this change.
The second approach is decision trees.
In other words, we let the algorithm generalize more slowly than before.
In the DCEA, we generalize it as {(Light, Delicate, [White, Red] )}.
The algorithm removes the  hypothesis covering the negative sample (line 13).
Assume that the most specific set is {(Light,  Delicate, Red)} and a positive example, (Light, Delicate, White) comes.
maintain several version spaces by split and merge operation [7].
There are several studies on learning disjunctive concepts via Version Space.
The second sample is negative.
The test attribute in a non-leaf node is one of the attributes making up the service description.
This technique is applied to learn the preferences as concepts.
By Algorithm 1, we do not miss any information.
Thus, we replace (?, ?, ?)
For example, consider the color attribute.
There are two classes: positive and negative.
The modified algorithm, DCEA, is given as Algorithm 1.
Incrementally, G specializes and S generalizes until G and S are equal to each other.
Some of these approaches use multiple version space.
A well-known incremental decision tree is ID5R [18].
However, in practice we have evaluated ID3 to be fast and the reconstruction cost to be negligible. 
The initialization phase is the same as the original algorithm (lines 1, 2).
}, - (Full,Delicate,Rose) {?, (?-Delicate), ?
Without doubt, there is a drawback of reconstruction such as additional process load.
Note that in Table 1, we do not eliminate {(?-Full), ?, ?}
{(Full,Strong,White)} {{(?-Full), ?, ?
}, {(Full,Strong,White)} {?, ?, (?-Rose)}} {{(?-Full), ?, ?
Table 1 illustrates the first three interactions and the workings of DCEA.
3.2 Disjunctive CEA Unfortunately, CEA is primarily targeted for conjunctive  concepts.
}, {{(Full,Strong,White)}, + (Medium,Moderate,Red) {?,(?-Delicate), ?
However, ID5R is known to suffer from high computational  complexity.
(Medium, Strong, Red).
We particularly investigate two approaches.
For instance, Hong et al.
EXAMPLE 1.
Joint Conf.
Joint Conf.
