This reflects the fact that search engines do a reasonable job of ranking results as well as biases to click top results and  noisewe attempt to separate these components in the analysis that follows.
First we consider the distribution of clicks for the relevant documents for these queries.
Our main approach is to model user web search behavior as if it were generated by two components: a relevance component -  queryspecific behavior influenced by the apparent result relevance, and a background component - users clicking indiscriminately.
We can then analyze the user behavior for all the instances where these queries were submitted to the search engine.
For this, we use a large random sample of queries in the search query log of a popular web search engine, the sets of results (identified by URLs) returned for each of the queries, and any explicit relevance judgments available for each query/result pair.
Hence, in addition to basic features, which we will describe in detail in Section 3.2, we compute derived features that measure the deviation of the observed feature value for a given search result from the expected values for a result, with no query-dependent information.
More formally, we postulate that the observed value o of a feature f for a query q and result r can be expressed as a mixture of two components: ),,()(),,( frqrelfCfrqo += (1) where )( fC is the prior background distribution for values of f aggregated across all queries, and rel(q,r,f) is the component of the behavior influenced by the relevance of the result r. As illustrated above with the clickthrough feature, if we subtract the background distribution (i.e., the expected clickthrough for a result at a given position) from the observed clickthrough frequency at a given position, we can approximate the relevance component of the clickthrough value1 .
To model this aspect of user experience we defined features to characterize the nature of the query and its relation to the snippet text.
Our general approach is to train a classifier to induce weights for the user behavior features, and consequently derive a predictive model of user preferences.
Query-text features TitleOverlap Fraction of shared words between query and title SummaryOverlap Fraction of shared words between query and summary QueryURLOverlap Fraction of shared words between query and URL QueryDomainOverlap Fraction of shared words between query and domain QueryLength Number of tokens in query QueryNextOverlap Average fraction of words shared with next query Browsing features TimeOnPage Page dwell time CumulativeTimeOnPage Cumulative time for all subsequent pages after search TimeOnDomain Cumulative dwell time for this domain TimeOnShortUrl Cumulative time on URL prefix, dropping parameters IsFollowedLink 1 if followed link to result, 0 otherwise IsExactUrlMatch 0 if aggressive normalization used, 1 otherwise IsRedirected 1 if initial URL same as final URL, 0 otherwise IsPathFromSearch 1 if only followed links after query, 0 otherwise ClicksFromSearch Number of hops to reach page from query AverageDwellTime Average time on page for this query DwellTimeDeviation Deviation from overall average dwell time on page CumulativeDeviation Deviation from average cumulative time on page DomainDeviation Deviation from average time on domain ShortURLDeviation Deviation from average time on short URL Clickthrough features Position Position of the URL in Current ranking ClickFrequency Number of clicks for this query, URL pair ClickRelativeFrequency Relative frequency of a click for this query and URL ClickDeviation Deviation from expected click frequency IsNextClicked 1 if there is a click on next position, 0 otherwise IsPreviousClicked 1 if there is a click on previous position, 0 otherwise IsClickAbove 1 if there is a click above, 0 otherwise IsClickBelow 1 if there is click below, 0 otherwise Table 3.1: Features used to represent post-search interactions for a given query and search result URL 3.4 Learning a Predictive Behavior Model Having described our features, we now turn to the actual method of mapping the features to user preferences.
These features allows us to model intra-query diversity of page browsing behavior (e.g., navigational queries, on average, are likely to have shorter page dwell time than transactional or informational queries).
For example, for queries with top relevant result in position 2, the relative click frequency at that position (second bar) is higher than the click frequency at other positions for these queries.
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 2 3 5 10 result position relativeclickfrequency PTR=1 PTR=2 PTR=3 PTR=5 PTR=10 Background Figure 3.2: Relative click frequency for queries with varying PTR (Position of Top Relevant document).
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 3 5 7 9 11 13 15 17 19 21 23 25 27 29 result position RelativeClickFrequency Figure 3.1: Relative click frequency for top 30 result positions over 3,500 queries and 120,000 searches.
In order to reduce the effect of individual user variations in behavior, we average observed feature values across all users and search sessions for each query-URL pair.
For example, for a query-URL pair we provide the number of clicks for the result (ClickFrequency), as 1 Of course, this is just a rough estimate, as the observed background distribution also includes the relevance component.
The training is done by comparing a wide range of implicit behavior measures with explicit user judgments for a set of queries.
As we noted earlier, real web search user behavior can be noisy in the sense that user behaviors are only probabilistically related to explicit relevance judgments and preferences.
Although clickthrough distributions are heavily biased towards top results, we have just shown how the â€˜relevance-driven" click distribution can be recovered by correcting for the prior, background distribution.
We now introduce our techniques for going beyond the clickthrough statistics and explicitly modeling post-search user behavior.
Our general model includes two feature types for describing user behavior: direct and deviational where the former is the directly measured values, and latter is deviation from the expected values estimated from the overall (query-independent) distributions for the corresponding directly observed features.
The aggregated click frequency at result position p is calculated by first computing the frequency of a click at p for each query (i.e., approximating the probability that a randomly chosen click for that query would land on position p).
For example, we compute how long users dwell on a page (TimeOnPage) or domain (TimeOnDomain), and the deviation of dwell time from expected page dwell time for a query.
3.2 Robust User Behavior Model Clicks on search results comprise only a small fraction of the post-search activities typically performed by users.
We now describe the actual features we use to represent user behavior.
3.3 Features for Representing User Behavior Our goal is to devise a sufficiently rich set of features that allow us to characterize when a user will be satisfied with a web search result.
We motivate our intuitions with a particularly important behavior feature, result clickthrough, analyzed next, and then introduce our general model of user behavior that incorporates other user actions (Section 3.2).
While there are many clicks above the first relevant document for each distribution, there are clearly peaks in click frequency for the first relevant result.
If we subtract the background distribution of Figure 3.1 from the mixed distribution of Figure 3.2, we obtain the distribution in Figure 3.3, where the remaining click frequency distribution can be interpreted as the relevance component of the results.
We attempt to learn a general implicit feedback interpretation strategy automatically instead of relying on heuristics or insights.
Clickthrough features: Clicks are a special case of user interaction with the search engine.
In summary, the user behavior for a query-URL pair is represented by a feature vector that includes both the directly observed features and the derived, corrected feature values.
Query-text features: Users decide which results to examine in more detail by looking at the result title, URL, and summary - in some cases, looking at the original document is not even necessary.
We also have explicit relevance judgments for the top 10 results for each query.
-0.06 -0.04 -0.02 0 0.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 1 2 3 5 10 result position correctedrelativeclickfrequency PTR=1 PTR=2 PTR=3 PTR=5 PTR=10 Figure 3.3: Relative corrected click frequency for relevant documents with varying PTR (Position of Top Relevant).
To learn the mapping from features to relevance preferences, we use a scalable implementation of neural networks, RankNet [4], capable of learning to rank a set of given items.
The resulting distribution agrees with previous observations that users click more often on top-ranked results.
These features are used to characterize interactions with pages beyond the results page.
Nevertheless, many users still click on the non-relevant results in position 1 for such queries.
This shows a stronger property of the bias in the click distribution towards top results - users click more often on results that are ranked higher, even when they are not relevant.
An important difference is that many of our features are (by design) query specific whereas theirs was (by design) a general,  queryindependent model of user behavior.
3.1 A Case Study in Click Distributions As we discussed, we aggregate statistics across many user sessions.
Note that the corrected click distribution correlates closely with actual result relevance as explicitly rated by human judges.
RankNet has demonstrated excellent performance in learning to rank objects in a supervised setting, hence we use RankNet for our experiments. 
The features we use to represent user search interactions are summarized in Table 3.1.
In general, individual user behavior, clickthrough and otherwise, is noisy, and cannot be relied upon for accurate relevance judgments.
For these queries we aggregate click data over more than 120,000 searches performed over a three week period.
We consider this approach to be preferable to heuristic strategies, because we can always mine more data instead of relying (only) on our intuition and limited laboratory evidence.
Once the user has submitted a query, they perform many different actions (reading snippets, clicking results, navigating, refining their query) which we capture and summarize.
For clarity, we organize the features into the groups Query-text, Clickthrough, and Browsing.
Figure 3.2 reports the aggregated click distribution for queries with varying Position of Top Relevant document (PTR).
Figure 3.1 shows the relative clickthrough frequency as a function of result position.
Browsing features: Simple aspects of the user web page interactions can be captured and quantified.
Our general idea is to model the deviations from the expected user behavior.
These include features such as overlap between the words in title and in query (TitleOverlap), the fraction of words shared by the query and the result summary (SummaryOverlap), etc.
A click on a result may mean that some user found the result summary promising; it could also be caused by people clicking indiscriminately.
well as whether there was a click on result below or above the current URL (IsClickBelow, IsClickAbove).
This aggregation gives additional robustness of not relying on individual noisy user interactions.
We conjecture that other aspects of user behavior (e.g., page dwell time) are similarly distorted.
For the present it suffices to note that we focus on a random sample of 3,500 queries that were randomly sampled from query logs.
We include both the direct features and the derived features described above.
Hence, instead of treating each user as a reliable expert, we aggregate information from many unreliable user search session traces.
These frequencies are then averaged across queries and normalized so that relative frequency of a click at the top position is 1.
If so, the label is assigned to the query/URL pair and to the corresponding feature vector for that search result.
This rich representation of user behavior is similar in many respects to the recent work by Fox et al.
This information was obtained via opt-in client-side instrumentation from users of a major web search engine.
These vectors of feature values corresponding to URLs judged relevant or non-relevant by human annotators become our training set.
Furthermore, we include derived, distributional features computed as described above.
More specifically, for each judged query we check if a result link has been judged.
We include all the features necessary to learn the clickthrough-based strategies described in Sections 4.1 and 4.4.
The derived feature values such as ClickRelativeFrequency and ClickDeviation are computed as described in Equation 1.
The data set is described in more detail in Section 5.2.
