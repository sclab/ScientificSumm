By using the aggregated behavior of large numbers of users (and not treating each user as an individual expert) we can correct for the noise inherent in individual interactions, and generate relevance judgments that are more accurate than techniques not specifically designed for the web search setting.
We present techniques to automatically interpret the collective behavior of users interacting with a web search engine to predict user preferences for search results.
If we could turn these interactions into relevance judgments, we could obtain large amounts of data for evaluating, maintaining, and improving information retrieval systems.
We discuss our results and outline future directions and various applications of this work in Section 7, which concludes the paper. 
Relevance measurement is crucial to web search and to information retrieval in general.
Recently, automatic or implicit relevance feedback has developed into an active area of research in the information retrieval community, at least in part due to an increase in available resources and to the rising popularity of web search.
Our contributions include: • A distributional model of user behavior, robust to noise within individual user sessions, that can recover relevance preferences from user interactions (Section 3).
Hence, it is preferable to automatically induce feedback interpretation strategies from large amounts of user interactions.
At the same time, millions of people interact daily with web search engines, providing valuable implicit feedback through their interactions with the search results.
A significant distinction is that web search is not controlled.
Automatically learning to interpret user behavior would allow systems to adapt to changing conditions, changing user behavior patterns, and different search settings.
Therefore, it is not clear whether these techniques will work for general real-world web search.
• A thorough evaluation of our user behavior models, as well as of previously published state-of-the-art techniques, over a large set of web search sessions (Sections 5 and 6).
Individual users may behave irrationally or maliciously, or may not even be real users; all of this affects the data that can be gathered.
However, explicit human ratings are expensive and difficult to obtain.
Furthermore, observations and insights obtained in laboratory settings do not necessarily translate to real world usage.
• Extensions of existing clickthrough strategies to include richer browsing and interaction features (Section 4).
But the amount of the user interaction data is orders of magnitude larger than anything available in a non-web-search setting.
Traditionally, search relevance is measured by using human assessors to judge the relevance of query-document pairs.
However, most traditional IR work was performed over controlled test collections and carefully-selected query sets and tasks.
