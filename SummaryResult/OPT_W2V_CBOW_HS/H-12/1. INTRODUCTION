Snippets may be even more useful in database or filesystem search applications in which no useful URL or title information is present.
In addition, one or more snippets are usually presented,  giving the searcher a sneak preview of the document contents.
In turn, the more documents can be  compressed, the more can fit in cache, and hence the more disk seeks can be avoided: the classic data compression tradeoff that is exploited in inverted file structures and computing ranked document lists [24].
Generation of query-biased snippets by Web search  engines indexing of the order of ten billion web pages and  handling hundreds of millions of search queries per day imposes a very significant computational load (remembering that each search typically generates ten snippets).
Each result in search results list delivered by current WWW search engines such as search.yahoo.com, google.com and search.msn.com typically contains the title and URL of the actual document, links to live and cached versions of the document and sometimes an indication of file size and type.
In the best case, snippets may obviate the need to open any documents by directly providing the answer to the searcher"s real information need, such as the contact details of a person or an organization.
As hitting the document cache is important, we examine document compaction, as opposed to compression, schemes by imposing an a priori ordering of sentences within a  document, and then only allowing leading sentences into cache for each document.
The  simpleminded approach of keeping a copy of each document in a file and generating snippets by opening and scanning files, works when query rates are low and collections are small, but does not scale to the degree required.
Note that the utility of snippets is by no means restricted to whole-of-Web search applications.
As the time to process a document in RAM is small in comparison to locating and reading the document into  memory, it may seem that compression is not required.
Efficient generation of snippets is also important at the scale of whole-of-government search services such as www.firstgov.gov (c. 25 million pages) and govsearch.australia.gov.au (c. 5 million pages) and within large enterprises such as IBM [2] (c. 50 million pages).
The addition of informative snippets to search results may substantially increase their value to searchers.
They may be static (for example, always show the first 50 words of the  document, or the content of its description metadata, or a  description taken from a directory site such as dmoz.org) or query-biased [20].
Controlling the RAM of physical systems for  experimentation is difficult, hence we use simulation to show that caching documents dramatically improves the performance of  snippet generation.
Accurate snippets allow the searcher to make good decisions about which results are worth accessing and which can be ignored.
The overhead of opening and reading ten files per query on top of  accessing the index structure to locate them, would be manifestly excessive under heavy query load.
A query-biased snippet is one selectively extracted on the basis of its relation to the searcher"s query.
Snippets are short fragments of text extracted from the document content (or its metadata).
This leads to further time savings, with only marginal impact on the quality of the snippets returned. 
Even storing ten billion files and the corresponding hundreds of terabytes of data is beyond the reach of traditional filesystems.
We present a new algorithm and compact single-file  structure designed for rapid generation of high quality snippets and compare its space/time performance against an obvious baseline based on the zlib compressor on various data sets.
We report the proportion of time spent for disk seeks, disk reads and cpu processing; demonstrating that the time for locating each document (seek time) dominates, as expected.
However, this is only true if there is no caching of documents in RAM.
Special-purpose filesystems have been built to address these problems [6].
