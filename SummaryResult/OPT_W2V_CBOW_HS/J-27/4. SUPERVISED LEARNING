W.l.g we may assume the error is bounded (since we are looking at what is essentially a finite set) therefore the probability that erσ(h, fb0 ) > ε cannot be too small, hence fb0 is not  PAClearnable with a sample of size n ✷ The following theorem gives an upper bound on the  sample complexity required for learning a set of functions with finite fat shattering dimension.
In the case of discrete learning, we would like to obtain a  function h that with high probability agrees with f. We would then take the probability Pσ(f(x) = h(x)) as the measure of the quality of the estimation.
Suppose that a learning algorithm observes a finite set of production data which it assumes comes from a Cobb-Douglas  production function and returns a hypothesis that is a polynomial of bounded degree.
Then any learning algorithm L for C has sample complexity satisfying mL(ε, δ) ≥ 1 2 fatC(4dε) An analog of this theorem for real valued functions with a tighter bound can be found in [2], this version will suffice for our needs.
For discrete function sets, sample complexity bounds may be derived from the VC-dimension of the set (see [19, 8]).
The minimal mL is called the sample complexity of C. Note that in the definition there is no mention of the time complexity to find h in H and evaluating h(p).
If the class of target functions has finite  "dimensionality" then a function in the class is characterized by its values on a finite number of points.
We define the γ-fat shattering dimension of C, denoted fatC(γ) as the maximal size of a γ-shattered set in Γ.
A set of demand functions C is probably approximately correct (PAC) learnable by hypotheses set H if for any ε, δ > 0, f ∈ C and distribution σ on the prices 39 there exists an algorithm L that for a set of observations of length mL = mL(ε, δ) = Poly(1 δ , 1 ε ) finds a function h from H such that erσ(f, h) < ε with probability 1 − δ.
, (pin , xn)) a learning algorithm L finds a function h. The  observation above and Fubini imply Eb(erσ(h, fb)) > ε.  Randomizing on the sample space we get Eb,z(erσ(h, fb)) > ε.
The theory of PAC learning for real valued functions is concerned predominantly with functions from Rd to R. In this section we introduce modifications to the classical notions of PAC learning to vector valued functions and use them to prove a lower bound for sample complexity.
, p2n} that is shattered by C. It suffices to show that at least one distribution requires large sample.
It follows from lemma 2 (with γ = 2d ) that for any fixed function h the probability that ||fb(p) − h(p)||∞ > 2ε for p ∈ ΓS is at least as high as getting heads on a fair coin toss.
Proof : Since the max exceeds the mean, it follows that if fb and fb correspond to labels such that bi = bi then ||fb(pi) − fb (pi)||∞ ≥ 1 d ||fb(pi) − fb (pi)||2 > γ d .
In the case of revealed preference, there is a function that takes the sample error to zero.
The theorem is proved in [2] for real valued functions, the proof for the real vector case is analogous and so omitted.
Demand functions are real vector functions and we therefore do not expect f and h to agree with high probability.
The theory tells us that if the sample is large enough then any function that tends to agree with the  labeling will, with high probability, be a good approximation of the target function for future observations.
Suppose that C is a class of functions  mapping from Γ to Rd +.
Let σ be the uniform distribution on ΓS and CS = {fb : b ∈ {0, 1}2n } be the set of functions that witness the shattering of {p1.
A set C is efficiently PAC-learnable if there is a Poly(1 δ , 1 ε ) time  algorithm for choosing h and evaluating h(p).
An upper bound on the sample complexity can also be proved for our definition of fat shattering, but we do not bring it here as the proof is much more tedious and analogous to the proof of theorem 4.
It is also assumed that the  observations are generated independently by some distribution on the domain of the relation and that this distribution is fixed.
, bn) ∈ {0, 1}n there exists a function fb ∈ C such that fb(pi) ∈ xi + H+ 0 if bi = 0 and f(pi) ∈ xi + H− 1 if bi = 1.
We use an adaptation of this notion to real vector valued function sets.
The estimation problem is concerned with the tradeoff between the size of the sample given to the algorithm and the degree of  confidence we have in the forecast it produces.
In the probably approximately correct (PAC) paradigm, the learning of a target function is done by a class of  hypothesis functions, that does or does not include the  target function itself; it does not necessitate any parametric assumptions on this class.
, pn ∈ Γ is γ-shattered by a class of real functions C if there exists x1, .
An analog to this notion of dimension for real functions is the fat shattering dimension.
Then L is a learning algorithm for C with sample complexity given by: mL(ε, δ) = O „ 1 ε2 (ln2 ( 1 ε )fatC(ε) + ln( 1 δ )) « for any ε, δ > 0. 
Thus, our measure of estimation error is given by: erσ(f, h) = Z (||f − h||∞)2 dσ.
This implies that for any x ∈ Rd + either ||fb(pi) − x||∞ > γ 2d or ||fb (pi) − x||∞ > γ 2d ✷ Theorem 3.
The basic idea is to observe the labeling of a finite number of points and find a  function from a class of hypotheses which tends to agree with this labeling.
If this size is unbounded then the dimension is infinite.
A sample error minimization (SEM) algorithm is an  algorithm that finds a hypothesis minimizing erS(S, h).
However, we are not limited to binary values and, indeed, in the demand functions we are studying the labels are real vectors.
Let C be a set of real-valued functions from X to [0, 1] with fatC(γ) < ∞.
, (pn, xn)} we  measure the agreement by the sample error erS(S, h) = X j (||xj − h(pj)||∞)2 .
The estimation problem in this case would be to assess the sample size needed to obtain a good estimate of the coefficients.
The labels are usually binary values indicating the membership of the  observed points in the set that is being learned.
Let Γ ⊂ Rd + and let C be a set of real functions from Γ to Rd +.
Let fb be a function chosen uniformly at random from CS.
The complexity problem is  concerned with the computational complexity of finding a  hypothesis that approximates the target function.
Then, for any x ∈ Rd + and labels b, b ∈ {0, 1}n such that bi = bi either ||fb(pi) − x||∞ > γ 2d or ||fb (pi) − x||∞ > γ 2d .
In a supervised learning problem, a learning algorithm is given a finite sample of labeled observations as input and is required to return a model of the functional relationship underlying the labeling.
This shows Eh,z(erσ(h, fb0 )) > ε for some fb0 .
To demonstrate the usefulness of the this notion we use it to derive a lower bound on the sample complexity.
Thus the target function is a mapping from prices to bundles, namely f : Rd + → Rd +.
This model, referred to as a  hypothesis, is usually a computable function that is used to forecast the labels of future observations.
Suppose the functions {fb : b ∈ {0, 1}n }  witness the shattering of {p1, .
We construct such a distribution.
Let A be approximate-SEM algorithm for C and define L(z) = A(z, ε0 6 ) for z ∈ Zm and ε0 = 16√ m .
The approximation problem would be to assess the error sustained from approximating a rational function by a polynomial.
The  approximation problem is concerned with the ability of hypotheses from a certain class to approximate target functions from a possibly different class.
There may be several learning algorithms for C with different sample complexities.
A parametric paradigm assumes that the underlying  functional relationship comes from a well defined family, such as the Cobb-Douglas production functions; the system must learn the parameters characterizing this family.
Nevertheless, the upper bounds  theorem we use does not require the sample error to be zero.
Suppose for a sequence of observations z = ((pi1 , x1), .
The prime objective of PAC theory is to develop the relevant notion of dimensionality and to formalize the tradeoff between  dimensionality, sample size and the level of confidence in the forecast.
Proof : Suppose n = 1 2 fatC(4dε) then there exists a set ΓS = {p1, .
Therefore Eb(||fb(p) − h(p)||∞) > 2ε.
In the revealed preference setting, our objective is to use a set of observations of prices and demand to forecast  demand for unobserved prices.
Before we can proceed with the formal definition, we must clarify what we mean by forecast and tend to agree.
For γ > 0, a set of points p1, .
The complexity problem would be the assessment of the time required to compute the polynomial coefficients.
, xn ∈ Rd + and parallel affine hyperplanes H0, H1 ⊂ Rd such that 0 ∈ H− 0 ∩ H+ 1 , dist(H0, H1) > γ and for each b = (b1, .
The learning problem has three major components:  estimation, approximation and complexity.
For given observations S = {(p1, x1), .
Rather we are content with having small mean square errors on all coordinates.
Theorem 4.
Definition 2.
Definition 1.
Lemma 2.
