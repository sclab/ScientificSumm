In our modeling of expert finding and profiling we collect evidence for expertise from multiple sources, in a heterogeneous collection, and integrate it with the co-occurrence of candidates" names and query terms-the language modeling  setting allows us to do this in a transparent manner.
Initial approaches to expertise finding often employed databases containing information on the skills and knowledge of each  individual in the organization [11].
Petkova and Croft [19] propose yet another approach, based on a combination of the above Model 1 and 2, explicitly modeling topics.
In the first step, we consider three baseline models, two taken from [4] (the Models 1 and 2 mentioned above), and one a refined version of a model introduced in [3] (which we refer to as Model 3 below); this third model is also similar to the model described by Petkova and Croft [19].
In contrast with focusing on particular document types, there is also an increased interest in the development of systems that index and mine published intranet documents as sources of evidence for  expertise.
One such published approach is the P@noptic system [9], which builds a representation of each person by concatenating all documents associated with that person-this is similar to Model 1 of Balog et al.
We use generative language modeling to find associations  between topics and people.
Macdonald and Ounis [16] propose a different approach for ranking candidate expertise with respect to a topic based on data fusion techniques, without using  collectionspecific heuristics; they find that applying field-based weighting models improves the ranking of candidates.
[21]; however, the features that we use for definining our expansions-including topical structure and  organizational structure-have not been used in this way before. 
Balog and de Rijke [2] study the related task of  finding experts that are similar to a small set of experts given as input.
[4], who formalize and compare two methods.
The models we  consider in our second round of experiments are mixture models  similar to contextual language models [1] and to the expanded  documents of Tao et al.
More recent approaches use specific document sets (such as email [6] or software [18]) to find expertise.
"s Model 1 directly models the knowledge of an expert from associated documents, while their Model 2 first locates documents on the topic and then finds the associated experts.
In the reported experiments the second method performs significantly better when there are sufficiently many associated documents per candidate.
Most of these tools (usually called  yellow pages or people-finding systems) rely on people to self-assess their skills against a predefined set of keywords.
For updating  profiles in these systems in an automatic fashion there is a need for intelligent technologies [5].
Turning to other expert retrieval tasks that can also be addressed using topic-people associations, Balog and de Rijke [3] addressed the task of determining topical expert profiles.
Most systems that took part in the 2005 and 2006 editions of the Expert Finding task at TREC implemented (variations on) one of these two models; see [10, 20].
As an aside, creating a textual summary of a person shows some similarities to biography finding, which has received a  considerable amount of attention recently; see e.g., [13].
Our modeling proceeds in two steps.
While their methods proved to be efficient on the W3C corpus, they require an amount of data that may not be available in the typical knowledge-intensive organization.
Balog et al.
