8.2 Experimental setup Given that the self-assessments are also sparse in our collection, in order to be able to measure differences between the various  models, we selected a subset of topics, and evaluated (some of the) runs only on this subset.
First, we evaluated the models of the organizational units, using all topics (ALL) and only the main topics (MAIN).
Table 3 reports on the multilingual results, where performance is evaluated on the full topic set.
And finally, is our simple way of combining the monolingual scores sufficient for obtaining significant improvements?
8.5 Multilingual models In this subsection we evaluate the method for combining  results across multiple languages that we described in Section 7.3.
We find a positive impact of the context models only for expert finding.
Second, we performed another evaluation, where we combined the contextual models with the candidate models (to score  candidates again).
The coverage of topics and candidates for the expert finding and profiling tasks,  respectively, is close to 100% in all cases.
Topics Model 1 Model 2 Model 3 MAP MRR MAP MRR MAP MRR Expert finding UK ALL 0.423 0.545 0.654 0.799 0.494 0.629 UK MAIN 0.500 0.621 0.704 0.834 0.587 0.699 NL ALL 0.439 0.560 0.672 0.826 0.480 0.630 NL MAIN 0.440 0.584 0.645 0.816 0.515 0.655 Expert profiling UK ALL 0.240 0.640 0.306 0.778 0.223 0.616 UK MAIN 0.523 0.677 0.519 0.648 0.461 0.587 NL ALL 0.203 0.716 0.254 0.770 0.183 0.627 NL MAIN 0.332 0.576 0.380 0.624 0.332 0.549 Table 5: Evaluating the context models on organizational units.
Expert finding Expert profiling Language Model 1 Model 2 Model 3 Model 1 Model 2 Model 3 %q MAP MRR %q MAP MRR %q MAP MRR %ca MAP MRR %ca MAP MRR %ca MAP MRR English only 97.8 0.237 0.372 98.6 0.280 0.441 98.5 0.166 0.293 100 0.199 0.387 88.7 0.281 0.525 90.9 0.169 0.329 Dutch only 61.3 0.249 0.401 62.6 0.265 0.436 62.6 0.195 0.344 91.9 0.164 0.426 90.1 0.195 0.488 91.9 0.125 0.328 Combination 99.4 0.297 0.444 99.7 0.324 0.491 99.7 0.223 0.388 100 0.241 0.445 92.1 0.313 0.564 93.2 0.224 0.411 Table 3: Performance of the combination of languages on the expert finding and profiling tasks (on candidates).
Method Model 1 Model 2 Model 3 MAP MRR MAP MRR MAP MRR English BASELINE 0.296 0.454 0.339 0.509 0.221 0.333 KLDIV 0.291 0.453 0.327 0.503 0.219 0.330 PMI 0.291 0.453 0.337 0.509 0.219 0.331 LL 0.319 0.490 0.360 0.524 0.233 0.368 HDIST 0.299 0.465 0.346 0.537 0.219 0.332 Dutch BASELINE 0.240 0.350 0.271 0.403 0.227 0.389 KLDIV 0.239 0.347 0.253 0.386 0.224 0.385 PMI 0.239 0.350 0.260 0.392 0.227 0.389 LL 0.255 0.372 0.281 0.425 0.231 0.389 HDIST 0.253 0.365 0.271 0.407 0.236 0.402 Method Model 1 Model 2 Model 3 MAP MRR MAP MRR MAP MRR English BASELINE 0.485 0.546 0.499 0.548 0.381 0.416 KLDIV 0.510 0.564 0.513 0.558 0.381 0.416 PMI 0.486 0.546 0.495 0.542 0.407 0.451 LL 0.558 0.589 0.586 0.617 0.408 0.453 HDIST 0.507 0.567 0.512 0.563 0.386 0.420 Dutch BASELINE 0.263 0.313 0.294 0.358 0.262 0.315 KLDIV 0.284 0.336 0.271 0.321 0.261 0.314 PMI 0.265 0.317 0.265 0.316 0.273 0.330 LL 0.312 0.351 0.330 0.377 0.284 0.331 HDIST 0.280 0.327 0.288 0.341 0.266 0.321 Table 4: Performance on the expert finding (top) and profiling (bottom) tasks, using knowledge area similarities.
8.1 Research Questions Our questions follow the refinements presented in the preceding section: Does exploiting the knowledge area similarity improve  effectiveness?
Method Model 1 Model 2 Model 3 MAP MRR MAP MRR MAP MRR Expert finding UK BL 0.296 0.454 0.339 0.509 0.221 0.333 UK CT 0.330 0.491 0.342 0.500 0.228 0.342 NL BL 0.240 0.350 0.271 0.403 0.227 0.389 NL CT 0.251 0.382 0.267 0.410 0.246 0.404 Expert profiling UK BL 0.485 0.546 0.499 0.548 0.381 0.416 UK CT 0.562 0.620 0.508 0.558 0.440 0.486 NL BL 0.263 0.313 0.294 0.358 0.262 0.315 NL CT 0.330 0.384 0.317 0.387 0.294 0.345 Table 6: Performance of the context models (CT) compared to the baseline (BL).
We managed to improve upon the baseline in all cases, but the improvement is more noticeable for the profiling task.
The relevance judgements were restricted to the main topic set, but were not expanded with subtopics.
Best scores for each model are in italic, absolute best scores for the expert finding and profiling tasks are in boldface.
In this section we present an experimental evaluation of our  advanced models.
Which of the various methods for capturing word  relationships is most effective?
We performed experiments with various λ settings, but did not observe significant differences in performance.
This set is referred as main topics, and consists of topics that are located at the top level of the topical hierarchy.
Runs were evaluated on the main topics set.
These scores demonstrate that despite its simplicity, our method for combining results over multiple languages achieves substantial improvements over the baseline. 
As far as expert finding goes, given a topic, the corresponding organizational unit can be identified with high precision.
The four methods used for  estimating knowledge-area similarity are KL divergence (KLDIV),  PointLang.
The poor performance on expert profiling may be due to the fact that context models alone did not perform very well on the profiling task to begin with.
The  explanation may be that general concepts (i.e., our main topics) may belong to several organizational units.
Noticably, for expert finding (and Model 1), it improves over 50% (for  English) and over 70% (for Dutch) on MAP.
However, the expert profiling task shows a different picture: the scores are low, and the task seems hard.
An organizational unit is considered to be relevant for a given topic (or vice versa) if at least one member of the unit selected the given topic as an expertise area.
The weights on these languages were set to be identical (λUK = λNL = 0.5).
8.4 Contextual information A two level hierarchy of organizational units (faculties and  institutes) is available in the UvT Expert collection.
All three models significantly  imLang.
Best scores are in boldface.
Best scores are in boldface.
(A main topic has subtopics, but is not a subtopic of any other topic.)
proved over all measures for both tasks.
For both tasks, the LL method performed best.
The relative improvement of the precision scores ranges from 10% to 80%.
The content-based approaches performed consistently better than HDIST.
8.3 Exploiting knowledge area similarity Table 4 presents the results.
Furthermore, is our way of bringing in contextual information useful?
This main set consists of 132 Dutch and 119 English topics.
The unit a person belongs to is used as a context for that person.
Table 5 reports on the results.
Table 6 reports on the results.
For which tasks?
In our setting the set of languages consists of English and Dutch: L = {UK, NL}.
wise mutual information (PMI), log-likelihood (LL), and distance within topic hierarchy (HDIST).
