First, it is interesting to notice that the incorporation of domain models can generally improve retrieval effectiveness in all the cases.
However, when both models are combined, there are additional improvements over the Feedback model, and these improvements are statistically significant in 2 cases out of 3.
In this series of tests, each of the queries 51-150 is used in turn as the test query while the other queries and their relevant documents (C1) or top-ranked retrieved documents (C2) are used to create domain models.
Second, we observe that the two methods to create domain models perform equally well (Tab.
Compared to the other results, we see consistent, although small in some cases, improvements over all the partial models.
We have tested both strategies to create domain models, but the differences between them are very small.
7.4 Domain Models In this section, we test several strategies to create and use domain models, by exploiting the domain information of the query set in various ways.
• We described two ways to gather documents for a domain: either using documents judged relevant to queries in the domain or using documents retrieved for these queries.
Queries 1-50 are used for training and 51-150 for testing.
This can be explained by the fact that the domains constructed with top-ranked documents tend to be more uniform than relevant documents with respect to term distribution, as the top retrieved documents usually have stronger statistical correspondence with the queries than the relevant documents.
In all the experiments, document models are created using Jelinek-Mercer smoothing.
Third, without Feedback model, the sub-domain models constructed with relevant documents perform much better than the whole domain models (Tab.
This also demonstrates the feasibility to use domain models for queries when no domain information is provided.
However, if domain models are constructed with top-ranked documents (Tab.
Below is an example of topic: <num> Number: 103 <dom> Domain: Law and Government <title> Topic: Welfare Reform We only use topic titles in all our tests.
++ and + mean significant changes in t-test with respect to the baseline without feedback, at the level of p<0.01 and p<0.05, respectively.
U2 - The domain model is determined by the system.
In addition, topics in the same domain can vary greatly, in particular in large domains such as science and technology, international politics, etc.
Measure Without FB With FB Without FB With FB AvgP 0.1884 (+20.00%)++ 0.2432 (+3.75%)** 0.2164 (+37.83%)++ 0.2463 (+5.08%)** Recall /48 355 17 430 20 020 18 944 20 260 Disks1-3 P@10 0.4640 0.5160 0.5050 0.5120 AvgP 0.1823 (+10.08%)++ 0.2350 (+8.00%)* 0.2157 (+30.25%)++ 0.2401 (+10.34%)** Recall /4 674 2 329 2 933 2 709 2 985 TREC7 P@10 0.3780 0.3760 0.3900 0.3900 AvgP 0.2519 (+5.53%) 0.2926 (+0.58%) 0.2724 (+14.12%)++ 0.3007 (+3.37%) Recall /4 728 2 829 3 279 3 090 3 338 TREC8 P@10 0.4360 0.4940 0.4720 0.5000 (The column WithoutFB is compared to the baseline model without feedback, while WithFB is compared to the baseline with feedback.
The following example can further confirm this observation, where we show the strongest expansion terms suggested by both types of relation for the query #384 space station moon: Co-occurrence Relations: year 0.016552 power 0.013226 time 0.010925 1 0.009422 develop 0.008932 offic 0.008485 oper 0.008408 2 0.007875 earth 0.007843 work 0.007801 radio 0.007701 system 0.007627 build 0.007451 000 0.007403 includ 0.007377 state 0.007076 program 0.007062 nation 0.006937 open 0.006889 servic 0.006809 air 0.006734 space 0.006685 nuclear 0.006521 full 0.006425 make 0.006410 compani 0.006262 peopl 0.006244 project 0.006147 unit 0.006114 gener 0.006036 dai 0.006029 Context-Dependent Relations: space 0.053913 mar 0.046589 earth 0.041786 man 0.037770 program 0.033077 project 0.026901 base 0.025213 orbit 0.025190 build 0.025042 mission 0.023974 call 0.022573 explor 0.021601 launch 0.019574 develop 0.019153 shuttl 0.016966 plan 0.016641 flight 0.016169 station 0.016045 intern 0.016002 energi 0.015556 oper 0.014536 power 0.014224 transport 0.012944 construct 0.012160 nasa 0.011985 nation 0.011855 perman 0.011521 japan 0.011433 apollo 0.010997 lunar 0.010898 In comparison with the baseline model with feedback (Tab.
Query Training Disk 2 0.86 350,085 231,219 1-50 Disks 1-3 Disks 1-3 3.10 785,932 1,078,166 51-150 TREC7 Disks 4-5 1.85 630,383 528,155 351-400 TREC8 Disks 4-5 1.85 630,383 528,155 401-450 Co-occurrence model in Table 4.
Measure Without FB With FB Without FB With FB AvgP 0.1718 (+9.43%)++ 0.2456 (+4.78%)** 0.1799 (+14.59%)++ 0.2452 (+4.61%)** Recall /48 355 16 558 20 131 17 341 20 155 Disks1-3 (U1) P@10 0.4300 0.5140 0.4220 0.5110 AvgP 0.1765 (+6.58%)++ 0.2395 (+10.06%)** 0.1785 (+7.79%)++ 0.2393 (+9.97%)** Recall /4 674 2 319 2 969 2 254 2 968 TREC7 (U2) P@10 0.3780 0.3820 0.3820 0.3820 AvgP 0.2434 (+1.97%) 0.2949 (+1.38%) 0.2441 (+2.26%) 0.2961 (+1.79%) Recall /4 728 2 772 3 318 2 734 3 311 TREC8 (U2) P@10 0.4380 0.4960 0.4280 0.5020 We also compare the domain models created with all the  indomain documents (Domain) and with only the top-10 retrieved documents in the domain with the query (Sub-Domain).
Measure Without FB With FB Without FB With FB AvgP 0.1650 (+5.10%)++ 0.2444 (+4.27%)** 0.1670 (+6.37%)++ 0.2449 (+4.48%)** Recall 16 343 20 061 16 414 20 090 Disks 1-3 (U2) P@10 0.4270 0.5100 0.4090 0.5140 Table 8.
We call it 0 5 10 15 20 25 30 35 Environm entFinance Int.Econom ics Int.Finance Int.Politics Int.R elations Law &G ov.
Measure Without FB With FB AvgP 0.1570 0.2344 (+49.30%) Recall /48 355 15 711 19 513Disks 1-3 P@10 0.4050 0.5010 AvgP 0.1656 0.2176 (+31.40%) Recall /4 674 2 237 2 777TREC7 P@10 0.3420 0.3860 AvgP 0.2387 0.2909 (+21.87%) Recall /4 728 2 764 3 237TREC8 P@10 0.4340 0.4860 Table 4.
This demonstrates that the impacts produced by feedback and term relations are different and complementary.
Strategies for creating domain models: C1 - With the relevant documents for the in-domain queries: this strategy simulates the case where we have an existing directory in which documents relevant to the domain are included.
For this series of tests, each collection is used in turn as training data while the other is used for testing.
All the improvements over  cooccurrence model are statistically significant (this is not shown in the table).
On domain models, we examine several questions: • When query domain is specified manually, is it useful to incorporate the domain model?
We can observe that the effectiveness is only slightly lower than those produced with manual identification of query domain (Tab.
The large differences between the two types of relation clearly show that context-dependent relations are more appropriate for query expansion.
An analysis immediately shows the reason: a domain model (in the way we created) only captures term distribution in the domain.
Therefore, in some large domains, characteristic terms have variable effects on queries.
This latter selects expansion terms with strongest global relation to the query.
7.3 Knowledge Models This model is combined with both baseline models (with or without feedback).
This shows that automatic domain identification is a way to select domain model as effective as manual identification.
This method is equivalent to [24].
Measure Without FB With FB Without FB With FB AvgP 0.1700 (+8.28%)++ 0.2454 (+4.69%)** 0.1918 (+22.17%)++ 0.2461 (+4.99%)** Recall /48 355 16 517 20 141 17 872 20 212 Disks1-3 (U1) P@10 0.4370 0.5130 0.4490 0.5150 AvgP 0.1715 (+3.56%)++ 0.2389 (+9.79%)* 0.1842 (+11.23%)++ 0.2408 (+10.66%)** Recall /4 674 2 270 2 965 2 428 2 987 TREC7 (U2) P@10 0.3720 0.3740 0.3880 0.3760 AvgP 0.2442 (+2.
(U2) AvgP 0.2501 (+6.70%) ** 0.2489 (+6.19%) ** Recall /48 355 20 514 20 367 Disks 1-3 P@10 0.5200 0.5230 AvgP 0.2462 (+13.14%) ** Recall /4 674 3 014TREC7 P@10 N/A 0.3960 AvgP 0.3029 (+4.13%) ** Recall /4 728 3 321TREC8 P@10 N/A 0.5020 
This is also the single factor which produced the highest improvements over the original query model.
We also compare the context-dependent knowledge model with the traditional context-independent term relations (defined between two single terms), which are used to expand queries.
The choice of this test collection is due to the availability of manually specified domain for each query.
1), this observation is not surprising: for many domains, we only have few training queries, thus few  indomain documents to create domain models.
Table 3 shows the retrieval effectiveness on all the collections.
On the other hand, as we only use term distribution, even if the top documents retrieved for the in-domain queries are irrelevant, they can still contain domain characteristic terms similarly to relevant documents.
7.2 Baseline Methods Two baseline models are used: the classical unigram model without any expansion, and the model with Feedback.
Here, we examine the possibility to automatically identify query domains.
For these queries, knowledge model is not applicable.
This model integrates all the components described in this paper: Original query model, Feedback model, Domain model and Knowledge model.
On the other hand, sub-domain models capture similar characteristics to Feedback model.
This choice is made according to the observation in [36] that the method performs very well for long queries.
In these tests, we use manual identification of query domain for Disks 1-3 (U1), but automatic identification for TREC7 and 8 (U2).
with rel.
On Knowledge model, in addition to testing its effectiveness, we also want to compare the context-dependent relations with context-independent ones.
On one hand, this confirms our earlier hypothesis that a domain may be too large to be able to suggest relevant terms for new queries in the domain.
13 domains are defined in these queries and their distributions among the two sets of queries are shown in Fig.
C2 - With the top-100 documents retrieved with the in-domain queries: this strategy simulates the case where the user specifies a domain for his queries without judging document relevance, and the system gathers related documents from his search history.
So we only report the results with the relevant documents.
Strategies for using domain models: U1 - The domain model is determined by the user manually.
It is also similar to the translation model [3].
Relevant documents for all in-domain queries vary greatly.
As we can see, simple co-occurrence relations can produce relatively strong improvements; but context-dependent relations can produce much stronger improvements in all cases, especially when feedback is not used.
In our case, as queries are expanded, they perform similarly to long queries.
For example, even when a query in International relations is classified in International politics, the latter domain can still suggest useful terms to the query.
Looking at the mixture weights, which may reflect the importance of each model, we observed that the best settings in all the collections vary in the following ranges: 0.1≤α0 ≤0.2, 0.1≤αDom ≤0.2, 0.1≤αK ≤0.2 and 0.5≤αF ≤0.6.
The same method is used on queries 1-50 to tune the parameters.
7.1 Setting The main test data are those from TREC 1-3 ad-hoc and filtering tracks, including queries 1-150, and documents on Disks 1-3.
Looking at the distribution of the domains (Fig.
Thus both strategies produce very similar effects.
All the improvements over the baseline model (with feedback) are statistically significant.
Looking at the accuracy of the automatic domain identification, however, it is surprisingly low: for queries 51-150, only 38% of the determined domains correspond to the manual identifications.
This allows us to compare with an approach using automatic domain identification.
We can see that the distribution varies strongly between domains and between the two query sets.
Domain models with relevant documents (C1) Domain Sub-Domain Coll.
This confirms the hypothesis we made, that by incorporating context information into relations, we can better determine the appropriate relations to apply and thus avoid introducing inappropriate expansion terms.
Our first observation is that the complete models produce the best results.
We see that the most important factor is Feedback model.
How effective is this method?
In other words, providing relevance judgments for queries does not add much advantage for the purpose of creating domain models.
This relation is measured by the sum of relations to each of the query terms.
Some statistics of the data are described in Tab.
However, the improvement scales are smaller than using Feedback and Relation models.
So when the latter is used, sub-domain models become superfluous.
Domain models with top-100 documents (C2) Domain Sub-Domain Coll.
Finally, we will see the impact of each component model when all the factors are combined.
Distribution of domains Table 2.
This result opens the door for a simpler method that does not require relevance judgments, for example using search history.
7.5 Complete Models The results with the complete model are shown in Table 8.
6), sub-domain models make much less differences.
3), we see that the improvements made by Knowledge model alone are slightly lower.
A detailed analysis reveals that the main reason is the closeness of several domains in TREC queries (e.g.
TREC collection statistics Collection Document Size (GB) Voc.
This observation seems to indicate that this model has the highest capability to capture the information need behind the query.
However, once Feedback model is used, the advantage disappears.
Dirichlet), especially for the main baseline method with Feedback model.
Some queries (4, 5 and 3 in the three query sets) only contain one word.
5 & 6, Domain models).
Knowledge models Co-occurrence Knowledge model Coll.
It indirectly validates our first hypothesis that a single user model or profile may be too large, so smaller domain models are preferred.
All the documents are preprocessed using Porter stemmer in Lemur and the standard stoplist is used.
** and * are similar but compared to the baseline model with feedback.)
Table 7 shows the results with this strategy using both strategies for domain model construction.
Baseline models Unigram Model Coll.
Measure Man.
In our preliminary tests, we also found this method performed better than the other methods (e.g.
7.4.2 Determining Query Domain Automatically It is not realistic to always ask users to specify a domain for their queries.
30%) 0.2957 (+1.65%) 0.2563 (+7.37%) 0.2967 (+1.99%) Recall /4 728 2 796 3 308 2 873 3 302 TREC8 (U2) P@10 0.4420 0.5000 0.4280 0.5020 Table 6.
The improvements on Disks 1-3 and TREC7 are statistically significant.
How do they compare?
Complete models (C1) All Doc.
This result confirms that the integration of contextual factors is effective.
However, in this situation, wrong domains assigned to queries are not always irrelevant and useless.
Therefore, the relatively low classification accuracy does not mean low usefulness of the domain models.
We have also tested on TREC 7 and 8 data.
7.4.1 Creating Domain models We test strategies C1 and C2.
Automatic query domain identification (U2) Dom.
However, even with lower weights, the other models do have strong impacts on the final effectiveness.
• If the query domain is not specified, can it be determined automatically?
T-test is also performed for statistical significance.
This demonstrates the benefit of integrating more contextual factors in IR.
Domain Coll.
with top-100 doc.
# of Doc.
This is much lower than the above 80% rates reported in [18].
This may seem surprising.
U S Econom ics U S Politics Query 1-50 Query 51-150 Figure 1.
Table 5.
Table 3.
Table 7.
6 vs. Tab.
International relations, International politics, Politics).
(C2) Coll.
(C1) Dom.
(U1) Auto.
M edical&Bio.M ilitaryPolitics Sci.&Tech.
