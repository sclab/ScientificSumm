Our  experiments also demonstrate the limits of the usefulness of our predictors when information from multiple runs is provided.
First, when making  predictions in the absence of retrievals from other systems, our predictors demonstrate robust, strong correlations with  average precision.
To our knowledge, this is the first large scale examination of zero-judgment, single-retrieval  performance prediction.
We have presented a new method for predicting the  performance of a retrieval ranking without any relevance  judgments.
First, our results could affect retrieval algorithm design.
We have demonstrated this improvement for many, diverse settings.
This performance, combined with a simple implementation, makes our predictors, in particular, very  attractive.
We believe that these two directions could be  rewarding given the theoretical and experimental evidence in this paper. 
Second, when provided retrievals from other systems, our extended methods demonstrate  competitive performance with state of the art baselines.
Second, our results could affect the design of minimal test collection algorithms.
Much of the recent work in ranking systems sometimes ignores correlations between document labels and scores.
Retrieval algorithms designed to consider spatial autocorrelation will conform to the cluster hypothesis and improve performance.
Our results suggest two conclusions.
We consider two cases.
