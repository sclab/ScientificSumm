It is also  different from ranking systems by the average performance on a set of queries.
Given a query, an information retrieval system produces a ranking of documents in the collection encoded as a set of scores associated with documents.
We refer to the set of scores for a particular query-system combination as a  retrieval.
Scores are often only computed for the top n documents from the collection.
We would like this ranking to approximate the ranking of retrievals by the evaluation measure.
This is different from ranking queries by the average performance on each query.
We place these scores in the length n vector, y, where yi refers to the score of the ith-ranked document.
We would like to predict the performance of this retrieval with respect to some evaluation measure (eg, mean average precision).
In this paper, we present results for ranking retrievals from arbitrary systems.
We use this method because of its simplicity and its success in previous work [15]. 
We adjust scores to have zero mean and unit variance.
