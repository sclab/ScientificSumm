In our tables, we will label results for Clarity using DV KL and the ranking robustness predictor using P. 5.2 Generalizability Experiments Our predictors do not require a particular baseline  retrieval system; the predictors can be computed for an  arbitrary retrieval, regardless of how scores were generated.
Since we are predicting the performance of retrievals which are not based on language modeling, we use a version of Clarity referred to as ranked-list Clarity [7].
Although previous work has presented the coefficient of determination (R2 ) to measure the quality of the regression, this measure cannot be reliably used when comparing slight improvements from combining predictors.
As stated in Section 2, we are interested in predicting the  performance of the retrieval generated by an arbitrary system.
Since we are  making a Gaussian assumption about our scores, we can sample scores for these unseen documents from the negative tail of the distribution.
As with ranked-list Clarity, we begin by  replacing all of the scores in y with their respective ranks.
5.1 Detailed Experiments In these experiments, we will predict the performance of language modeling scores using our autocorrelation  predictor, ρ(y, ˜y); we do not consider ρ(y, yµ) or ρ(˜y, yµ)  because, in these detailed experiments, we focus on ranking the retrievals from a single system.
With the uniform linear combination of these m retrievals  represented as yµ, we can compute the divergence as Dn KL(ˆy ˆyµ) where we use the superscript n to indicate that the  summation is over the set of n documents.
Our hypothesis is that documents with high topical similarity should have  correlated scores.
Our experiments focus on testing the predictive power of each of our predictors: ρ(y, ˜y), ρ(y, yµ), and ρ(˜y, yµ).
In these experiments, we use the following topic collections: TREC 4 ad-hoc, TREC 5 ad-hoc, Robust 2004, Terabyte 2004, and Terabyte 2005.
Our estimation of P(Q|θi) from the ranks, then is, P(Q|θi) = ( 2(c+1−yi) c(c+1) if yi ≤ c 0 otherwise (7) where c is a cutoff parameter.
Our methodology is consistent with previous research in that we predict the relative performance of a retrieval by  comparing a ranking based on our predictor to a ranking based on average precision.
When information from multiple runs on the same query is available, we use Aslam and Pavlu"s document-space  multinomial divergence as a baseline [1].
We found that this substantially improved fits for all predictors, including the baselines.
This baseline was developed in the context of predicting query difficulty but we adopt it as a reasonable baseline for predicting retrieval performance.
The value of the predictor is the Spearman rank correlation between the original  ranking and the corrupted ranking.
Then, we adjust the elements of y in the following way, ˆyi = 1 2n 0 @1 + nX k=yi 1 k 1 A (8) In our multirun experiments, we only use the top 75  documents from each retrieval (n = 75); this is within the range of parameter values suggested by the authors.
The  adjusted R2 allows us to evaluate the improvement in  prediction achieved by adding a parameter but loses the statistical interpretation of R2 .
Therefore, we avoided collections where scores were unlikely to be correlated (eg, question-answering) or were likely to be negatively correlated (eg, novelty).
5.3 Evaluation Given a set of retrievals, potentially from a combination of queries and systems, we measure the correlation of the rank ordering of this set by the predictor and by the  performance metric.
5.2.3 Parameter Settings When given multiple retrievals, we use documents in the union of the top k = 75 documents from each of the m  retrievals for that query.
The first set of experiments presents detailed comparisons of our  predictors to previously-proposed predictors using identical data sets.
5.1.1 Topics and Collections These performance prediction experiments use language model retrievals performed for queries associated with  collections in the TREC corpora.
This predictor corrupts the top k documents from retrieval and re-computes the language model scores for these corrupted documents.
We use the ad-hoc tracks from TREC3-8, TREC Robust 2003-2005, TREC Terabyte  20042005, TREC4-5 Spanish, TREC5-6 Chinese, and TREC  Enterprise Expert Search 2005.
In order to ensure comparability with  previous results, we present Kendall"s τ correlation between the predictor"s ranking and ranking based on average precision of the retrieval.
Therefore, in a second set of experiments, we demonstrate the ability of our techniques to generalize to a variety of collections, topics, and retrieval systems.
Specifically, we sample from the part of the distribution lower than the minimum value of in the normalized retrieval.
Our second baseline is Zhou and Croft"s ranking robustness predictor.
This rank-based method first normalizes the scores in a retrieval as an n-dimensional multinomial.
Nevertheless, our collections include corpora where correlations are weakly justified (eg, non-English corpora) or not  justified at all (eg, expert search).
We use retrievals, values for baseline predictors, and evaluation measures reported in previous work [19].
Using TREC collections  allows us to confidently associate an average precision with a retrieval.
Our first baseline is the  classic Clarity predictor presented in Equation 6.
We will refer to this predictor as DV KL, superscripted by V to indicate that the Kullback-Leibler divergence is with respect to the term embedding space.
Our second set of experiments demonstrates the  generalizability of our approach to arbitrary retrieval methods, corpus types, and corpus languages.
We will use Kendall"s τ to evaluate the magnitude of the correlation and the adjusted R2 to  evaluate the combination of variables. 
We present results for two sets of experiments.
Therefore, we adopt the adjusted coefficient of  determination which penalizes models with more variables.
5.2.2 Baselines In our detailed experiments, we used the Clarity measure as a baseline.
In some cases, a system did not score a document in the union.
This conversion begins by replacing all of the scores in y with the respective ranks.
We considered linear combinations of pairs of predictors (labeled by the components) and all predictors (labeled as β).
We cast a wide net in order to locate  collections where our predictors might fail.
The predictor is the divergence between the  candidate distribution, y, and the mean distribution, yµ .
However, we admit not tuning this parameter for either our system or the baseline.
We use Equation 6 to estimate P(w|θQ) and DV KL(θQ θC ) to compute the value of the predictor.
5.1.2 Baselines We provide two baselines.
If the size of this union is ˜n, then yµ and each yi is of length ˜n.
Ranked-list clarity converts document ranks to P(Q|θi) values.
This introduces randomness into our algorithm but we believe it is more appropriate than  assigning an arbitrary fixed value.
For entities, entries in W are proportional to the number of documents in which two entities cooccur.
We optimized the linear regression using the square root of each predictor.
We believe that that is one of the most attractive aspects of our algorithm.
Clarity is designed to be used with language modeling systems.
For all English and Spanish corpora, we construct the  matrix W according to the process described in Section 3.1.
5.2.1 Topics and Collections We gathered a diverse set of collections from all possible TREC corpora.
Predictors can sometimes perform better when linearly combined [9, 11].
In all cases, we use only the automatic runs for ad-hoc tracks submitted to NIST.
For Chinese corpora, we use na¨ıve character-based tf.idf vectors.
As suggested by the authors, we fix the algorithm parameters c and λ2 so that c = 60 and λ2 = 0.10.
Unless explicitly noted, all correlations are significant with p < 0.05.
