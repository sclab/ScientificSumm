On Reuters" data set, because there are too many relevant documents for each topic in the corpus, we used a random sample of 10% of each topic for training, and 10% of the remaining documents for testing.
On Reuters, the  number of rating for a simulated user is the number of documents relevant to the corresponding topic.
5.1 Evaluation Data Set To evaluate the proposed technique, we used the following three major data sets (Table 1): MovieLens Data: This data set was created by combining the relevance judgments from the MovieLens[9] data set with documents from the Internet Movie Database (IMDB).
To answer the third question, we tested the efficiency of the new algorithm on the entire Netflix data set where about half a million user models need to be learned together.
The first level of the tree  contains four topics, denoted as C, E, M, and G. For the experiments in this paper, the tree was cut at level 1 to create four smaller trees, each of which corresponds to one smaller data set: Reuters-E Reuters-C,  ReutersM and Reuters-G. For each small data set, we created several profiles, one profile for each node in a sub-tree, to simulate multiple users, each with a related, yet separate definition of relevance.
The average score for a document was 3.58.
Data Users Docs Ratings per User MovieLens 6,040 3,057 151 Netflix-all 480,189 17,770 208 Netflix-1000 1000 17,770 127 Reuters-C 34 100,000 3949 Reuters-E 26 100,000 1632 Reuters-G 33 100,000 2222 Reuters-M 10 100,000 6529 Netflix Data: This data set was constructed by combining documents about movies crawled from the web with a set of actual movie rental customer relevance  judgments from Netflix[19].
Do we need to take the effort to use a Bayesian  approach and learn a prior from other users?
This likeability rating was used as a  measurement of how relevant the document representing the corresponding movie is to the user.
For the experiments on the MovieLens and Netflix data sets, we used a random sample of 90% of each user for  training, and the rest for testing.
The average score for documents is 3.55.
To answer the second question, we  compared the proposed new algorithm with the standard EM algorithm to see whether the new learning algorithm is  better.
We first evaluated the performance on each individual user, and then estimated the macro average over all users.
Based on this database, we created one  document per movie that contained the relevant  information about it (e.g.
Only the first 100,000 news were used in our experiments.
All the user profiles on a sub-tree are supposed to share the same prior model distribution.
We considered documents with likeability scores of 4 or 5 as  relevant, and documents with a score of 1 to 3 as  irrelevant to the user.
Each document is assigned to one of several locations on the hierarchical tree.
For the MovieLens and Netflix data sets, algorithm  effectiveness was measured by mean square error, while on the Reuters data set classification error was used because it was more informative.
To answer the first question, we compared the Bayesian  hierarchical models with commonly used Norm-2 regularized linear regression models.
Documents  representing each movie were constructed from the portion of the IMDB database that is available for public  download[13].
This number was reduced to 1000 customers through random sampling.
Similar to MovieLens, we considered documents with likeability scores of 4 or 5 as relevant.
In fact, the commonly used  approach is equivalent to the model learned at the end of the first EM iteration.
Since this corpus explicitly  indicates only the relevant documents for a topic(user), all other documents are considered irrelevant.
Can the new algorithm quickly learn many user  models?
The average customer on the  reduced data set provided 127 judgments, with 70 being deemed relevant.
Statistical tests (t-tests) were carried out to see whether the results are significant.
The Reuters corpus comes with a topic hierarchy.
MovieLens provided relevance judgments on 3,057 documents from 6,040 separate users.
Does the new algorithm work better than the standard EM algorithm for learning the Bayesian hierarchical linear model?
Reuters Data: This is the Reuters Corpus, Volume 1.
On average, each user rated 151 movies, of these 87 were judged to be relevant.
Table 1: Data Set Statistics.
5.2 Evaluation We designed the experiments to answer the following three questions: 1.
MovieLens allows users to rank how much he/she enjoyed a specific movie on a scale from 1 to 5.
There are around 100 million rating on a scale of 1 to 5 for 17,770 documents.
Netflix publicly provides the relevance judgments of 480,189 anonymous customers.
It covers 810,000 Reuters English language news stories from August 20, 1996 to August 19, 1997.
For all runs, we set (a, b, c, Î£ ) = (0.1, 10, 0.1, 1) manually. 
directors, actors, etc.).
