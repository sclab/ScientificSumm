The accuracy of the new  algorithm is similar to that of the standard EM algorithm at each iteration.
The results suggest that only on a corpus where the number of unrelated user-feature pairs is much larger than the number of related pairs, such as on the  Netflix data set, the proposed technique will get a significant improvement over standard EM.
Although the proposed technique is faster than standard Figure 2: Performance on a Netflix subset with 1,000 users.
Figure 2, Figure 3, and Figure 4 show that on all data sets, the Bayesian hierarchical modeling approach has a  statistical significant improvement over the regularized linear regression model, which is equivalent to the Bayesian  hierarchical models learned at the first iteration.
Figure 2 and Figure 3 show that the proposed new  algorithm works better than the standard EM algorithm on the Netflix and MovieLens data sets.
Further analysis shows a negative correlation between the number of training data for a user and the improvement the system gets.
Thus the two learning algorithms  perform similarly.
Norm-2 regularized linear models are equivalent to the Bayesian hierarchical models learned at the first iteration, and are statistical significantly worse than the Bayesian hierarchical models.
The new algorithm is statistical significantly better than EM algorithm at iterations 2 - 10.
1 6 11 16 21 0.5 1 1.5 2 2.5 3 3.5 Iterations MeanSquareError New Algorithm Traditional EM 1 6 11 16 21 0.35 0.4 0.45 0.5 0.55 0.6 0.65 Iterations ClassificationError New Algorithm Traditional EM Figure 4: Performance on a Reuters-E subset with 26 profiles.
This is not  surprising since the number of related feature-users pairs is much smaller than the number of unrelated feature-user pairs on these two data sets, and thus the proposed new algorithm is expected to work better.
Figure 4 shows that the two algorithms work similarly on the Reuters-E data set.
The new algorithm is statistical significantly better than EM algorithm at iteration 2 to 17 (evaluated with mean square error).
This suggests that the borrowing information from other users has more significant improvements for users with less  training data, which is as expected.
Performances on Reuters-C, Reuters-M, Reuters-G are similar.
Our results show that the modified EM algorithm converges quickly, and 2 - 3 modified EM iterations would result in a reliable estimation.
However, the strength of the correlation differs over data sets, and the amount of  training data is not the only characteristics that will influence the final performance.
0 2 4 6 8 10 1 1.05 1.1 1.15 1.2 1.25 1.3 1.35 1.4 Iterations MeanSquareError New Algorithm Traditional EM 1 2 3 4 5 6 7 8 9 10 0.33 0.34 0.35 0.36 0.37 0.38 0.39 Iterations ClassificationError New Algorithm Traditional EM Figure 3: Performance on a MovieLens subset with 1,000 users.
We evaluated the algorithm on the whole Netflix data set (480,189 users, 159,836 features, and 100 million ratings) running on a single CPU PC (2GB  memory, P4 3GHz).
The general patterns are very similar on other Reuters" subsets.
However, the experiments also show that when the assumption does not hold, the new algorithm does not hurt performance.
Further analysis shows that only 58% of the user-feature pairs are unrelated on this data set.
Since the number of unrelated user-feature pairs is not  extremely large, the sparseness is not a serious problem on the Reuters data set.
This demonstrates that the proposed technique can efficiently handle large-scale system like  Netflix. 
1 2 3 4 5 0.011 0.0115 0.012 0.0125 0.013 0.0135 0.014 Iterations MeanSquareError New Algorithm Traditional EM 1 2 3 4 5 0.0102 0.0104 0.0106 0.0108 0.011 0.0112 0.0114 Iterations ClassificationError New Algorithm Traditional EM EM, can it really learn millions of user models quickly?
The system finished one modified EM  iteration in about 4 hours.
