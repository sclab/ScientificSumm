Although, the results here are significantly above-random, it is unclear what level of performance is necessary for sentence detection to be useful in and of itself and not simply as a means to document ranking and classification.
The primary hypothesis we are concerned with is that n-grams are critical for this task; if this is true, we expect to see a significant gap in performance between the document-level classifiers that use n-grams (denoted Document Ngram) and those using only unigram features (denoted Document Unigram).
For the sentence-level approach, all sentences in a  document are either entirely in the training set or entirely in the test set for each fold.
In order to  demonstrate the impact this task can have on e-mail users, we conducted a user study using an earlier less-accurate version of the sentence classifier - where instead of using just a single sentence, a  threesentence windowed-approach was used.
Thus, as in other text problems, the accuracy numbers are  deceptively high sheerly because of the default accuracy attainable by always predicting no.
As a result, good classifiers cannot rely on aggregating evidence from a large number of weak indicators across the entire document.
We are currently investigating whether the sentence-level classifiers do perform  better over different test corpora without retraining. 
Additionally, action-item detection is distinct from many text classification tasks in that a single sentence can change the class label of the document.
We perform this comparison over the hand-stripped corpus since that eliminates spurious no judgments that would come from  including quoted material, etc.
Additionally, the large separation at the top right of the curves corresponds to the area where the optimal F1 occurs for each classifier, agreeing with the large improvement from 0.6904 to 0.7682 in F1 score.
Even though we discarded the header information, examining the top-ranked features at the document-level reveals that many of the features are names or parts of e-mail addresses that occurred in the body and are highly associated with e-mails that tend to  contain many or no action-items.
This is because the window of text is so small that the unigram  representation, when done at the sentence-level, implicitly picks up on the power of the n-grams.
We note that these features will be  sensitive to the particular distribution (senders/receivers) and thus the document-level approach may produce classifiers that transfer less readily to alternate contexts and users at different institutions.
The first would be an action-item in most contexts while the  second would not.
In order to perform fair comparisons between  conditions, the overall number of tokens in each message set should be approximately equal; that is, the cognitive reading load should be approximately the same before the classifier"s reordering.
This would mean that a tenth of the user"s action-items would be placed at the top of their action-item sorted inbox.
There were three distinct sets of e-mail in which users had to find action-items.
Na¨ıve Bayes poor performance with the n-gram representation is not surprising since the bag-of-n-grams causes excessive  doublecounting as mentioned in Section 3.2.1; however, na¨ıve Bayes is not hurt at the sentence-level because the sparse examples provide few chances for agglomerative effects of double counting.
When the result is statistically significant, it is shown in bold.
Accuracy F1 Unigram Ngram Unigram Ngram kNN 0.9519 0.9536 0.6540 0.6686 na¨ıve Bayes 0.9419 0.9550 0.6176 0.6676 SVM 0.9559 0.9579 0.6271 0.6672 Voted Perceptron 0.8895 0.9247 0.3744 0.5164 Table 6: Performance of the Sentence-Level Classifiers at  Sentence Detection Although Cohen et al.
Computing the sentence-level agreement  using the reconciled gold standard judgments with each of the  annotators" individual judgments gives a kappa of 0.89 for Annotator One and a kappa of 0.92 for Annotator Two.
As would be expected for e-mail, it is a long-tailed distribution with about half the  messages having more than 60 tokens in the body (this paragraph has 65 tokens).
F1 Winner Accuracy Winner kNN Sentence Sentence na¨ıve Bayes Sentence Sentence SVM Sentence Sentence Voted Perceptron Sentence Document Table 5: Significance results for sentence-level classifiers vs. document-level classifiers for the document detection problem.
These sets were either presented in a random order (Unordered), ordered by the classifier (Ordered), or ordered by the classifier and with the 0.4 0.5 0.6 0.7 0.8 0.9 1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Precision Recall Action-Item Detection SVM Performance (Post Model Selection) Document Unigram Sentence Ngram Figure 3: Both n-grams and a small prediction window lead to consistent improvements over the standard approach.
For example, If you would like to keep your job, come to tomorrow"s meeting implies a required action where If you would like to join Annotator 1 No Yes Annotator 2 No 5810 65 Yes 74 352 Table 2: Agreement of Human Annotators at Sentence Level the football betting pool, come to tomorrow"s meeting does not.
Of the 328 e-mails containing action-items, 259 messages have one action-item segment; 55 messages have two action-item segments; 11 messages have three action-item segments.
Classifiers Document Unigram Document Ngram Sentence Unigram Sentence Ngram F1 kNN 0.6670 ± 0.0288 0.7108 ± 0.0699 0.7615 ± 0.0504 0.7790 ± 0.0460 na¨ıve Bayes 0.6572 ± 0.0749 0.6484 ± 0.0513 0.7715 ± 0.0597 0.7777 ± 0.0426 SVM 0.6904 ± 0.0347 0.7428 ± 0.0422 0.7282 ± 0.0698 0.7682 ± 0.0451 Voted Perceptron 0.6288 ± 0.0395 0.6774 ± 0.0422 0.6511 ± 0.0506 0.6798 ± 0.0913 Accuracy kNN 0.7029 ± 0.0659 0.7486 ± 0.0505 0.7972 ± 0.0435 0.8092 ± 0.0352 na¨ıve Bayes 0.6074 ± 0.0651 0.5816 ± 0.1075 0.7863 ± 0.0553 0.8145 ± 0.0268 SVM 0.7595 ± 0.0309 0.7904 ± 0.0349 0.7958 ± 0.0551 0.8173 ± 0.0258 Voted Perceptron 0.6531 ± 0.0390 0.7164 ± 0.0376 0.6413 ± 0.0833 0.7082 ± 0.1032 Table 3: Average Document-Detection Performance during Cross-Validation for Each Method and the Sample Standard Deviation (Sn−1) in italics.
Each of the following number of features was tried: 10, 25, 50, 100, 250, 750, 1000, 2000, 4000.
Finally, when considering a new type of classification task, one of the most basic questions is whether an accurate classifier built for the task can have an impact on the end-user.
This points out that part of the problem of going beyond bag-of-words may be the methodology, and investigating such properties as  learning curves and how well a model transfers may highlight  differences in models which appear to have similar performance when tested on the distributions they were trained on.
In order to choose the number of features to use for each classifier, we perform nested cross-validation and choose the settings that yield the optimal  document-level F1 for that classifier.
Because their tasks are highly similar to ours, we employ both classifiers to ensure we are not overlooking a  competitive alternative classifier to the SVM for the basic bag-of-words representation.
That is, for the n-gram representation, the n-grams and position features are also subject to removal by the feature selection method.
Therefore, the document-level unigram  approach cannot gain much leverage, even when modeling their joint probability correctly, since these words will often co-occur in the document but not necessarily in a phrase.
Different levels of feature selection were considered for each classifier.
Both annotators were free to label the subject as an action-item, but since neither did, we omit the subject line of the message as well.
In the high precision area of the graph, the consistent edge of the sentence-level classifier is rather impressive - continuing at precision 1 out to 0.1 recall.
Therefore, the finer-grained sentence-level judgments allows a unigram representation to  succeed but only when performed in a small window - behaving as an n-gram representation for all practical purposes.
The annotators are said to agree at the document-level when both marked the same document as containing no action-items or both marked at least one action-item regardless of whether the text segments were the same.
Since the user studies used a three sentence window, we believe this played a role as well as sentence detection accuracy.
At the sentence-level, the annotators agreed 98% of the time, and the kappa statistic for inter-annotator agreement is 0.82.
For example, highlighting the wrong sentence near an actual action-item hurts the user"s trust, but if a vague indicator (e.g., an arrow) points to the approximate area the user is not aware of the near-miss.
In order to produce one single set of judgments, the human  annotators went through each annotation where there was  disagreement and came to a consensus opinion.
4.5 Results The results for document-level classification are given in Table 3.
4.1.1 Data Labeling Two human annotators labeled each message as to whether or not it contained an action-item.
Document Winner Sentence Winner kNN Ngram Ngram na¨ıve Bayes Unigram Ngram SVM Ngram† Ngram Voted Perceptron Ngram† Ngram Table 4: Significance results for n-grams versus unigrams for document detection using document-level and sentence-level classifiers.
This allows us to balance the  cognitive load in terms of number of tokens that must be read in the user-studies we report - including quoted material would  complicate the user studies since some users might skip the material while others read it.
0 20 40 60 80 100 120 140 160 0 200 400 600 800 1000 1200 1400 NumberofMessages Number of Tokens All Messages Action-Item Messages 0 0.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 0.18 0.2 0 200 400 600 800 1000 1200 1400 PercentageofMessages Number of Tokens All Messages Action-Item Messages Figure 2: The Histogram (left) and Distribution (right) of Message Length.
As mentioned above, this might be a result of the large room for improvement that still exists for sentence detection, but anecdotal evidence suggests this might also be a result of how the information is presented to the user rather than the accuracy of sentence detection.
This further strengthens the evidence that an alternate classifier with the bag-of-words  representation could not reach the same level of performance.
This difference in performance produced by the n-gram representation is statistically significant for each classifier except for na¨ıve Bayes and the accuracy metric for kNN (see Table 4).
The Voted Perceptron classifier does improve when the number of training  iterations are increased, but it is still lower than the SVM classifier.
In order to determine the sentence-level agreement, we use each judgment to create a sentence-corpus with labels as described in Section 3.2.2, then consider the agreement over these sentences.
Further improvement would  signify that the order of the words matter even when only  considering a small sentence-size window.
4.6 Discussion In contrast to problems where n-grams have yielded little  difference, we believe their power here stems from the fact that many of the meaningful n-grams for action-items consist of common words, e.g., let me know.
A segment is a contiguous section of text selected by the human annotators and may span several sentences or a complete phrase contained in a sentence.
The best performance for each classifier is shown in bold.
When used in an offline-manner, multiple passes can be made through the training data.
prevents tainting the cross-validation since otherwise a test item could occur as quoted material in a training document.
In addition, they identified each segment of the e-mail which contained an action-item.
To isolate the effects of quoted material, we have three  versions of the corpora.
With regard to the sentence detection problem, we note that the F1 measure gives a better feel for the remaining room for improvement in this difficult problem.
[5] obtain worse results using an SVM than a Voted Perceptron with one training iteration, they conclude that the best solution for detecting speech acts may not lie in an area with a large margin.
A value close to −1 implies the annotators agree far less often than would be expected randomly, while a value close to 1 means they agree more often than randomly expected.
The annotators did not collect statistics during this process but anecdotally reported that the majority of disagreements were either cases of clear annotator oversight or different interpretations of conditional statements.
More  importantly for the n-gram hypothesis, the n-grams lead to the best document-level classifier performance as well.
Additionally, users typically show practice effects by improving at the overall task and thus performing better at later message sets.
As would be expected, the difference between the sentence-level n-gram representation and unigram representation is small.
Additionally, ensuring all quoted material is removed 1 We have an even more highly anonymized version of the  corpus that can be made available for some outside experimentation.
The F1 measure [18, 21] is the harmonic mean of precision and recall where Precision = Correct Positives Predicted Positives and Recall = Correct Positives Actual Positives .
For this study, only the body of each e-mail message was used.
Feature selection was performed using the chi-squared  statistic.
When the F1 result is statistically significant, it is shown in bold.
That is, unlike document detection where  actionitem documents are fairly common, action-item sentences are very rare.
By thoroughly sampling  alternative classifier choices we demonstrate that representation  improvements over bag-of-words are not due to using the information in the bag-of-words poorly.
With each new training example, a correct classification increases the weight on the current perceptron and an incorrect classification updates the perceptron.
Annotator 1 No Yes Annotator 2 No 391 26 Yes 29 298 Table 1: Agreement of Human Annotators at Document Level Annotator One labeled 324 messages as containing action items.
This is typically handled by varying the ordering of the sets across users so that the means are comparable.
Quoted material can act as noise when learning since it may include action-items from previous messages that are no longer  relevant.
At the document-level, the annotators agreed 93% of the time.
The studies reported here are performed with the hand-stripped version.
4.2 Classifiers For this experiment, we have selected a variety of standard text classification algorithms.
In practice, we have also found it to be a computational convenience that  frequently leads to comparable results with numerically optimizing k via a cross-validation procedure.
Further highlighting the improvement from finer-grained  judgments and n-grams, Figure 3 graphically depicts the edge the SVM sentence-level classifier has over the standard bag-of-words approach with a precision-recall curve.
Both the voted perceptron and the SVM give a solution from the same hypothesis space - in this case, a linear classifier.
We have done this in order to provide both a competitive and thorough sampling of learning methods for the task at hand.
4.2.1 kNN We employ a standard variant of the k-nearest neighbor  algorithm used in text classification, kNN with s-cut score  thresholding [19].
When the accuracy result is significant, it is shown with a † .
Considering the relative unexplored nature of classification at the sentence-level, this gives great hope for further increases in performance.
Feature selection is always applied to all candidate features.
[5] observed that the Voted Perceptron with a single training iteration outperformed SVM in a set of  similar tasks, we see no such behavior here.
The messages were anonymized by replacing the names of each individual and  institution with a pseudonym.1 After attempting to identify and eliminate duplicate e-mails, the corpus contains 744 e-mail messages.
For significance tests, we use a two-tailed t-test [21] to compare the values obtained during each cross-validation fold with a p-value of 0.05.
4.3 Performance Measures To compare the performance of the classification methods, we look at two standard performance measures, F1 and accuracy.
After reconciling the judgments there are 416 e-mails with no action-items and 328 e-mails containing  actionitems.
The output of the classifier uses the weights on the perceptra to make a final voted classification.
In terms of message characteristics, there were on average 132 content tokens in the body after stripping.
Although, the classifier consistently aids the users, we did not gain an additional end-user impact by highlighting.
This is important since it is easy to improve a strawman classifier by introducing a new representation.
They were instructed that an action item is an explicit request for information that requires the recipient"s attention or a required action and told to highlight the phrases or sentences that make up the request.
4.1 The Data Our corpus consists of e-mails obtained from volunteers at an educational institution and cover subjects such as: organizing a research workshop, arranging for job-candidate interviews,  publishing proceedings, and talk announcements.
In either case, when a language-modeling approach is desired, modeling the n-grams directly would be preferable to na¨ıve Bayes.
The voted perceptron is an online-learning method that keeps a history of past perceptrons used, as well as a weight signifying how often that perceptron was correct.
Figure 4 shows that at intervals of 5, 10, and 15 minutes, users consistently found significantly more action-items when assisted by the classifier, but were most critically aided in the first five  minutes.
A few examples are terms such as org, bob, and gov.
At the document-level, the kappa statistic for inter-annotator  agreement is 0.85.
We use a tfidf-weighting of the terms with a  distanceweighted vote of the neighbors to compute the score before  thresholding it.
Examining Table 3, we  observe that this is indeed the case for every classifier except na¨ıve Bayes.
Figure 4: Users find action-items quicker when assisted by a classification system.
4.4 Experimental Methodology We perform standard 10-fold cross-validation on the set of  documents.
Additionally, the hand-stripped version has had any xml content and e-mail signatures removed - leaving only the essential content of the message.
Two  messages have four action-item segments, and one message has six action-item segments.
In order to choose the value of s for thresholding, we perform leave-one-out cross-validation over the training set.
However, by examining Figure 2 we see the length distributions are nearly identical.
This value is both strong enough to expect the  problem to be learnable and is comparable with results for similar tasks [5, 6].
Sentence detection results are presented in Table 6.
Quoted material refers to the text of a previous e-mail that an author often leaves in an e-mail message when responding to the e-mail.
In  using this classifier, we smoothed word and class probabilities using a Bayesian estimate (with the word prior) and a Laplace m-estimate, respectively.
In selecting algorithms, we have chosen algorithms that are not only known to work well but which differ along such lines as discriminative vs. generative and lazy vs.  eager.
There are  approximately 4700 unigram tokens without feature selection.
This allows us to compare agreement over no judgments.
4.2.3 SVM We have used a linear SVM with a tfidf feature representation and L2-norm as implemented in the SVMlight package v6.01 [11].
The hand-stripped version contains the messages after quoted material has been removed by a human.
Furthermore, it is well-known that the Voted Perceptron increases the margin of the solution after each pass through the training data [10].
R is the empirical estimate of the probability of random agreement given the empirical class priors.
While omitting further detail, we note the sets were balanced for the total number of tokens and a latin square design was used to balance practice effects.
This only reduces the number of no agreements.
This rule for choosing k is theoretically motivated by results which show such a rule converges to the optimal  classifier as the number of training points increases [8].
After stripping, the majority of words left are usually actual message content.
The agreement of the human annotators is shown in Tables 1 and 2.
The auto-stripped version contains the messages after quoted  material has been automatically removed.
Please contact the authors for more information on obtaining this data.
A bin size of 20 words was used.
4.2.2 Na¨ıve Bayes We use a standard multinomial na¨ıve Bayes classifier [16].
Only tokens in the body after hand-stripping were counted.
We use the same feature representation and kernel as we have for the SVM, a linear kernel with tfidf-weighting and an L2-norm.
All default settings were used.
Annotator Two labeled 327 messages as containing action items.
4.2.4 Voted Perceptron Like the SVM, the Voted Perceptron is a kernel-based  learning method.
This leaves 6301 automatically segmented sentences.
The value of k is set to be 2( log2 N + 1) where N is the number of training points.
center sentence in the highest confidence window highlighted  (Order+help).
The kappa statistic [3, 5] is often used to evaluate inter-annotator agreement: κ = A − R 1 − R A is the empirical estimate of the probability of agreement.
Of course, many conditional statements are not so clearly interpretable.
For action-item  messages, there were 115.
The raw form contains the basic messages.
After identity anonymization, the corpora has three basic  versions.
Since Cohen et al.
