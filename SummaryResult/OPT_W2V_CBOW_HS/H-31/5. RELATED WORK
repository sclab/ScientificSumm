Poisson distribution has been previously studied in the document generation models [16, 22, 3, 24], leading to the development of one of the most effective retrieval formula BM25 [23].
We show that  Poisson language model is natural to accommodate the per-term smoothing without heuristic twist of the semantics of a  generative model, and is able to efficiently better model the mixture background, both analytically and empirically. 
[26] introduces a way to empirically search for an exponential model for the documents.
[24] studies the parallel derivation of three  different retrieval models which is related to our comparison of Poisson and multinomial.
The most popular and  fundamental one is the query-generation language model [21, 13].
To the best of our knowledge, there has been no study of query generation models based on Poisson distribution.
Poisson mixtures [3] such as 2-Poisson [22], Negative multinomial, and Katz"s  KMixture [9] has shown to be effective to model and retrieve documents.
Language model smoothing [2, 28, 29] and background structures [15, 10, 25, 27] have been studied with  multinomial language models.
All existing query generation language models are based on either multinomial distribution [19, 6, 28, 13] or  multivariate Bernoulli distribution [21, 17, 18].
Once again, none of this work explores Poisson distribution in the query generation framework.
[7] analytically shows that term specific smoothing could be useful.
Language models have been shown to be effective for many retrieval tasks [21, 28, 14, 4].
We introduce a new family of language models, based on Poisson  distribution.
However, the Poisson model in their paper is still under the document generation  framework, and also does not account for the document length variation.
