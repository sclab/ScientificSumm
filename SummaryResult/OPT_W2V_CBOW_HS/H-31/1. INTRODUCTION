We  derive several smoothing methods for Poisson model in parallel to those used for multinomial distributions, and compare the corresponding retrieval models with those based on  multinomial distributions.
In Section 2, we introduce the new family of query generation models with Poisson distribution, and present various smoothing  methods which lead to different retrieval functions.
Indeed, similar to the multinomial distribution, the Poisson distribution models term frequencies, but  without the constraint that all the term probabilities must sum to 1, and similar to multivariate Bernoulli, it models each term independently, thus can easily accommodate per-term smoothing.
Among many variants of language  models proposed, the most popular and fundamental one is the query-generation language model [21, 13], which leads to the query-likelihood scoring method for ranking documents.
To score a  document, we would first estimate a multivariate Poisson model based on the document, and then score it based on the  likelihood of the query given by the estimated Poisson model.
As in the existing work on multinomial language models, smoothing is critical for this new family of models.
We find that while with some  smoothing methods, the new model and the multinomial model lead to exactly the same formula, with some other  smoothing methods they diverge, and the Poisson model brings in more flexibility for smoothing.
However, multivariate Bernoulli also has one potential advantage over multinomial from the viewpoint of retrieval: in a multinomial distribution, the probabilities of all the terms must sum to 1, making it hard to accommodate per-term smoothing, while in a  multivariate Bernoulli, the presence probabilities of different terms are completely independent of each other, easily  accommodating per-term smoothing and weighting.
The heavy use of multinomial distribution is partly due to the fact that it has been successfully used in speech recognition, where multinomial distribution is a  natural choice for modeling the occurrence of a particular word in a particular position in text.
In Section 3, we analytically compare the Poisson language model with the multinomial language model, from the perspective of  retrieval.
In such a model, given a query q and a document d, we  compute the likelihood of generating query q with a model estimated based on document d, i.e., the conditional  probability p(q|d).
In particular, a key difference is that the Poisson model can naturally accommodate  perterm smoothing, which is hard to achieve with a multinomial model without heuristic twist of the semantics of a  generative model.
Compared with  multivariate Bernoulli, multinomial distribution has the advantage of being able to model the frequency of terms in the query; in contrast, multivariate Bernoulli only models the presence and absence of query terms, thus cannot capture different frequencies of query terms.
As a new type of probabilistic retrieval models, language models have been shown to be effective for many retrieval tasks [21, 28, 14, 4].
In some sense, the Poisson model combines the advantage of multinomial in modeling term frequency and the advantage of the multivariate Bernoulli in accommodating per-term smoothing.
In this new family of models, we model the frequency of each term independently with a Poisson distribution.
Virtually all the existing query generation language  models are based on either multinomial distribution [19, 6, 28] or multivariate Bernoulli distribution [21, 18].
Another potential advantage of the Poisson model is that its  corresponding background model for smoothing can be improved through using a mixture model that has a closed form  formula.
In this paper, we propose and study a new family of query generation models based on the Poisson distribution.
This new background model is shown to outperform the standard background model and reduce the sensitivity of retrieval performance to the smoothing parameter.
We exploit this potential advantage to develop a new term-dependent smoothing algorithm for Poisson model and show that this new smoothing algorithm can improve performance over term-independent smoothing algorithms using either Poisson or multinomial model.
Note that term absence is also indirectly captured in a multinomial model through the constraint that all the term probabilities must sum to 1.
This advantage is seen for both one-stage and two-stage smoothing.
We can then rank documents based on the likelihood of generating the query.
We then design empirical experiments to compare the two families of language models in Section 4.
The  multinomial distribution is especially popular and also shown to be quite effective.
The rest of the paper is organized as follows.
We discuss the related work in 5 and conclude in 6. 
