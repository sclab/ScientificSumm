The likelihood that a query q is generated from the document language model Λd can be written as p(q|d) = w∈V p(c(w, q)|Λd) (1) This representation is clearly different from the multinomial query generation model as (1) the likelihood includes all the terms in the vocabulary V , instead of only those appearing in q, and (2) instead of the appearance of terms, the event space of this model is the frequencies of each term.
2.2.1 Bayesian Smoothing using Gamma Prior Following the risk minimization framework in [11], we  assume that a document is generated by the arrival of terms in a time period of |d| according to the document language model, which essentially consists of a vector of Poisson rates for each term, i.e., Λd = λd,1, ..., λd,|V | .
One is the document language model estimated with  maximum likelihood estimator, and the other is a model  estimated from the collection background, p(·|C), which assigns non-zero rate to w. For example, we may use an interpolation coefficient  between 0 and 1 (i.e., δ ∈ [0, 1]).
Alternatively, we assume that a query is generated by sampling the frequency of words from a series of independent Poisson processes [20].
Any smoothing methods for the document language model can be used to estimate p(·|d) such as the Gamma smoothing as discussed in Section 2.2.1.
We use Bayesian estimation with the following Gamma prior, which has two parameters, α and β: Gamma(λ|α, β) = βα Γ(α) λα−1 e−βλ For each term w, the parameters αw and βw are chosen to be αw = µ ∗ λC,w and βw = µ, where µ is a parameter and λC,w is the rate of w estimated from some background language model, usually the collection language model.
Let w be a piece of text composed by an author and c(w1), ..., c(wn) be a frequency vector representing w, where c(wi, w) is the frequency count of term wi in text w. In retrieval, w could be either a query or a document.
The posterior distribution of Λd is given by p(Λd|d, C) ∝ w∈V e−λw(|d|+µ) λ c(w,d)+µλC,w−1 w which is a product of |V | Gamma distributions with  parameters c(w, d) + µλC,w and |d| + µ for each word w. Given that the Gamma mean is α β , we have ˆλd,w = λd,w λd,wp(λd,w|d, C)dλd,w = c(w, d) + µλC,w |d| + µ This is precisely the smoothed estimate of multinomial language model with Dirichlet prior [28].
In order to distinguish the content and non-discriminative words in a query, we follow [29] and assume that a query is generated by sampling from a two-component mixture of Poisson language models, with one component being the document model Λd and the other being a query background language model p(·|U).
As the second summation is difficult to compute efficiently, we conflate all non-query terms as one pseudo  non-queryterm, denoted as N. Using the pseudo-term formulation and a Poisson collection model, we can rewrite the retrieval formula as Score(d, q) ∝ w∈d∩q log(1 + 1 − δ δ e−λd,w (λd,w|q|)c(w,q) e−λd,C |q| (λd,C )c(w,q) ) + log (1 − δ)e−λd,N |q| + δe−λC,N |q| 1 − δ + δe−λC,N |q| (3) where λd,N = |d|− w∈q c(w,d) |d| and λC,N = |C|− w∈q c(w,C) |C| .
With this simple  interpolation, we can score a document with Score(d, q) = w∈V log((1 − δ)p(c(w, q)|d) + δp(c(w, q)|C)) (2) Using the maximum likelihood estimator for p(·|d), we have λd,w = c(w,d) |d| , thus Equation 2 becomes Score(d, q) ∝ w∈d∩q [log(1 + 1 − δ δ e−λd,w|q| (λd,w|q|)c(w,q) c(w, q)!
In Poisson  language models, however, we do not have the same constraint as in a multinomial model (i.e., w∈V p(w|d) = 1).
The  maximum likelihood estimate (MLE) of λi is ˆλi = d∈D c(wi, d) d∈D w ∈V c(w , d) Note that this MLE is different from the MLE for the  Poisson distribution without considering the document lengths, which appears in [22, 24].
This looks similar to the interpolation  smoothing, except that p(·|Λd) now should be a smoothed language model, instead of the one estimated with MLE.
We may then score each document with the query likelihood computed using the  following two-stage smoothing model: p(c(w, q)|Λd, U) = (1 − δ)p(c(w, q)|Λd) + δp(c(w, q)|U) (4) where δ is a parameter, roughly indicating the amount of noise in q.
With n such independent Poisson processes, each  explaining the generation of one term in the vocabulary, the  likelihood of w to be generated from such Poisson processes can be written as p(w|Λ) = n i=1 p(c(wi, w)|Λ) = n i=1 e−λi·|w| (λi · |w|)c(wi,w) c(wi, w)!
Given a document d, we may estimate a Poisson language model Λd using d as a sample.
2.2 Smoothing in Poisson Retrieval Model In general, we want to assign non-zero rates for the query terms that are not seen in document d. Many smoothing methods have been proposed for multinomial language  models[2, 28, 29].
However, if a query term is unseen in the document, the MLE of the Poisson distribution would assign zero  probability to the term, causing the probability of the query to be zero.
· p(c(w, q)|C) ) − log (1 − δ)e−λd,w|q| + δp(c(w, q) = 0|C) 1 − δ + δp(c(w, q) = 0|C) ] + w∈d log (1 − δ)e−λd,w|q| + δp(c(w, q) = 0|C) 1 − δ + δp(c(w, q) = 0|C) We can also use a Poisson language model for p(·|C), or use some other frequency-based models.
A document is assumed to be generated from a  potentially different model.
2.2.3 Two-Stage Smoothing As discussed in [29], smoothing plays two roles in retrieval: (1) to improve the estimation of the document language model, and (2) to explain the common terms in the query.
With a homogeneous Poisson process, the frequency count of each event, i.e., the number of  occurrences of wi, follows a Poisson distribution with associated parameter λit, where λi is a rate parameter characterizing the expected number of wi in a unit time.
The rate of a term is estimated  independently of other terms.
In this section, we introduce three different strategies to smooth a Poisson language model, and show how they lead to different retrieval functions.
PROCESS In the query generation framework, a basic assumption is that a query is generated with a model estimated based on a document.
A document can be scored with the likelihood in  Equation 1.
2.2.2 Interpolation (Jelinek-Mercer) Smoothing Another straightforward method is to decompose the query generation model as a mixture of two component models.
The second summation can be actually treated as a document prior, which penalizes long documents.
As in existing language modeling approaches, the main challenge of constructing a reasonable retrieval model is to find a smoothed language model for p(·|d).
p(·|U) models the typical term frequencies in the user"s queries.
In one extreme, we can use the vocabulary of the whole collection.
Given a particular document d, we want to estimate Λd.
We consider the frequency counts of the n unique terms in w as n different types of events, sampled from n independent homogeneous Poisson processes, respectively.
In other words, we may assume that there is exactly one non-query term in the vocabulary for each query.
In general, we have to discount the  probabilities of some words seen in the text to leave some extra  probability mass to assign to the unseen words.
We refer to these n independent Poisson processes with parameter Λ as a Poisson Language Model.
As a compromise, we may conflate all the non-query terms as one single pseudo term.
With no prior knowledge on p(·|U), we could set it to p(·|C).
Let D = {d1, ..., dm} be an observed set of document  samples generated from the Poisson process above.
Thus we do not have to discount the probability of seen words in order to give a non-zero rate to an unseen word.
In the other extreme, we may focus on the terms in the query and ignore other terms, but some useful information may be lost by ignoring the  nonquery terms.
In the retrieval formula above, the first summation can be computed efficiently.
In our experiments, we adopt this pseudo non-query term strategy.
In most existing work [12, 6, 28, 29], people assume that each query word is sampled independently from a multinomial distribution.
Without losing generality, we set t to the length of the text w (people write one word in a unit time), i.e., t = |w|.
The probability density function of such a Poisson Distribution is given by P(c(wi, w) = k|λit) = e−λit (λit)k k!
Instead, we only need to guarantee that k=0,1,2,... p(c(w, d) = k|d) = 1.
In practice, we have the flexibility to choose the  vocabulary V .
The empirical study of the smoothing methods is  presented in Section 4. 
2.1 The Generation Process Let V = {w1, ..., wn} be a vocabulary set.
Suppose t is the time period during which the author  composed the text.
However, this may bring in noise and  considerable computational cost.
where Λ = {λ1, ..., λn} and |w| = n i=1 c(wi, w).
