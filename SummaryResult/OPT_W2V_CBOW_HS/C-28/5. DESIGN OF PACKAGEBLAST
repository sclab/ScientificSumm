The slave processes τ database segments and notifies the master, which uses the last Ω notifications to compute the next allocation block size based on the  selected allocation strategy and the weight provided by PSS.
We also propose a strategy to compute grid nodes execution weight which distributes work units (database segments) to grid nodes according to their current computational power.
We opted to replicate the segmented database in every slave grid node to improve data accesses times and to  provide a potential for fault tolerance.
It distributes the work units generated by the previous module and collects the notifications.
The expression used is Φ(m, pi, P) (equation 12), defined as the weighted mean from the last Ω notifications sent by each pi slave node.
It calculates the weight of each slave node and decides how many work units will be assigned to a particular slave node, according to the chosen allocation policy.
The module Allocation Strategies contains  implementations for the pre-defined allocation policies (Fixed, SS, GSS, TSS and FAC2) and also makes possible the creation of new allocation strategies.
Distribute Work Units is the module responsible for the communication between the master and slaves nodes.
Just as in mpiBLAST (section 4), we decided to use database segmentation in PackageBLAST with an NCBI tool called formatdb, which was modified to generate more database segments of smaller size.
Φ(m, pi, P ) = P ∗ P i=1 Γ(m,pi,Ω) Γ(m,pi,Ω) P i=1 P i=1 Γ(m,pi,Ω) Γ(m,pi,Ω) (12) Γ(m, pi, Ω) (equation 13) specifies the average computing time of a segment in a node pi, considering the last Ω  notifications of TE(m, pi, τ), which is the average computation time of τ work units (database segments) assigned by the master m to a slave pi.
Γ(m, pi, Ω) = min(Ω,k) j=1 T E(m, pi, τ) min(Ω, k) (13) 5.3 PackageBLAST"s General Architecture PackageBLAST was designed as a grid service over Globus 3, based on Web Services and Java.
Thus, we propose the use of a framework where many allocation policies can be  incorporated.
The expression used for work unit allocation is shown in equation 11, where A(N, P) is the allocation policy for a  system with N workload units and P nodes and Φ(m, pi, P) is the weight calculated by PSS.
Besides that, we propose PSS (Package Weighted  Adaptive Self-Scheduling), a new strategy that adapts the chosen allocation policy to a grid with local workload.
A(N, P) can be a pre-defined allocation policy or a user-defined one.
5.1 Database Segmentation and Replication Segmentation consists in the division of a database archive in many portions of smaller size, called segments, that can be processed independently.
Considering the heterogeneity and dynamic characteristics of the grid, PSS is able to modify the length of the work units during execution, based on average processing time of each node.
158 allocate(m, pi, N, P ) = A(N, P ) ∗ Φ(m, pi, P ) (11) To distribute database segments to nodes, the master  analyzes periodic slave notifications.
This flow continues until all segments are processed. 
We propose an adaptive task allocation framework which is a grid service to perform BLAST searches against  sequence database segments.
It enables grid nodes to search smaller parts of a sequence database, reducing the number of disk accesses and hence improving BLAST performance.
To start processing, a minimum number of slaves must register into the master node, by  calling a master grid service.
The user specifies the sequence to be compared and chooses the allocation strategy.
Finally, the module Generate Reports obtains the  intermediary outputs sent by the slave nodes through file transfer and merges them into a single BLAST output report.
The framework, called  PackageBLAST, provides an infrastructure to choose or incorporate allocation strategies in a master/slave application.
The module Generate Work Units is the core of the PSS mechanism.
Then, the master sends a XML message to the slave  informing its new segments to process.
Also, a single query sequence can be compared against all segments in parallel.
After receiving connections from the slaves, the master notifies them about their initial  segments to compare.
So, the user can choose or even create the allocation policy which is the most appropriate to his/her environment and his/her BLAST parameters.
In general, the following execution flow is executed.
The master node starts execution and waits for slave connections.
By now, our framework contains five allocation policies: Fixed, SS, GSS, TSS, FAC2, all described in  section 3.
BLAST receives MASTER Strategies Allocation Work units Generate Work Units Distribute Reports Generate work units (to slaves)reports searches Figure 3: PackageBLAST architecture.
5.2 Task Allocation As [13], we think that no allocation policy will produce the best results for every situation.
At the moment of computation of Γ, if there is not enough notifications of TE, the calculation is done with total k notifications already received.
Figure 3 presents the PackageBLAST architecture.
Figure 2 illustrates this.
