In order to find the MLEs, we have two choices: (1) use numerical estimation to estimate all three parameters at once (2) fix the value of θ, and estimate the other two (β and γ or σl and σr) given our choice of θ, then consider alternate values of θ.
3.2 Motivation for Asymmetric Distributions Most of the previous parametric approaches to this problem  either directly or indirectly (when fitting only the posterior)  correspond to fitting Gaussians to the class-conditional densities; they differ only in the criterion used to estimate the parameters.
Namely, if we have a raw output score that can be used for discrimination, then the empirical  behavior between the modes (label B in Figure 2) is often very different than that outside of the modes (labels A and C in Figure 2).
Then, we can simply consider all choices of θ and choose the one with the maximum likelihood over all choices of θ.
The first of these tries to fit the posterior function directly, i.e.
Perfect classification corresponds to using two very asymmetric distributions, but in this case, the probabilities are actually one and zero and many methods will work for typical purposes.
Our work differs from earlier approaches primarily in three points: (1) We provide asymmetric parametric models suitable for use when little training data is available; (2) We explicitly analyze the quality of probability estimates these and competing methods produce and provide significance tests for these results; (3) We target text  classifier outputs where a majority of the previous literature targeted the output of search engines.
We assume the same definitions as above (the complete  derivation omitted for space is available in [2]), and in addition, let: Sl2 = x∈D|x≤θ x2 Sr2 = x∈D|x>θ x2 Dl2 = Sl2 − Slθ + θ2 Nl Dr2 = Sr2 − Srθ + θ2 Nr.
there is one p(s|+) p(s|−) Bayes" RuleP(+) P(−) Classifier P(+| s(d)) Predict class, c(d)={+,−} confidence s(d) that c(d)=+ Document, d and give unnormalized Figure 1: We are concerned with how to perform the box  highlighted in grey.
An asymmetric Laplace distribution can be achieved by placing two exponentials around the mode in the following manner: p(x | θ, β, γ) =    βγ β+γ exp [−β (θ − x)] x ≤ θ (β, γ > 0) βγ β+γ exp [−γ (x − θ)] x > θ (1) where θ, β, and γ are the model parameters.
A B C 0 0.2 0.4 0.6 0.8 1 −10 −5 0 5 10 p(s|Class={+,−}) Unnormalized Confidence Score s p(s | Class = +) p(s | Class = −) Figure 2: Typical View of Discrimination based on Gaussians However, using standard Gaussians fails to capitalize on a basic characteristic commonly seen.
Assuming the number of candidate θs are O(n), this process is O(n), and the overall process is  dominated by sorting the scores, O(n log n) (or expected linear time).
and X ∼ Γ(X | θ, σl, σr), the likelihood is N i Γ(X | θ, β, γ).
The analytical solution for the MLEs for a fixed θ is: σl,MLE = Dl2 + D 2/3 l2 D 1/3 r2 N (4) σr,MLE = Dr2 + D 2/3 r2 D 1/3 l2 N .
We define the following values: Nl = | {x ∈ D | x ≤ θ} | Nr = | {x ∈ D | x > θ} | Sl = x∈D|x≤θ x Sr = x∈D|x>θ x Dl = Nlθ − Sl Dr = Sr − Nrθ.
0 0.002 0.004 0.006 0.008 0.01 -300 -200 -100 0 100 200 p(s|Class={+,-}) Unnormalized Confidence Score s Gaussian A-Gaussian Figure 3: Gaussians vs. Asymmetric Gaussians.
As a  result, an asymmetric distribution may be a more appropriate choice for application to the raw output score of a classifier.
Since increasing s usually  indicates increased likelihood of belonging to the positive class, then the rightmost distribution usually corresponds to p(s|+).
θ is the mode of the distribution, β is the inverse scale of the exponential to the left of the mode, and γ is the inverse scale of the exponential to the right.
3.3 Estimating the Parameters of the  Asymmetric Distributions This section develops the method for finding maximum  likelihood estimates (MLE) of the parameters for the above asymmetric distributions.
Practically, some examples will fall between θ− and θ+, and it is often  important to estimate the probabilities of these examples well (since they correspond to the hard examples).
Then we increase θ and just step past the scores that have shifted from the right side of the distribution to the left.
The elegance of the formulae is that the estimates will tend to be symmetric only insofar as the data dictate it (i.e.
Note that Dl and Dr are the sum of the absolute differences  between the x belonging to the left and right halves of the distribution (respectively) and θ.
We can create an asymmetric Gaussian in the same manner: p(x | θ, σl, σr) =    2√ 2π(σl+σr) exp −(x−θ)2 2σ2 l x ≤ θ (σl, σr > 0) 2√ 2π(σl+σr) exp −(x−θ)2 2σ2 r x > θ (2) where θ, σl, and σr are the model parameters.
and X ∼ Λ(X | θ, β, γ), the likelihood is N i Λ(X | θ, β, γ).
(5) By continuity arguments, when N = 0, we assign σr = σl = inf , and when N = 0 and Dl2 = 0 (resp.
Both are termed generalizations of an Asymmetric Laplace in [14], but we refer to them as described above to reflect the nature of how we derived them for this task.
A text classifier produces a prediction about a document and gives a score s(d) indicating the strength of its decision that the document belongs to the positive class (relevant to the topic).
While these distributions are composed of halves, the resulting function is a single continuous distribution.
A natural first candidate for an asymmetric distribution is to  generalize a common symmetric distribution, e.g.
Assuming that θ falls in some range [φ, ψ] dependent upon only the observed documents, then this alternative is also easily  computable.
Furthermore, this family of distributions can still fit a symmetric distribution, and finally, in the empirical evaluation, evidence is presented that demonstrates this asymmetric behavior (see Figure 4).
An estimator for each of the class-conditional densities (i.e.
, xN } where the xi are i.i.d.
Now, we fix θ and compute the maximum likelihood for that choice of θ.
Because text classifiers have training data to use to separate the classes, the  final behavior of the score distributions is primarily a factor of the amount of training data and the consequent separation in the classes achieved.
Intuitively, the area between the modes corresponds to the hard  examples, which are difficult for this classifier to distinguish, while the areas outside the modes are the extreme examples that are usually easily distinguished.
(3) These estimates are not wholly unexpected since we would obtain Nl Dl if we were to estimate β independently of γ.
Similarly, when N = 0 and Dl = 0, we assign β = inf where inf is a very large constant that corresponds to an extremely sharp distribution (i.e.
These distributions allow us to fit our data with much greater flexibility at the cost of only fitting six parameters.
, xN } where the xi are i.i.d.
Justifications can be given for both why you may find more and less examples between θ− and θ+ than outside of them, but there are few empirical reasons to believe that the distributions should be symmetric.
In addition, if the scores are sorted, then we can perform the whole process quite efficiently.
This is in contrast to search engine retrieval where the distribution of scores is more a factor of language distribution across documents, the similarity function, and the length and type of query.
By continuity arguments, when N = 0, we assign β = γ = 0 where 0 is a small constant that acts to disperse the distribution to a uniform.
This suggests that we may want to uncouple the scale of the outside and inside segments of the distribution (as depicted by the curve denoted as A-Gaussian in Figure 3).
Because of the simplicity of analysis in the latter alternative, we choose this method.
3.3.1 Asymmetric Laplace MLEs For D = {x1, x2, .
perfect classification) there will exist scores θ− and θ+ such that all examples with score greater than θ+ are relevant and all examples with scores less than θ− are irrelevant.
Starting with the minimum θ = φ we would like to try, we loop through the scores once and set Nl, Sl, Nr, Sr appropriately.
function estimator that performs a direct mapping of the score s to the probability P(+|s(d)).
We will use the notation Λ(X | θ, β, γ) to refer to this distribution.
3.3.2 Asymmetric Gaussian MLEs For D = {x1, x2, .
We could  instead try mixture models for each component or other extensions, but most other extensions require at least as many parameters (and can often be more computationally expensive).
the Laplace or the Gaussian.
p(s|+) and p(s|−)) is produced, then Bayes" rule and the class priors are used to obtain the estimate for P(+|s(d)).
To refer to this asymmetric Gaussian, we use the notation Γ(X | θ, σl, σr).
The complete derivation is omitted because of space but is  available in [2].
The distance | θ− − θ+ | corresponds to the margin in some classifiers, and an attempt is often made to maximize this quantity.
In addition, the motivation above should provide significant cause to believe the underlying distributions actually behave in this way.
Dr2 = 0), we  assign σl = 0 (resp.
A  Shortcoming of Symmetric Distributions - The vertical lines show the modes as estimated nonparametrically.
Given Nl, Sl, Nr, Sr, we can compute the posterior and the MLEs in constant time.
Again, the same computational complexity analysis applies to estimating these parameters. 
Finally the MLEs for β and γ for a fixed θ are: βMLE = N Dl + √ DrDl γMLE = N Dr + √ DrDl .
To our knowledge, neither family of distributions has been  previously used in machine learning or information retrieval.
The MLEs can be worked out similar to the above.
almost all mass at θ for that half).
the closer Dl and Dr are to being equal, the closer the resulting inverse scales).
We assume throughout there are only two classes: the positive and the negative (or irrelevant) class ("+" and "-" respectively).
There are two general types of parametric approaches.
The second type of approach breaks the problem down as shown in the grey box of Figure 1.
The internals are for one type of approach.
3.1 Problem Definition The general problem we are concerned with is highlighted in Figure 1.
Ideally (i.e.
Furthermore, no examples fall between θ− and θ+.
We can visualize this as depicted in Figure 2.
σr = 0).
Dr = 0 is handled similarly.
