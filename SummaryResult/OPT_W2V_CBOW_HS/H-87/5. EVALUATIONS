Table 4 summarizes the results in TDT5SU; adaptive LR was the winner in this case, with relevance feedback on 0.05% of the test documents improving the utility by 20.9% over the results of PRF Rocchio.
For comparison we also include the best results of Rocchio-based methods on these corpora, which are our own results of Rocchio on TDT3 and TDT5, and the best results reported by NIST for TREC10 and TREC11.
We optimized the parameters of our systems on TDT3, and fixed those parameters in the runs on TDT5 for our submissions to TDT2004.
Given that we only have one labeled example per topic in the TDT5 training sets, it is impossible to effectively optimize these parameters on the training data, and we had to choose an external corpus for validation.
5.4 Summary of Adaptation Process After we decided the parameter settings using validation, we perform the adaptive filtering in the following steps for each topic: 1) Train the LR/Rocchio model using the provided positive training examples and 30 randomly sampled negative examples; 2) For each document in the test corpus: we first make a prediction about relevance, and then get relevance feedback for those (predicted) positive documents.
Our PRF Rocchio (with a fixed threshold for all the topics) had the best performance.
4 The TREC10-best result (0.496 by Oracle) is only available in T10U which is not directly comparable to the scores in T11SU, just indicative.
In the case of minimizing Ctrk, the safe range for λ is between 0 and 0.1, and setting λ between 0.005 and 0.05 yielded relatively large improvements over the performance of standard LR because training a model for extremely high recall is statistically more tricky, and hence more regularization is needed.
Since exhaustive testing of all possible parameter settings is computationally intractable, we followed a step-wise forward chaining procedure instead: we pre-specified an order of the parameters in a method (Rocchio or LR), and then tuned one parameter at the time while fixing the settings of the remaining parameters.
Table 3: AF methods on TDT5 (Performance in Ctrk) Base Roc PRF Roc Adp Roc LR % of RF 0% 0% 0.6% 0.2% Ctrk 0.076 0.0707 0.0324 0.0382 ±% +7% (baseline) -54% -46% Table 4: AF methods on TDT5 (Performance in TDT5SU) Base Roc PRF Roc Adp Roc LR(λ=.01) % of RF 0% 0% 0.04% 0.05% TDT5SU 0.57 0.6452 0.69 0.78 ±% -11.7% (baseline) +6.9% +20.9% Evidently, both Rocchio and LR are highly effective in adaptive filtering, in terms of using of a small amount of labeled data to significantly improve the model accuracy in statistical learning, which is the main goal of AF.
This observation does not support the previous report by [13], but we are not surprised because we are not convinced that Rocchio prototypes are more accurate than LR models for topics in the early stage of the AF process, and we believe that using a Rocchio prototype as the mean in the Gaussian prior would introduce undesirable bias to LR.
We also put the 1st and 3rd quartiles as sticks for each site.2 T11SU Ours 0.7328 Site3 0.7281 Site2 0.6672 Site4 0.382 Metric = TDT5SU (the higher the better) 0.7328 0.7281 0.6672 0.382 0 0.2 0.4 0.6 0.8 1 Ours Site3 Site2 Site4 Figure 3:TDT2004 results in TDT5SU of systems using true relevance feedback.
The shared parameters include the sample weightsα , β and γ , the sample size of the negative training documents (i.e., )(TD− ), the term-weighting scheme, and the maximal number of non-zero elements in each document vector.
0.05 0.26 0.67 0.69 0.00 0.10 0.20 0.30 0.40 0.50 0.60 0.70 0.80 0.90 0.02 0.03 0.04 0.06 0.08 0.1 0.15 0.2 0.25 0.3 Threshold TDT5SU TDT3 TDT5 TREC10 TREC11 Figure 5: Performance curves of adaptive Rocchio Figure 5 compares the performance curves in TDT5SU for Rocchio on TDT3, TDT5, TREC10 and TREC11 when the decision threshold varied.
We report our empirical findings in four parts: the TDT2004 official evaluation results, the cross-corpus parameter optimization results, and the results corresponding to the amounts of relevance feedback.
From this set of results, we see that LR significantly outperformed Rocchio on all the corpora, even in the runs of standard LR without any tuning, i.e.
We measured the performance of the runs on TREC10 and TREC11 using T11SU, and the performance of the runs on TDT3 and TDT5 using TDT5SU.
In the case of maximizing the utilities, the safe interval for λ is between 0 and 0.01, meaning that the performance of regularized LR is stable, the same as or improved slightly over the performance of standard LR.
(Ours is the Rocchio method.)
To answer it, we evaluated Rocchio and LR on TDT with the following settings: • Basic Rocchio, no adaptation at all • PRF Rocchio, updating topic profiles without using true relevance feedback; • Adaptive Rocchio, updating topic profiles using relevance feedback on system-accepted documents plus 10 documents randomly sampled from the pool of  systemrejected documents; • LR with 0 rr =μ , 01.0=λ and threshold = 0.004; • All the parameters in Rocchio tuned on TDT3.
In this case, systems had only one labeled training example per topic during the entire training and testing processes, although unlabeled test documents could be used as soon as predictions on them were made.
For this reason we set a higher threshold (0.004) instead of the theoretically optimal threshold (0.0017) in LR to avoid an untolerable computation cost.
Another interesting observation in these results is that the performance of LR did not improve when using a Rocchio prototype as the mean in the prior; instead, the performance decreased in some cases.
We found TF-IDF most effective for both Rocchio and LR, and used this setting in all our experiments.
Table 3 summarizes the results in Ctrk: Adaptive Rocchio with relevance feedback on 0.6% of the test documents reduced the tracking cost by 54% over the result of the PRF Rocchio, the best system in the TDT2004 evaluation for topic tracking without relevance feedback information.
Ctrk Ours 0.0324 Site2 0.0467 Site3 0.1366 Site4 0.2438 Metric = Ctrk (the lower the better) 0.0324 0.0467 0.1366 0.2438 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 Ours Site2 Site3 Site4 Figure 2: TDT2004 results in Ctrk of systems using true relevance feedback.
That is, compared to the ad-hoc scores in Rocchio, the normalized posterior probabilities make the threshold optimization in LR a much easier problem.
Ctrk and TDT5SU were used as the metrics.
The performance on each corpus was measured using the corresponding metrics, that is, T11SU for the runs on TREC10 and TREC11, and TDT5SU and Ctrk for the runs on TDT3,.
Our Rocchio (with adaptive profiles but fixed universal threshold for all topics) had the best result in Ctrk, and our logistic regression had the best result in TDT5SU.
5.3 Percentages of labeled data How much relevance feedback (RF) would be needed during the AF process is a meaningful question in real-world applications.
Among the choices of TREC10, TREC11 and TDT3, we chose TDT3 (c.f.
The performance of LR is summarized with respect to λ tuning on the corpora of TREC10, TREC11 and TDT3.
CTrk Ours 0.0707 Site2 0.1545 Site5 0.5669 Site4 0.6507 Site6 0.8973 Primary Topic Traking Results in TDT2004 0.0707 0.8973 0.6507 0.1545 0.5669 0 0.2 0.4 0.6 0.8 1 1.2 Ours Site2 Site5 Site4 Site6 Ctrk Figure 4:TDT2004 results in Ctrk of systems without using true relevance feedback.
This empirical finding is consistent with a previous report [13] for LR on TREC11 although our results of LR (0.585~0.608 in T11SU) are stronger than the results (0.49 for standard LR and 0.54 for LR using Rocchio prototype as the prior) in that report.
Logistic regression has less difficulty with respect to threshold tuning because it produces probabilistic scores of )|1Pr( xy = upon which the optimal threshold can be directly computed if probability estimation is accurate.
We also believe that variance reduction (in the testing phase) should be controlled by the choice of λ (but not μ r ), for which we conducted the experiments as shown in Figure 6.
The robustness, we believe, comes from the probabilistic nature of the system-generated scores.
Table 2: Results of LR with different Bayesian priors Corpus TDT3 TDT5 TREC10 TREC11 LR(μ=0,λ=0) 0.7562 0.7737 0.585 0.5715 LR(μ=0,λ=0.01) 0.8384 0.7812 0.6077 0.5747 LR(μ=roc*,λ=0.01) 0.8138 0.7811 0.5803 0.5698 Best Rocchio 0.6628 0.6917 0.4964 0.475 3 The LR results (0.77~0.78) on TDT5 in this table are better than our TDT2004 official result (0.73) because parameter optimization has been improved afterwards.
*: μ r was set to the Rocchio prototype 0 0.2 0.4 0.6 0.8 0.000 0.001 0.005 0.050 0.500 Lambda Performance Ctrk on TDT3 TDT5SU on TDT3 TDT5SU on TDT5 T11SU on TREC11 Figure 6: LR with varying lambda.
The difficulty comes from the ad-hoc (non-probabilistic) scores generated by the Rocchio method: the distribution of the scores depends on the corpus, making cross-corpus threshold optimization a tricky problem.
Although the theoretical threshold could be inaccurate, it still suggests the range of near-optimal settings.
In either case, tuning λ is relatively safe, and easy to do successfully by cross-corpus tuning.
If we were using TREC10 or TREC11 instead of TDT3 as the validation corpus for TDT5, or if the TDT3 corpus were not available, we would have difficulty in obtaining strong performance for Rocchio in TDT2004.
With these threshold settings in our experiments for LR, we focused on the  crosscorpus validation of the Bayesian prior parameters, that is, μ r and λ.
(Ours is PRF Rocchio.)
Recall that Ctrk is an extremely high-recall oriented metric, causing frequent updating of profiles and hence an efficiency problem in LR.
All the parameters of our runs were tuned on the TDT3 corpus.
Figure 2 and Figure 3 show the results; the best run from each team was selected with respect to Ctrk or TDT5SU, respectively.
Moreover, logistic regression is known to converge towards the Bayes classifier asymptotically while Rocchio classifiers" parameters do not.
We also tested our methods on TREC10 and TREC11 for further analysis.
Both Rocchio and LR have parameters that must be  prespecified before the AF process.
Section 2) because it is most similar to TDT5 in terms of the nature of the topics (Section 2).
3) Model and IDF statistics will be incrementally updated if we obtain its true relevance feedback. 
Adaptive filtering without using true relevance feedback was also a part of the evaluations.
2 We use quartiles rather than standard deviations since the former is more resistant to outliers.
The computation time in machine-hours was 0.33 for the run of adaptive Rocchio and 14 for the run of LR on TDT5 when optimizing Ctrk.
These curves peak at different locations: the TDT3-optimal is closest to the TDT5-optimal while the TREC10-optimal and TREC1-optimal are quite far away from the TDT5-optimal.
5.2 Cross-corpus parameter optimization How much the strong performance of our systems depends on parameter tuning is an important question.
Incremental LR, on the other hand, was weaker but still impressive.
More importantly, our cross-benchmark evaluation gives strong evidence for the robustness of LR.
Another influential choice in our experiment settings is term weighting: we examined the choices of binary, TF and TF-IDF (the ltc version) schemes.
Given the penalty ratio for misses vs. false alarms as 2:1 in T11SU, 10:1 in TDT5SU and 583:1 in Ctrk (Section 3.3), the corresponding optimal thresholds (t) are 0.33, 0.091 and 0.0017 respectively.
(Ours is LR with 0=μ r and 005.0=λ ).
The method-specific parameters include the decision threshold in Rocchio, and μ r , λ and MI (the maximum number of iterations in training) in LR.
Figure 4 shows the summarized official submissions from each team.
5.1 TDT2004 benchmark results The TDT2004 evaluations for adaptive filtering were conducted by NIST in November 2004.
We repeated this procedure for several passes as time allowed.
Results for other sites are also listed anonymously for comparison.
Such a setting has been conventional for the Topic Tracking task in TDT until 2004.
Table 2 summarizes the results 3 .
Multiple research teams participated and multiple runs from each team were allowed.
