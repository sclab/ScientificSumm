TREC11 was the evaluation benchmark for adaptive filtering in TREC 2002, consisting of the same set of documents as those in TREC10 but with a slightly different splitting point for the training and test sets.
The granularity differences among topics and the corresponding non-stationary distributions make the cross-benchmark evaluation interesting.
The tracking part of the corpus consists of 407,459 news stories in the period of April to September, 2003 from 15 news agents or broadcast sources in English, Arabic and Mandarin, with machine-translated versions of the non-English stories.
1 http://www.ldc.upenn.edu/Projects/TDT2001/topics.html Table 1: Statistics of benchmark corpora for adaptive filtering evaluations N(tr) is the number of the initial training documents; N(ts) is the number of the test documents; n+ is the number of positive examples of a predefined topic; * is an average over all the topics.
The TDT topics differ from TREC topics both conceptually and statistically.
Figure 1 compares the document densities of a TREC topic (Civil Wars) and two TDT topics (Gunshot and APEC Summit Meeting, respectively) over a 3-month time period, where the area under each curve is normalized to one.
The splitting point for training-test sets is different for each topic in TDT.
The tracking part of the corpus consists of 71,388 news stories from multiple sources in English and Mandarin (AP, NYT, CNN, ABC, NBC, MSNBC, Xinhua, Zaobao, Voice of America and PRI the World) in the period of October to December 1998.
TREC10 was the evaluation benchmark for adaptive filtering in TREC 2001, consisting of roughly 806,791 Reuters news stories from August 1996 to August 1997 with 84 topic labels (subject categories)[7].
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 Week P(topic|week) Gunshot (TDT5) APEC Summit Meeting (TDT3) Civil War(TREC10) Figure 1: The temporal nature of topics 
We only used the English versions of those documents in our experiments for this paper.
Cross-benchmark evaluations allow us to test this hypothesis and possibly identify the weaknesses in current approaches to adaptive filtering in tracking the drifting trends of topics.
Instead of generic, ever-lasting subject categories (as those in TREC), TDT topics are defined at a finer level of granularity, for events that happen at certain times and locations, and that are born and die, typically associated with a bursty distribution over chronologically ordered news stories.
Machine-translated versions of the non-English stories (Xinhua, Zaobao and VOA Mandarin) are provided as well.
The first two weeks (August 20th to 31st , 1996) of documents is the training set, and the remaining 11 & Â½ months (from September 1st , 1996 to August 19th , 1997) is the test set.
The TREC11 topics (50) are quite different from those in TREC10; they are queries for retrieval with relevance judgments by NIST assessors [8].
The average size of TDT topics (events) is two orders of magnitude smaller than that of the TREC10 topics.
TDT3 was the evaluation benchmark in the TDT2001 dry run1 .
We used four benchmark corpora in our study.
TDT5 was the evaluation benchmark in TDT2004 [4].
Table 1 shows the statistics about these data sets.
For example, algorithms favoring large and stable topics may not work well for short-lasting and  nonstationary topics, and vice versa.
