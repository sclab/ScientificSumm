We find that users who comment more on the same feature are more likely to agree on a common numerical rating for that  particular feature.
The authors conclude that the model explains the empirical distribution of reports, and offers insights into smarter ways of estimating the true quality of the product.
Intuitively, lengthy comments reveal the  importance of the feature to the user.
Numerous empirical studies [10, 15, 13, 5] show that buyers  seriously consider online feedback when making purchasing decisions, and are willing to pay reputation premiums for products or services that have a good reputation.
Second we investigate the relationship between a review 1 http://www.amazon.com 2 http://www.tripadvisor.com/ 134 Figure 1: The TripAdvisor page displaying reviews for a popular Boston hotel.
By analyzing the time sequence of reports, we  conclude that past reviews influence the future reports, as they create some prior expectation regarding the quality of  service.
In the absence of clear incentives, users with a moderate outlook will not bother to voice their  opinions, which leads to an unrepresentative sample of reviews.
Our first result can be used to determine a  featureby-feature estimate of quality, where for each feature, a  different subset of reviews (i.e., those with lengthy comments of that feature) is considered.
The first is simple linguistic evidence from the textual review that usually accompanies the  numerical ratings.
Improving the way we aggregate the information available from online reviews requires a deep understanding of the underlying factors that bias the rating behavior of users.
Both results can be used to improve the way reputation mechanisms aggregate the information from individual  reviews.
Under these circumstances, using the arithmetic mean to predict quality (as most forums actually do) gives the typical user an estimator with high variance that is often false.
Recent analysis, however, raises important questions  regarding the ability of existing forums to reflect the real  quality of a product.
A perusal of online  reviews shows that ratings are often part of discussion threads, where one post is not necessarily independent of other posts.
Since people tend to be more knowledgeable in the aspects they consider important, users who discuss a given feature in more details might be assumed to have more authority in evaluating that feature.
One may see, for example, users who make an effort to  contradict, or vehemently agree with, the remarks of previous users.
The subjective perception of the user is influenced by the gap between the prior expectation and the actual  performance of the service [17, 18, 16, 21] which will later reflect in the user"s rating.
The second leads to an  algorithm that outputs a more precise estimate of the real quality. 
and the reviews that preceded it.
We propose a model that captures the dependence of ratings on prior expectations, and validate it using the empirical data we collected.
The spread of the internet has made it possible for online feedback forums (or reputation mechanisms) to become an important channel for Word-of-mouth regarding products, services or other types of commercial interactions.
[12] propose the Brag-and-Moan Model where users rate only if their utility of the product (drawn from a normal distribution) falls outside a median interval.
Using actual hotel reviews from the TripAdvisor2 website, we consider two additional sources of information besides the basic numerical ratings submitted by users.
We use text-mining techniques similar to [7] and [3], however, we are only interested in identifying what aspects of the service the user is discussing, without computing the semantic orientation of the text.
For example, [12, 1] show that Amazon1 ratings of books or CDs follow with great probability bi-modal, U-shaped  distributions where most of the ratings are either very good, or very bad.
Controlled experiments, on the other hand, reveal opinions on the same items that are normally  distributed.
In the present paper we extend this line of research, and attempt to explain further facts about the behavior of users when reporting online feedback.
Name of hotel and  advertisements were deliberatively erased.
Hu et al.
