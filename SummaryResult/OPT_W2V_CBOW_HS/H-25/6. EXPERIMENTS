This is done by truncating the relevance feedback  modified query model to a size equal to the number of checked terms for the same topic.
Comparative evaluation of relevance feedback against other  methods is complicated by the fact that some documents have already been viewed during feedback, so it makes no sense to include them in the retrieval results of the second run.
The truth Figure 3: Comparison of expansion term quality between  relevance feedback (with 10 feedback documents) and term  feedback (with 3 × 16 CFs) −1−0 0−1 1−2 2−3 3−4 4−5 5−6 0 50 100 150 200 250 300 350 σKLD #terms relevance feedback term feedback is, although there are many unwanted terms in the expanded query model from feedback documents, there are also more relevant terms than what the user can possibly select from the list of presentation terms generated with pseudo-feedback documents, and the positive effects often outweights the negative ones.
We think that there are two possible explanations for this phenomenon of term feedback being active even when  relevance feedback does not work: First, even if none of the top N (suppose it is a small number) documents are relevant, we may still find relevant documents in top 60, which is more inclusive but usually unreachable when people are doing relevance feedback in interactive ad-hoc search, from which we can draw feedback terms.
Finally we compare term feedback to relevance feedback and show that it has its particular advantage.
All methods perform considerably better than the  pseudofeedback baseline, with TCFB3C achieving a highest 41.1% improvement in MAP, indicating significant contribution of term feedback for clarification of the user"s information need.
We can then define precision and recall of user term judgment accordingly: precision is the fraction of terms checked by the user that are relevant; recall is the fraction of presented relevant terms that are checked by the user.
Table 5: Change of MAP when using all (and only) relevant terms (σKLD > 1.0) for feedback.
The total number of  presented terms (M) is fixed at 48, so by comparing the performance of different types of clarification forms we can know the effects of different degree of clustering.
Therefore, we can easily  simulate what happens when the number of presentation terms decreases 2 There are complexities arising from terms appearing in top L of multiple clusters, but these are exceptions from M to M : we will keep all judgments of the top L = M /K terms in each cluster and discard those of others.
We see that TFB gets big improvement when there is an  oracle who checks all relevant terms, while CFB meets a bottleneck around MAP of 0.325, since all it does is adjust cluster weights, and when the learned weights are close to being accurate, it  cannot benefit more from term feedback.
This is true for topic 367 piracy, where the top 10 feedback  documents are all about software piracy, yet there are documents  between 10-60 that are about piracy on the seas (which is about the real information need), contributing terms such as pirate, ship for selection in the clarification form.
Because the presentation terms within each cluster are generated in decreasing order of their frequencies, the presentation list forms a subset of the original one if its size is reduced2 .
user"s  information gain, if the feedback documents are relevant, they should be kept in the top of the ranking; if they are irrelevant, they should be left out.
We adopt the Simplified KL Divergence metric used in [24] to decide query expansion terms as our term relevance  measure: σKLD(w) = p(w|R) log p(w|R) p(w|¬R) where p(w|R) is the probability that a relevant document contains term w, and p(w|¬R) is the probability that an irrelevant document contains w, both of which can be easily computed via maximum likelihood estimate given document-level relevance judgment.
We see that the user is usually able to finish term feedback within a reasonably short amount of time: for more than half of the topics the clarification form is completed in just 1 minute, and only a small fraction of topics (less than 10% for 1 × 48 and 3 × 16) take more than 2 minutes.
original term feedback relevant term feedback TF1 0.288 0.354 TF3 0.288 0.354 TF6 0.278 0.346 CF3 0.305 0.325 CF6 0.301 0.326 TCF3 0.309 0.345 TCF6 0.304 0.341 6.4 Comparison with Relevance Feedback Now we compare term feedback with document-level relevance feedback, in which the user is presented with the top N documents from an initial retrieval and asked to judge their relevance.
For example, at only 12 terms CFB3C (the clarification form is of size 3 × 4) can still improve 36.5% over the baseline, dropping slightly from 39.3% at 48 terms.
Although TCFB is just a simple mixture of TFB and CFB by interpolation, it is able to outperform both.
Then we discuss the effects of varying the parameter setting in the algorithms, as well as the number of presentation terms.
N MAP Pr@30 RR 5 0.302 0.586 4779 10 0.345 0.670 4916 20 0.389 0.772 5004 TCFB3C 0.309 0.491 4947 We see that the performance of TCFB3C is comparable to that of relevance feedback using 5 documents.
Figure 2: Clarification form completion time distributions 0−30 30−60 60−90 90−120 120−150 150−180 0 5 10 15 20 25 30 35 completion time (seconds) #topics 1×48 3×16 6×8 Figure 2 shows the distribution of time needed to complete a clarification form3 .
Baseline TFB1C TFB3C TFB6C CFB1C CFB3C CFB6C TCFB1C TCFB3C TCFB6C MAP 0.219 0.288 0.288 0.278 0.254 0.305 0.301 0.274 0.309 0.304 Pr@30 0.393 0.467 0.475 0.457 0.399 0.480 0.473 0.431 0.491 0.473 RR 4339 4753 4762 4740 4600 4907 4872 4767 4947 4906 % 0% 31.5% 31.5% 26.9% 16.0% 39.3% 37.4% 25.1% 41.1% 38.8% Table 3: MAP variation with the number of presented terms.
We conjecture the reason to be that while TFB"s  performance heavily depends on how many good terms are chosen for query expansion, CFB only needs a rough estimate of cluster weights to work.
The user is able to recognize only 6.9 of these terms on average.
There seems to be a trade-off between increasing topic diversity by clustering and losing extra relevant terms: when there are more clusters, each of them gets fewer terms to present, which can hurt a major relevant cluster that contains many relevant terms.
On the other hand, If the user had correctly checked all such relevant terms, the performance of our algorithms would have increased a lot, as shown in Table 5.
We can then compare the terms in the truncated model with the checked terms.
We find that term feedback tends to produce expansion terms of higher quality(those with σKLD > 1) compared to relevance feedback (with 10 feedback documents).
Although it is poorer than when there are 10 feedback documents in terms of MAP and Pr@30, it does retrieve more documents (4947) when going down the ranked list.
This is natual, as for a large cluster number of 6, it is easier to get into the situation where each cluster gets too few presentation terms to make topic diversification useful.
In the case of 3 × 16 clarification forms, the average number of terms checked as relevant by the user is 13.3 per topic, and the average number of relevant terms whose σKLD value exceed 1.0 is 12.6.
The baseline turns out to perform above average among the track participants.
Also, the 3 × 16 clarification forms seem to be more robust than the 6 × 8 ones: at 12 terms the MAP of TFB6C is 87.1% of that at 48 terms, lower than 90.6% for TFB3C.
Table 3 shows the performance of various algorithms as the number of presentation terms ranges from 6 to 48.
The evaluation metrics we adopt include mean average (non-interpolated) precision (MAP), precision at top 30 (Pr@30) and total relevant retrieved (RR).
It is interesting to know whether our algorithms" performance  deteriorates when the user is presented with fewer terms.
Table 4 shows the number of checked terms, relevant terms and relevant checked terms when σ0 is set to 1.0, as well as the precision/recall of user term judgment.
Overall, we are surprised to see that the algorithms are still able to perform reasonably well when the number of presentation terms is small.
Table 6: Performance of relevance feedback for different  number of feedback documents (N).
Therefore, we use relevance feedback to produce a ranking of top 1000 retrieved documents but with every feedback document excluded, and then prepend the relevant feedback documents at the front.
We designed three sets of clarification forms for term  feedback, differing in the choice of K, the number of clusters, and L, the number of presented terms from each cluster.
It is for these hard topics that user feedback is most helpful, as it can provide information to disambiguate the queries; with easy topics the user may be unwilling to spend efforts for feedback if the automatic retrieval results are good enough.
For example, at 12 terms the MAP of TFB3C is 90.6% of that at 48 terms, while the numbers for CFB3C and TCFB3C are 98.0% and 96.1%  respectively.
However, this does not hold for term feedback.
CFB1C is actually worse because it cannot adjust the weight of its  (single) cluster from term feedback and it is merely  pseudofeedback.
For TFB, the performance is almost equal on the 1 × 48 and 3 × 16 clarification forms in terms of MAP (although the latter is slightly better in Pr@30 and RR), and a little worse on the 6 × 8 ones.
Surprisingly, in 11 out of 13 such cases where relevance  feedback seems impossible, the user is able to check at least 2  relevant terms from the 3 × 16 clarification form (we consider term t to be relevant if σKLD(t) > 1.0).
Indeed, the precision and recall of user feedback terms (as defined previously) are far from perfect.
After user feedback is received, we run the term feedback algorithms (TFB, CFB or TCFB) to estimate updated query  models, which are then used for a second iteration of retrieval.
Furthermore, in 10 out of them TCFB3C outperforms the pseudo-feedback baseline,  increasing MAP from 0.076 to 0.146 on average (these are particularly hard topics).
This suggests that term feedback is suitable for interactive ad-hoc retrieval, where a user usually does not want to spend too much time on providing feedback.
We try to compare the quality of automatically inserted terms in relevance feedback with that of manually selected terms in term feedback.
with low retrieval performance.
The major finding we can make from Table 4 is that the user is not particularly good at identifying relevant terms, which echoes the discovery in [18].
After an initial run using this baseline retrieval method, we take the top 60 documents for each topic and apply the theme discovery algorithm to output the  clusters (1, 3, or 6 of them), based on which we generate clarification forms.
In other words, term feedback is truly helpful for improving retrieval accuracy.
Table 6 shows the performance of relevance feedback for different values of N and compares it with TCFB3C.
For example, in topic 639 consumer online shopping, a document needs to mention what contributes to shopping growth to really match the specified information need, hence none of the top 10 feedback documents are regarded as relevant.
Note that when the clarification forms contain more clusters, fewer terms are checked: 14.8 for 1 × 48, 13.3 for 3 × 16 and 11.2 for 6×8.
# terms TFB1C TFB3C TFB6C CFB3C CFB6C TCFB3C TCFB6C 6 0.245 0.240 0.227 0.279 0.279 0.281 0.274 12 0.261 0.261 0.242 0.299 0.286 0.297 0.281 18 0.275 0.274 0.256 0.301 0.282 0.300 0.286 24 0.276 0.281 0.265 0.303 0.292 0.305 0.292 30 0.280 0.285 0.270 0.304 0.296 0.307 0.296 36 0.282 0.288 0.272 0.307 0.297 0.309 0.297 42 0.283 0.288 0.275 0.306 0.298 0.309 0.300 48 0.288 0.288 0.278 0.305 0.301 0.309 0.303 NIST.
Actually, when we use the truncated query model instead of the intact one refined from relevance feedback, the MAP is only 0.304.
In this case, we may still extract useful terms from these documents, even if they do not qualify as  relevant ones.
The topics  included 50 ones previously known to be hard, i.e.
Both CFB3C and CFB6C perform better than their TFB  counterparts in all three metrics, suggesting that feedback on a secondary cluster structure is indeed beneficial.
6.1 Experiment Setup and Basic Results We took the opportunity of TREC 2005 HARD Track[2] for the evaluation of our algorithms.
We evaluate the different retrieval methods" performance on their rankings of the top 1000 documents.
What is then, the extent to which the user is good at term  feedback?
CFB, each of which is better than the other in nearly half of the topics.
We stem the terms, choose Dirichlet smoothing with a prior of 2000, and truncate query language models to 50 terms (these settings are used throughout the experiments).
This is not infrequent, as one might have thought: out of the 50 topics, there are 13 such cases when N = 5, 10 when N = 10, and still 3 when N = 20.
We consider a term relevant if its Simplified KL Divergence value is greater than a certain threshold σ0.
Similar pattern holds for relevant terms and relevant checked terms.
One such situation is when none of the top N feedback documents is relevant, rendering relevance feedback useless.
Next we analyze user term feedback behavior and its relation to retrieval performance.
We find that the performance of TFB is more susceptible to  presentation term reduction than that of CFB or TCFB.
Table 2 shows the performance of various methods and configurations of K × L. The suffixes (1C, 3C, 6C) after TFB,CFB,TCFB stand for the number of clusters (K).
We first  describe our experiment setup and present an overview of various methods" performance.
Figure 3 shows the distribution of the terms" σKLD scores.
They are: 1× 48, a big cluster with 48 terms, 3 × 16, 3 clusters with 16 terms each, and 6 × 8, 6 clusters with 8 terms each.
6.3 User Feedback Analysis In this part we study several aspects of user"s term feedback  behavior, and whether they are connected to retrieval performance.
The form is self-explanatory; there is no need for extra user training on how to use it.
The last row is the percentage of MAP improvement over the baseline.
terms 15.0 12.6 11.2 # rel.
The feedback process is simulated using document relevance judgment from NIST.
6.2 Reduction of Presentation Terms In some situations we may have to reduce the number of  presentation terms due to limits in display space or user feedback efforts.
In this section, we describe our experiment results.
Take the topic in Figure 1 for example.
We find that a user often makes mistakes when judging term  relevance.
Participants of the track were able to submit custom-designed clarification forms (CF) to solicit feedback from human assessors provided by Table 2: Retrieval performance for different methods and CF types.
CFB6C, the performance advantage of TCFB over TFB/CFB is significant at p < 0.05 using the Wilcoxon signed rank test.
For each topic, an assessor would complete the forms ordered by 6 × 8, 1 × 48 and 3 × 16, spending up to three minutes on each form.
Sometimes a relevant term may be left out because its  connection to the query topic is not obvious to the user.
Our initinal queries are constructed only using the topic title descriptions, which are on average 2.7 words in length.
The top N documents may be related to the topic, but nonetheless irrelevant.
This does not contradict the fact that the latter yields higher retrieval performance.
As our baseline we use the KL divergence retrieval method implemented in the Lemur Toolkit1 with 5 pseudo-feedback documents.
Table 4: Term selection statistics (topic average) CF Type 1 × 48 3 × 16 6 × 8 # checked terms 14.8 13.3 11.2 # rel.
When this happens, one can only back off to the original retrieval method; the power of relevance feedback is lost.
Also note that TCFB fails to outperform TFB, probably because TFB is sufficiently accurate.
We are interested to know under what circumstances term  feedback has advantage over relevance feedback.
We use the mixture model based feedback method  proposed in [25], with mixture noise set to 0.95 and feedback  coefficient set to 0.9.
Does it have serious impact on retrieval performance?
To  answer these questions, we need a measure of individual terms" true relevance.
The sample clarification form shown in Figure 1 is of type 3 × 16.
This is not true in the case of TFB v.s.
But  nevertheless, the feedback terms such as retail, commerce are good for query expansion. 
checked terms 7.9 6.9 5.9 precision 0.534 0.519 0.527 recall 0.526 0.548 0.527 Blanc Tunnel between France and Italy in 1999, but the user failed to select such keywords as mont, blanc, french and italian due to his/her ignorance of the event.
For all other parameters we use Lemur"s default settings.
Second, for some topics, a document needs to meet some special condition in order to be relevant.
For example, TCFB3C means the TCFB method on the 3 × 16 clarification forms.
It is a simple and compact interface in which the user can check relevant terms.
This supports our speculation that TCFB overcomes the drawbacks of TFB (paying attention only to checked terms) and CFB (not  distinguishing checked and unchecked terms in a cluster).
Other times a dubious term may be included but turns out to be irrelevant.
The tracks used the AQUAINT  collection, a 3GB corpus of English newswire text.
Similarly, for CFB it is 95.0% against 98.0%.
There was a fire disaster in Mont 3 The maximal time is 180 seconds, as the NIST assessor would be forced to submit the form at that moment.
If σKLD(w) > 0, w is more likely to appear in relevant documents than irrelevant ones.
Therefore, it is not always helpful to have more clusters, e.g., TFB6C is actually worse than TFB1C.
Thus, to make it fair w.r.t.
Indeed, without proper  context it would be hard to make perfect judgment.
The parameter settings μ = 4, λ = 0.1, α = 0.3 are near optimal.
From Table 2 we can make the following observations: 1 http://www.lemurproject.com 1.
Except for TCFB6C v.s.
