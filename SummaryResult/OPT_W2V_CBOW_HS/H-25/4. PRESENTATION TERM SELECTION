If the relevant terms are plentiful, but all concentrate on a single aspect of the query topic, then we will only be able to get feedback on that aspect and missing others, resulting in a breadth loss in retrieved results.
In our approach, the top N documents from an initial retrieval using the original query form the source of feedback terms: all terms that appear in them are considered candidates to present to the user.
The selected terms are then  presented to the user for judgment.
The  documents are regarded as sampled from a mixture of K + 1  components, including the K clusters and the background model: p(w|d) = λBp(w|θB) + (1 − λB) K i=1 πd,ip(w|θi) where w is a word, λB is the mixture weight for the background model θB, and πd,i is the document-specific mixture weight for the i-th cluster model θi.
First, we introduce a background model θB that is estimated from collection statistics and explains the common terms, so that they are much less likely to appear in the presentation list.
These documents serve as pseudo-feedback, since they provide a much richer context than the original query (usually very short), while the user is not asked to judge their relevance.
We also filter out terms in the original query text because they tend to always be relevant when the query is short.
Proper selection of terms to be presented to the user for  judgment is crucial to the success of term feedback.
Note that only the middle cluster is relevant.
Table 1: Cluster models for topic 363 Transportation tunnel disasters Cluster 1 Cluster 2 Cluster 3 tunnel 0.0768 tunnel 0.0935 tunnel 0.0454 transport 0.0364 fire 0.0295 transport 0.0406 traffic 0.0206 truck 0.0236 toll 0.0166 railwai 0.0186 french 0.0220 amtrak 0.0153 harbor 0.0146 smoke 0.0157 train 0.0129 rail 0.0140 car 0.0154 airport 0.0122 bridg 0.0139 italian 0.0152 turnpik 0.0105 kilomet 0.0136 firefight 0.0144 lui 0.0095 truck 0.0133 blaze 0.0127 jersei 0.0093 construct 0.0131 blanc 0.0121 pass 0.0087 · · · · · · · · · From each of the K estimated clusters, we choose the L = M/K terms with highest probabilities to form a total of M  presentation terms.
We then estimate the cluster models by  maximizing the probability of the pseudo-feedback documents being generated from the multinomial mixture model: log p(D|Λ) = d∈D w∈V c(w; d) log p(w|d) where D = {di| i = 1, 2, · · · N} is the set of the N documents, V is the vocabulary, c(w; d) is w"s frequency in d and Λ = {θi| i = 1, 2, · · · K} ∪ {πdij | i = 1, 2, · · · N, j = 1, 2, · · · K} is the set of model parameters to estimate.
This is similar to active feedback[21], which suggests that a retrieval system should actively probe the user"s  information need, and in the case of relevance feedback, the feedback documents should be chosen to maximize learning benefits (e.g.
The cluster models can be  efficiently estimated using the Expectation-Maximization (EM)  algorithm.
Therefore, it is important to carefully select presentation terms to maximize expected gain from user feedback, i.e., those that can potentially reveal most evidence of the user"s information need.
Second, the terms are selected from multiple clusters in the pseudo-feedback documents, to ensure sufficient representation of different aspects of the topic.
We rely on the mixture multinomial model, which is used for theme discovery in [26].
We also do not explicitly exploit negative feedback (i.e., penalizing irrelevant terms), because with binary feedback an unchecked term is not necessarily irrelevant (maybe the user is  unsure about its relevance).
The simplest way of selecting feedback terms is to choose the most frequent M terms from the N documents.
In this study we only deal with binary judgment: a presented term is by default unchecked, and a user may check it to  indicate relevance.
We could ask the user for finer  judgment (e.g., choosing from highly relevant, somewhat relevant, do not know, somewhat irrelevant and highly irrelevant), but binary feedback is more compact, taking less space to display and less user effort to make judgment. 
First, a lot of common noisy terms will be selected due to their high frequencies in the document collection, unless a stop-word list is used for filtering.
A sample (completed) feedback form is shown in Figure 1.
Second, the  presentation list will tend to be filled by terms from major aspects of the topic; those from a minor aspect are likely to be missed due to their relatively low frequencies.
If the terms are poorly chosen and there are few relevant ones, the user will have a hard time looking for useful terms to help clarify his/her  information need.
We solve the above problems by two corresponding measures.
If a term happens to be in top L in multiple clusters, we assign it to the cluster where it has highest probability and let the other clusters take one more term as compensation.
diversely so as to increase coverage).
Due to the latter reason, it is possible to make N quite large (e.g., in our experiments we set N = 60) to increase its coverage of different aspects in the topic.
This method,  however, has two drawbacks.
Specifically, we assume the N documents contain K clusters {Ci| i = 1, 2, · · · K}, each characterized by a multinomial word distribution (also known as unigram language model) θi and corresponding to an aspect of the topic.
For its details, we refer the reader to [26].
Table 1 shows the cluster models for TREC query Transportation tunnel disasters (K = 3).
