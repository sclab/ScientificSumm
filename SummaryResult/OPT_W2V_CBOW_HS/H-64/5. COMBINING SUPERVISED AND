To use these weights in the relation browser we currently assign to each document the two topics on which it scored highest.
Also, evaluation of the utility of the learned topics for users will be undertaken. 
Specifically, using k-means, we clustered each of the 1279  documents into one of k clusters, with the number of clusters chosen by analyzing the within-cluster mean squared  distance at different values of k (see Section 6.1).
Prind is a probabilistic version of the Rocchio classification algorithm [9].
Having clustered the documents and trained a suitable classifier, the remaining 14,000 documents in the collection are labeled by means of automatic classification.
Like prind, naive Bayes attempts to classify documents into the most  probable class.
The output of this process is a score for every  document in the collection on each of the automatically  discovered topics.
Finally, support vector machines were thoroughly explicated by Vapnik [18], and applied specifically to text in [10].
UNSUPERVISED LEARNING FORTOPIC DISCOVERY To derive suitably general topics for the application of a dynamic interface to the BLS collection, we combined  document clustering with text classification techniques.
However, these clusters mark only the first step in a two-phase process of topic identification.
We tested three statistical classification  techniques: probabilistic Rocchio (prind), naive Bayes, and  support vector machines (SVMs).
They define a  decision boundary by finding the maximally separating  hyperplane in a high-dimensional vector space in which document classes become linearly separable.
Once the Editor"s Desk documents were assigned to  clusters, we constructed a k-way classifier that estimates the strength of evidence that a new document di is a member of class Ck.
In future work we will adopt a more rigorous method of deriving  documenttopic weight thresholds.
That is, for each document di we derive a k-dimensional vector,  quantifying the association between di and each class C1 .
All were implemented using McCallum"s BOW text classification library [14].
Deriving topic scores via naive Bayes for the entire  15,000document collection required less than two hours of CPU time.
These scores may then be used to populate a relation browser interface, or they may be added to a  traditional information retrieval system.
Interested readers are referred to Joachims" article for 6 http://www.bls.gov/opub/ted 154 further details of the classification method.
Constructing mutually exclusive clusters violates our assumption that documents may belong to multiple classes.
At the end of the process,  documentcluster affinity is measured by a real-valued number.
It is described in detail in [15].
