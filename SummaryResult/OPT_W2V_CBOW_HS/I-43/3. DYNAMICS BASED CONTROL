• Agent Level in turn combines the environment model from the Environment Design level, the dynamics estimation  procedure, the deviation measure and the target dynamics  specification from User Level, to produce a sequence of actions that create system dynamics as close as possible to the  targeted specification.
DBC"s target dynamics specification can use the environment"s state space as a means to describe, discern, and preserve changes that occur within the system.
• User Level in turn relies on the environment model produced by Environment Design to specify the target system  dynamics it wishes to observe.
Notice how DBC discards rewards, and replaces it by another optimality criterion (structural differences are summarized in Table 1): • Environment Design level is to specify a tuple < S, A, T, O, Ω, s0 >, where: - S is the set of all possible environment states; - s0 is the initial state of the environment (which can also be viewed as a probability distribution over S); The Sixth Intl.
The User Level also specifies the  estimation or learning procedure for system dynamics, and the measure of deviation.
In this situation, any further  constraints, e.g., smoothness of motion, speed limits in different  locations, or speed reductions during sharp turns, are explicitly and uniformly expressed by the tactical target, and can result in faster and more effective action selection by a DBC algorithm. 
Thus specification of target dynamics can be expressed by q ∈ F, and the learning or tracking algorithm can be represented as a function L : O×(A×O)∗ → F; that is, it maps sequences of observations and actions performed so far into an estimate τ ∈ F of system dynamics.
The other alternative would be to rely on the  estimation procedure provided by the User Level-to utilize the  Environment Design Level model of the environment to choose actions, so as to manipulate the dynamics estimator into believing that a  certain dynamics has been achieved.
3.2 DBC View of the State Space It is important to note the complementary view that DBC and POMDPs take on the state space of the environment.
• Agent Level is then faced with a problem of selecting a  control signal function a∗ to satisfy a minimization problem as follows: a∗ = arg min a Pr(d(τa, q) > θ) where d(τa, q) is a random variable describing deviation of the dynamics estimate τa, created by L under control signal a, from the ideal dynamics q.
There are many possible variations available at the User Level to define divergence between system dynamics; several of them are: - Trace distance or L1 distance between two  distributions p and q defined by D(p(·), q(·)) = 1 2 x |p(x) − q(x)| - Fidelity measure of distance F(p(·), q(·)) = x p(x)q(x) - Kullback-Leibler divergence DKL(p(·) q(·)) = x p(x) log p(x) q(x) Notice that the latter two are not actually metrics over the space of possible distributions, but nevertheless have  meaningful and important interpretations.
Implicit in this minimization problem is that L is manipulated via the environment, based on the environment model produced by the Environment  Design Level.
Alternatively, the state space could directly include the notion of speed.
Now, even if the reward  function would encode more, and finer, details of the properties of  motion, the POMDP solution will have to search in a much larger space of policies, while still being guided by the implicit concept of the reward accumulation procedure.
on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 791 - A is the set of all possible actions applicable in the  environment; - T is the environment"s probabilistic transition function: T : S ×A → Π(S).
Data would also be available about the system response to different actions performed; combined with a dynamics estimator defined by the User Level, this can provide an important tool for the environment model calibration at the Environment Design Level.
As a result, DBC has a greater ability to express state sequencing properties, which are grounded in the environment model and its state space definition.
Thus, for strong enough  estimator algorithms, successful manipulation would mean a successful simulation of the specified target dynamics (i.e., beyond discerning via the available sensory input).
DBC concentrates on the underlying principle of state  sequencing, the system dynamics.
That is, Ω(o|s , a, s) is the probability that one will observe o given that the environment has moved from state s to state s under action a.
For example, the User Level can operate as an exploratory device for a learning algorithm, inferring an ideal dynamics target from the environment model at hand that would expose and verify most critical features of system behavior.
POMDPs regard state as a stationary snap-shot of the environment;  whatever attributes of state sequencing one seeks are reached through properties of the control process, in this case reward accumulation.
POMDPs would encode terrain as state space points, while speed would be ensured by negative reward for every step taken without reaching the  goalaccumulating higher reward can be reached only by faster motion.
For instance, the Agent Level could provide data about target dynamics feasibility, allowing the User Level to modify the requirement, perhaps focusing on attainable features of system  behavior.
• Environment Design Level is concerned with the formal specification and modeling of the environment.
• User Level, in the case of Markovian environment, operates on the set of system dynamics described by a family of  conditional probabilities F = {τ : S × A → Π(S)}.
For instance,  KullbackLeibler divergence is an important tool of information  theory [3] that allows one to measure the price of encoding an information source governed by q, while assuming that it is governed by p. The User Level also defines the threshold of dynamics  deviation probability θ.
For  example, this level would specify the laws of physics within the system, and set its parameters, such as the gravitation  constant.
In this case, feasibility and system response data from the Agent Level would provide key information for an environment model update.
3.1 DBC for Markovian Environments For a Partially Observable Markovian Environment, DBC can be specified in a more rigorous manner.
That is, T(s |a, s) is the  probability that the environment will move from state s to state s under action a; - O is the set of all possible observations.
For POMDPs, this would mean that the same concept is encoded twice, in some sense: directly in the state space, and  indirectly within reward accumulation.
As we are interested in the continual development of a stochastic system, such as happens in classical control theory [16] and  continual planning [4], as well as in our example of museum sweeps, the question becomes how the Agent Level is to treat the  deviation measurements over time.
Specific action selection then depends on system formalization.
The specification of Dynamics Based Control (DBC) can be  broken into three interacting levels: Environment Design Level, User Level, and Agent Level.
This can be viewed as if the sequencing of states and the attributes of that sequencing are only introduced by and for the controlling mechanism-the POMDP policy.
For example, consider the task of moving through rough terrain towards a goal and reaching it as fast as possible.
In the museum guard scenario above, these would correspond to a stochastic sweep schedule, and a measure of relative surprise between the specified and actual sweeping.
Design Agent Model Ideal Dynamics Estimator Estimator Dynamics Feasibility System Response Data Figure 1: Data flow of the DBC framework Extending upon the idea of Actor-Critic algorithms [5], DBC data flow can provide a good basis for the design of a learning  algorithm.
On the other hand, the tactical target expression of variations and correlations between position and speed of motion is now grounded in the state space representation.
This is what the sensor input would look like for an outside observer; - Ω is the observation probability function: Ω : S × A × S → Π(O).
Notice that this manipulation is not direct, but via the environment.
To this end, we use a probability threshold-that is, we would like the Agent Level to maximize the probability that the deviation measure will remain below a certain threshold.
In fact, the  combination of feasibility and response data can provide a basis for the application of strong learning algorithms such as EM [2, 9].
One possibility would be to create a mixture of available system trends, much like that which happens in Behavior-Based Robotic architectures [1].
DBC levels can also have a back-flow of information (see  Figure 1).
Joint Conf.
