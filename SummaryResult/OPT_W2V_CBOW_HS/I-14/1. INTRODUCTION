Specifically, the contributions of this paper include: (1) a reinforcement learning based approach for agents to acquire satisfactory routing policies based on estimates of the potential contribution of their neighboring agents; (2) two strategies to speed up the learning process.
The intention of the reinforcement learning is to adapt the agents" routing decisions to the dynamic network situations and learn from past search sessions.
In this line of research, one of the core  problems that concerns researchers is to efficiently route user queries in the network to agents that are in possession of appropriate documents.
However, this organizational structure does not take into account the arrival patterns of queries, including their  frequency, types, and where they enter the system, nor the available communication bandwidth of the network and  processing capabilities of individual agents.
To our best knowledge, this is one of the first reinforcement learning applications in addressing distributed content  sharing problems and it is indicative of some of the issues in applying reinforcement in a complex application.
Second, the similarity-based approaches do not take into account the run-time characteristics of the P2P IR systems, including environmental parameters, bandwidth usage, and the  historical information of the past search sessions, that provide valuable information for the query routing algorithms.
In this paper, we develop a reinforcement learning based IR approach for improving the performance of distributed IR search algorithms.
Another way of viewing this paper is that our basic  approach to distributed IR search is to construct a hierarchical overlay network(agent organization) based on the  contentsimilarity measure among agents" document collections in a bottom-up fashion.
Thereafter, these agents route the queries based on the learned policies and update the estimates on the expected utility based on the new routing policies.
In the past work, we have shown that this organization improves search performance significantly.
The goal of the learning algorithm, even though it consumes some network bandwidth, is to shorten the routing time so that more queries are processed per time unit while at the same time finding more relevant documents.
While the content similarity between queries and local nodes  appears to be a creditable indicator for the number of  relevant documents residing on each node, these approaches are limited by a number of factors.
Agents can acquire better search strategies by collecting and analyzing feedback information from previous search sessions.
This contrasts with the content-similarity based approaches where similar operations are repeated for every incoming query and the processing time keeps largely constant over time.
The remainder of this paper is organized as follows:  Section 2 reviews the hierarchical content sharing systems and the two-phase search algorithm based on such topology.
These estimates are  updated gradually by learning from the feedback information returned from previous search sessions.
This process is conducted in an iterative manner.
Section 3 describes a reinforcement learning based approach to direct the routing process; Section 4 details the experimental settings and analyze the results.
First of all,  similaritybased metrics can be myopic since locally relevant nodes may not be connected to other relevant nodes.
Particularly, agents  maintain estimates, namely expected utility, on the downstream agents" capabilities of providing relevant documents for  specific types of incoming queries.
In the absence of global  information, the dominant strategies in addressing this problem are content-similarity based approaches [6, 13, 14, 15].
Based on the  updated expected utility information, the agents derive  corresponding routing policies.
Over the last few years there have been increasing  interests in studying how to control the search processes in peer-to-peer(P2P) based information retrieval(IR) systems [6, 13, 14, 15].
Section 5 discusses related studies and Section 6 concludes the paper. 
