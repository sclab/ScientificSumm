4.3 Results analysis and evaluation Figure 3 demonstrates the ARSS(Average Reward per Search Session) versus the number of incoming queries over time for the the Single-Step based Non-learning Algorithm (SSNA), and the Single-Step Learning Algorithm(SSLA) for data collection TREC-VLC-921.
However, it remains as future work to discover the  correlation between the magnitude of the performance gains and the size of the data collection and/or the extent of the  heterogeneity between the sub-collections. 
The two  different sets of partitions allow us to observe how the  distributed learning algorithm is affected by the homogeneity of the collections.
The average reward for TSLA approach, where learning algorithm is exploited, starts at the same level with the TSNA algorithm and improves the average reward over time until 2000−2500 queries joining the system.
In particular, column TSNA-Random shows the results for dataset TREC-123-100-Random with the TSNA approach.
In our experiments, λ is set as 0.0543 so that the mean of the incoming queries from the environment to the agent network is 50 per time unit.
TRANO is built on top of the Farm [4], a time based distributed simulator that provides a data  dissemination framework for large scale distributed agent network based organizations.
on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 0 50 100 150 200 250 300 350 400 0 500 1000 1500 2000 2500 3000 Cumulativeutility Query number Cumulative utility over the number of incoming queries TSLA-921 SSNA-921 SSLA-921 TSNA-921 Figure 5: The cumulative utility versus the number of search sessions TREC-VLC-921 In addition, we assume that all agents have an equal chance of getting queries from the environment, i.e, λ is the same for every agent.
Once the topology is built, queries randomly selected from the query set 301−350 on TREC-VLC-921 and query set 1− 50 on TREC-123-100-Random and TREC-123-100-Source are injected to the system based on a Poisson distribution P(N(t) = n) = (λt)n n!
Furthermore, TREC-123-100 is split into two sets of 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0 500 1000 1500 2000 2500 3000 ARSS Query number ARSS versus the number of incoming queries for TREC-VLC-921 SSLA-921 SSNA-921 Figure 3: ARSS(Average reward per search  session) versus the number of search sessions for 1phase search in TREC-VLC-921 0 0.1 0.2 0.3 0.4 0.5 0.6 0 500 1000 1500 2000 2500 3000 ARSS Query number ARSS versus query number for TREC-VLC-921 TSLA-921 TSNA-921 Figure 4: ARSS(Average reward per search  session) versus the number of search sessions for 2phase search in TREC-VLC-921 sub-collections in two ways: randomly and by source.
Figure 4 shows the ARSS(Average Reward per Search  Session) versus the number of incoming queries over time for the the Two-Step based Non-learning Algorithm(TSNA), and the Two-Step Learning Algorithm(TSLA) for data  collection TREC-VLC-921.
The first number is the actual  cumulative utility while the second number is the percentage gain in terms of the utility over TSNA approach.
The column TSLA-Random shows the results for dataset TREC-123-100-Random with the TSLA approach.
This is because that the documents in the sub-collection of TREC-123-100-Random are selected  randomly which makes the collection model, the signature of the collection, less meaningful.
Moreover, two-phase learning based algorithm is better than single-phase based learning algorithm because the maximal reward an agent can receive from searching its neighborhood within TTL hops is related to the total  number of the relevant documents in that area.
During the topology generation process, degree information of each agent is estimated by the algorithm introduced by Palmer et al.
It shows that the average reward for SSNA algorithm ranges from 0.02 − 0.06 and the performance changes little over time.
It illustrates that the cumulative  utility of non-learning algorithms increases largely linearly over time, while the gains of learning-based algorithms  accelerate when more queries enter the system.
The average reward for SSLA approach starts at the same level with the SSNA algorithm.
It is  observed that dataset TREC-VLC-921 is more heterogeneous than TREC-123-100 in terms of source, document length, and relevant document distribution from the statistics of the two data collections listed in [13].
The hierarchical agent organization is generated by the algorithm described in our previous algorithm [15].
The results show that the average performance gain for TSLA approach over TNLA approach is 35% after stabilization.
The TSLA combines the merits of both approaches and outperforms them.
Table 1 shows that the performance  improvement for TREC-123-100-Random is not as significant as the other datasets.
Overall, Figures 4, 5, and Table 1 demonstrate that the reinforcement learning based approach can considerably  enhance the system performance for both data collections.
The service time for the communication queue and local search queue, i.e tQij and trs, is set as 0.01 time unit and 0.05 time units  respectively.
Columns TSNA-Source and TSLA-Source show the results for dataset TREC-123-100-Source with TSNA and TSLA approaches respectively.
Figure 5 shows the cumulative utility versus the number of incoming queries over time for SSNA, SSLA,TSNA, and TSLA respectively.
4.1 TRANO Testbed TRANO (Task Routing on Agent Network Organization) is a multi-agent based network based information retrieval testbed.
In distributed information retrieval domain, the two data collections are split to 921 and 100 sub-collections.
In  simulation, each agent is pulsed regularly and the agent checks the incoming message queues, performs local operations and then forwards messages to other agents .
On the contrary, the  two-stepbased learning algorithm can relocate the search session to a neighborhood with more relevant documents.
The five columns show the results for four different approaches.
TRANO supports importation and  exportation of agent organization profiles including topological connections and other features.
Since both algorithms are designed based on the assumption that document collections can be well represented by their collection model, this result is not surprising.
In our experiments, there are ten types of queries acquired by clustering the query set 301 − 350 and 1 − 50.
The following sub-sections introduce the TRANO testbed, the datasets, and the experimental results.
But the performance increases over time and the average performance gain stabilizes at about 25% after query range 2000 − 3000.
Each TRANO agent is  composed of an agent view structure and a control unit.
The documents in each subcollection in dataset TREC-123-100-Source are more  coherent than those in TREC-123-100-Random.
The experiments are conducted on TRANO simulation toolkit with two sets of datasets, TREC-VLC-921 and  TREC123-100.
These experimental results demonstrate that learning-based approaches  consistently perform better than non-learning based routing  algorithm.
4.2 Experimental Settings In our experiment, we use two standard datasets,  TRECVLC-921 and TREC-123-100 datasets, to simulate the  collections hosted on agents.
Table 1 lists the cumulative utility for datasets  TREC123-100-Random and TREC-123-100-Source with  hierarchical organizations.
The TSNA approach has a relatively consistent performance with the average reward ranges from 0.05 − 0.15.
In our experiments, we estimate the upward limit and downward degree limit using linear discount factors 0.5, 0.8 and 1.0.
Thus, even the optimal routing policy can do little beyond reaching these relevant documents faster.
There are two numbers in each cell in the  column TSLA-Random.
Hence, TREC-VLC-921 is much closer to real document distributions in P2P  environments.
The two partitions are denoted as TREC-123-100-Random and TREC-123-100-Source respectively.
The TREC-VLC-921 and  TREC123-100 datasets were created by the U.S. National Institute for Standard Technology(NIST) for its TREC conferences.
[9] with parameters α = 0.5 and β = 0.6.
Joint Conf.
e−λ 236 The Sixth Intl.
