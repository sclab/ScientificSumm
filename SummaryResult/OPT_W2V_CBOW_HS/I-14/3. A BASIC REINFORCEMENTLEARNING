Their neighbors then generate a new routing policy based on the updated observation and in turn they calculate the expected utility based on the new policies and continue this iterative process.
Note that if we want to limit the number of downstream agents for search session sj as 5, the probability of forwarding the query to all neighboring agents should add up to 5.
In the process of propagating reward backward, agents update  estimates of their own potential utility value, generate an  upto-dated policy and pass their updated results to the  neighboring agents based on the algorithm described in Section 3.
In addition, when neighboring agents update their estimates at the same time, oscillation may arise during the learning process[1].
3.1.2 Update Expected Utility Once the routing policy at step n+1, πn+1 i , is determined based on the above formula, agent Ai can update its own  expected utility, Un+1 i (QSi), based on the the updated routing policy resulted from the formula 5 and the updated U values of its neighboring agents.
Instead, agent just have to determine the classification of the query properly and follow the learned policies.
The advantage of this learning algorithm is that once a routing policy is learned, agents do not have to repeatedly compare the similarity of queries as long as the network topology remains unchanged.
The k is introduced based on the idea of a stochastic routing policy and it makes the forwarding probability of the dn i +k highest agent less than 100%.
In particular, at time n, given a set of expected  utility, an agent Ai, whose directly connected agents set is DirectConn(Ai) = {Ai0 , ..., Aim }, determines its  corresponding stochastic routing policy for a search session of state QSj based on the following steps: (1) Ai first selects a subset of agents as the potential downstream agents from set DirectConn(Ai), denoted as PDn(Ai, QSj).
Under the assumption that after a query is forwarded to Ai"s neighbors the subsequent search sessions are independent, the update formula is similar to the Bellman update formula in Q-Learning: Un+1 i (QSj) = (1 − θi) ∗ Un i (QSj) + θi ∗ (Rn+1 i (QSj) + X k πn+1 i (QSj, αik )Un k (QSj)) (7) where QSj = (Qj, ttl − 1) is the next state of QSj = (Qj, ttl); Rn+1 i (QSj) is the expected local reward for query class Qk at agent Ai under the routing policy πn+1 i ; θi is the coefficient for deciding how much weight is given to the old value during the update process: the smaller θi value is, the faster the agent is expected to learn the real value, while the greater volatility of the algorithm, and vice versa.
This is a reverse process of the search results propagation.
This update message includes the potential reward as well as the corresponding state QSi = (qk, ttll) of agent Ai.
In formula 3, the first term on the right of the equation, dn+1 i |P Dn(Ai,QSj )| , is used to to determine the forwarding  probability by equally distributing the forward width, dn+1 i , to the agents in PDn(Ai, QSj) set.
The routing policy πi of agent Ai is stochastic and its outcome for a search session with state QSj is defined as: πi(QSj) = {(αi0 , πi(QSi, αi0 )), (αi1 , πi(QSi, αi1 )), ...} (2) Note that operator πi is overloaded to represent either the probabilistic policy for a search session with state QSj,  denoted as πi(QSj); or the probability of forwarding the query to a specific neighboring agent Aik ∈ DirectConn(Ai)  under the policy πi(QSj), denoted as πi(QSj, αik ).
In contrast, in the two-step search algorithm, the query initiator first attempts to seek a more appropriate starting point for the query by introducing an exploratory step as described in Section 2.
In particular, since the learning process consumes network resources (especially bandwidth), agents can choose to initiate learning only when the network load is relatively low, thus minimizing the extra  communication costs incurred by the learning algorithm.
In this paper, we set the timer to ttimer = ttli ∗ 2 + tf , where ttli ∗ 2 is the sum of the travel time of the queries in the network, and tf is the expected time period that users would like to wait.
(2) For each agent Aik in the PDn(Ai, QSj), the  probability of forwarding the query to Aik is determined in the following way in order to assign higher forwarding  probability to the neighboring agents with higher expected utility value: πn+1 i (QSj, αik ) = dn+1 i |PDn(Ai, QSj)| + β ∗ ` Uik (QSj) − PDU(Ai, QSj) |PDn(Ai, QSj)| ´ (3) where PDUn(Ai, QSj) = X o∈P Dn(Ai,QSj ) Uo(QSj) and QSj is the subsequent state of agent Aik after agent Ai forwards the search session with state QSj to its  neighboring agent Aik ; If QSj = (qk, ttl0), then QSj = (qk, ttl0 − 1).
To speed up the learning process, while updating the  expected utility values of an agent Ai"s neighboring agents we specify that Um(Qk, ttl0) >= Um(Qk, ttl1) iff ttl0 > ttl1 Thus, when agent Ai receives an updated expected utility value with ttl1, it also updates the expected utility values with any ttl0 > ttl1 if Um(Qk, ttl0) < Um(Qk, ttl1) to speed up convergence.
Figure 2: Agent Ai"s Partial Observation about its neighbors(A0, A1...) The load information of Am, Lm, is defined as Lm = |MFm| Cm , where |MFm| is the length of the message-forward queue and Cm is the service rate of agent Am"s message-forward queue.
on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Rn+1 i (QSj) = Rn i (QSj) +γi ∗ (r(QSj) − Rn i (QSj)) ∗ P(qj|Qj ) (8) where r(QSj) is the local reward associated with the search session.
Given the query types set, an incoming query qi can be classified to one query class Q(qi) by the formula: Q(qi) = arg max Qj P(qi|Qj) (1) where P(qi|Qj ) indicates the likelihood that the query qi is generated by the query class Qj [8].
β is  determined according to: β = min ` m − dn+1 i m ∗ umax − PDUn(Ai, QSj) , dn+1 i PDUn(Ai, QSj) − m ∗ umin ´ (4) where m = |PDn(Ai, QSj)|, umax = max o∈P Dn(Ao,QSj ) Uo(QSj) and umin = min o∈P Dn(Ao,QSj ) Uo(QSj) This formula guarantees that the final πn+1 i (QSj, αik ) value is well defined, i.e, 0 ≤ πn+1 i (QSj, αik ) ≤ 1 and X i πn+1 i (QSj, αik ) = dn+1 i However, such a solution does not explore all the  possibilities.
In this  iterative process, agents update their estimates on the potential utility of their current routing policies and then propagate the updated estimates to their neighbors.
Thereafter, upon receipt of a query, in addition to the normal operations  described in the previous section, an agent Ai also sets up a timer to wait for the search results returned from its  downstream agents.
Therefore, equation (2) means that the probability of  forwarding the search session to agent Ai0 is πi(QSi, αi0 ) and so on.
Besides the aforementioned strategy of updating the expected utility values, we also employ an active update strategy where agents notify their  neighbors whenever its expected utility is updated.
This heuristic is based on the fact that the utility of a search session is a non-decreasing function of time t. 3.3 Discussion In formalizing the content routing system as a learning task, many assumptions are made.
Note that based on the characteristics of the queries entering the system and agents" capabilities, the loading of agents may not be uniform.
In the λ-Greedy  approach, in addition to assigning higher probability to those agents with higher expected utility value, as in the equation (3).
on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 233 neighboring agents that agent Ai can forward to at time n. This formula specifies that the potential downstream agent set PDn(Ai, QSj) is either the subset of neighboring agents with dn i + k highest expected utility value for state QSj among all the agents in DirectConn(Ai), or all their neighboring agents.
The Sixth Intl.
Depending on the similarity between a specific query qi and its corresponding query type Qi, the local reward associated with the search session has different impact on the Rn i (QSj) estimation.
They will be compared to the relevance judgment that is provided by the final users (as described in the experiment section, the relevance judgement for the query set is provided along with the data collections).
The size of the potential downstream agent is specified as |PDn(Ai, QSj)| = min(|NEI(Ai), dn i + k)| where k is a constant and is set to 3 in this paper; dn i , the forward width, is defined as the expected number of The Sixth Intl.
The λ is determined according to the following  equation: λn+1 = λ0 ∗ e−c1n (6) where λ0 is the initial exploration rate, which is a  constant; c1 is also a constant to adjust the decreasing rate of the exploration rate; n is the current time unit.
Setting up dn i value properly can improve the utilization rate of the network bandwidth when much of the network is idle while mitigating the traffic load when the network is highly loaded.
BASED SEARCH APPROACH In the aforementioned distributed search algorithm, the routing decisions of an agent Ai rely on the similarity  comparison between incoming queries and Ai"s neighboring agents in order to forward those queries to relevant agents  without flooding the network with unnecessary query messages.
A simple extension would be to keep track of individual load for each neighbor of the agent. 
Note that these detailed results and corresponding agent information will still be stored at Ai until the feedback information is passed from its upstream agent and the performance of its  downstream agents can be evaluated.
The estimated utility information will be used to update Ai"s expected utility for its routing policy.
In the search process, each agent Ai maintains partial  observations of its neighbors" states, as shown in Fig.
On the other hand, in the learning mode, in  parallel with distributed search sessions, agents also participate in a learning process which will be detailed in this section.
The second term is used to adjust the probability of being chosen so that agents with higher expected utility values will be favored.
In our work, the state of a search session sj is stipulated as: QSj = (qk, ttlj) where ttlj is the number of hops that remains for the search session sj , qk is the specific query.
3.1.3 Reward function After a search session stops when its TTL values expires, all search results are returned back to the user and are  compared against the relevance judgment.
The expected utility provides routing guidance for future search sessions.
Specifically, the update formula for dn+1 i is dn+1 i = dn i ∗ (1 + 1 − Li |DirectConn(Ai)| ) In this formula, the forward width is updated based on the traffic conditions of agent Ai"s neighborhood, i.e Li, and its previous value.
3.1 The Model An agent"s routing policy takes the state of a search  session as input and output the routing actions for that query.
The assumption here is that the set of query types is learned ahead of time and belongs to the common knowledge of the agents in the network.
Each neighboring agent, Aj, reacts to this kind of  update message by updating the expected utility value for state QSj(qk, ttll + 1) according to the newly-announced changed expected utility value.
The dn+1 i value is updated based on dn i , the  previous and current observations on the traffic situation in the neighborhood.
Despite the difference in the quality of starting points, the major part of the learning process for the two algorithms is largely the same as described in the following paragraphs.
Agents that appear to be not-so-good choices will also be sent queries based on a dynamic exploration rate.
Upon change of expected utility value, agent Ai sends out its updated utility estimation to its neighbors so that they can act upon the changed expected utility and corresponding state.
The remaining search bandwidth is used for learning by assigning probability λn evenly to agents Ai2 in the set DirectConn(Ai) − PDn(Ai, QSj).
Setting up the timer speeds up the learning because agents can avoid waiting too long for the downstream agents to return search results.
In particular, for agents in the set PDn(Ai, QSj), πn+1 i1 (QSj) is determined in the same way as the above, with the only difference being that dn+1 i is replaced with dn+1 i ∗ (1 − λn).
Before learning starts, each agent initializes the expected utility value for all possible states as 0.
Secondly, the stochastic nature of the routing policy partly remedies this problem.
Load Information Expected Utility For Different Query Types Neighboring Agents ... A0 A1 A3 A2 Un 0 (QS0) ... ... ... ... ...... Un 0 (QS1) Un 1 (QS1) Un 2 (QS1) Un 3 (QS1) Un 1 (QS0) Un 2 (QS0) Un 3 (QS0) Ln 0 Ln 1 Ln 2 Ln 3 ... QS0 QS1 ...
This rationale for setting up such a cut-off value is that the importance of recall ratio decreases with the abundance of relevant documents in real world, therefore users tend to focus on only a limited number of searched results.
This strategy contrasts to the Lazy update, where agents only echo their  neighboring agents with their expected utility change when they exchange information.
where SR is the set of returned search results, Rel(SR) is the set of relevant documents in the search results.
The set of QL can be generated by running a simple online classification algorithm on all the queries that have been processed by the agents, or an oﬄine algorithm on a pre-designated training set.
After collecting the utilization rate information from all its neighbors, agent Ai computes Li as a single measure for assessing the average load  condition of its neighborhood: Li = P k Lk |DirectConn(Ai)| Agents exploit Li value in determining the routing  probability in its routing policy.
Once they complete the update, the agents would again in turn inform related neighbors to  update their values.
Once the timer expires or it has received response from all its downstream agents, Ai merges and  forwards the search results accrued from its downstream agents to its upstream agent.
Agents can choose to initiate and stop learning processes without  affecting the system performance.
An element αij represents an action to route a given query to the neighboring agent Aij ∈ DirectConn(Ai).
3.1.1 Update the Policy An iterative update process is introduced for agents to learn a satisfactory stochastic routing policy.
This may lead to  double counting problem that the relevant documents of some agents will be counted more than once for the state where the TTL value is more than 1.
The partial observation includes non-local information such as the potential utility estimation of its neighbor Am for query state QSj, denoted as Um(QSj), as well as the load  information, Lm.
The reward will be calculated and propagated backward to the agents along the way that search results were passed.
The trade off between the two  approaches is the network load versus learning speed.
The disadvantage of this learning-based approach is that the learning process needs to be conducted whenever the network structure changes.
3.2 Deployment of the Learning algorithm This section describes how the learning algorithm can be used in either a single-phase or a two-phase search process.
This equation specifies that users give 1.0 reward if the number of returned relevant documents reaches a predefined number c. Otherwise, the reward is in proportion to the number of relevant documents returned.
Therefore Lm characterizes the utilization of an agent"s communication channel, and thus provide non-local information for Am"s neighbors to adjust the parameters of their routing policy to avoid inundating their downstream agents.
For example, a single measure is currently used to indicate the traffic load for an agent"s neighborhood.
Note that, as described in Section 3.2, information about neighboring agents is piggybacked with the query message propagated among the agents whenever possible to reduce the traffic overhead.
Note that in the learning protocol, the learning process does not interfere with the distributed search process.
P(qj|Qj ) indicates how relevant the query qj is to the query type Qj, and γi is the learning rate for agent Ai.
In order to balance between exploitation and  exploration, a λ-Greedy approach is taken.
This process goes on until the TTL value in the update message increases to the TTL limit.
In contrast to IP-level based packet routing, the routing decision of each agent for a particular search  session sj depends on the routing history of sj.
The expected utility, Un i (QSj), is used to estimate the  potential utility gain of routing query type QSj to agent Ai under policy πn i .
The details of the actual routing protocol will be  introduced in Section 3.2 when we introduce how the learning algorithm is deployed in real systems.
on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 235 (2) Another challenge for this learning algorithm is that in a real network environment observations on neighboring agents may not be able to be updated in time due to the communication delay or other situations.
Under this stochastic policy, the routing action is  nondeterministic.
In the non-learning mode, agents operate in the same way as they do in the normal distributed search processes described in [14, 15].
With the absense of the cycles, the  estimates inside the tree would be close to the accurate value.
QL is an attribute of qk that indicates which type of queries qk most likely belong to.
This paper explores several approaches to speed up the learning process.
Rn+1 (s) is updated according to the following equation: 234 The Sixth Intl.
However, in the context of the hierarchical agent organizations, two factors mitigate this problems: first, the agents in each content group form a tree-like structure.
These observations are updated periodically by the neighbors.
In this section, we propose a more general approach by framing this problem as a reinforcement learning task.
πn+1 i2 (QSj, αik ) = dn+1 i ∗ λn |DirectConn(Ai) − PDn(Ai, QSj)| (5) where PDn(Ai, QSj) ⊂ DirectConn(Ai).
The advantage of such a strategy is that the best neighboring agents will not be selected repeatedly, thereby mitigating the potential hot spots situations.
Section 3.2 describes a protocol to deploy the learning algorithm in the network.
The superscript n indicates the value at the nth iteration in an iterative learning process.
The duration of the timer is related to the TTL value.
However, this heuristic is myopic because a relevant  direct neighbor is not necessarily connected to other relevant agents.
The set of atomic routing actions of an agent Ai is denoted as {αi}, where {αi} is defined as αi = {αi0 , αi1 , ..., αin }.
Therefore, the assumption that all subsequent search sessions are  independent does not hold in reality.
In the single-phase search algorithm, search sessions start from the initiators of the queries.
Section 3.3 discusses the convergence of the learning  algorithm.
The search results will eventually be returned to the search session initiator A0.
Future work includes exploring how learning can be accomplished when this assumption does not hold.
Thus a faster convergence speed can be achieved.
In the above formula, this impact is reflected by the coefficient, the P(qj|Qj) value.
In real systems, these assumptions may not hold, and thus the learning algorithm may not converge.
There are many potential extensions for this learning model.
Two problems are of particular note, (1) This content routing problem does not have Markov properties.
Note that the exploration rate λ is not a constant and it decreases  overtime.
Assuming the set of search results is SR, the reward Rew(SR) is defined as: Rew(SR) = j 1 if |Rel(SR)| > c |Rel(SR)| c otherwise.
on Autonomous Agents and Multi-Agent Systems (AAMAS 07) a reinforcement learning based model.
The section is structured as follows, Section 3.1 describes 232 The Sixth Intl.
Joint Conf.
Joint Conf.
Joint Conf.
Joint Conf.
In pursuit of greater flexibility, agents can switch between two modes: learning mode and non-learning mode.
