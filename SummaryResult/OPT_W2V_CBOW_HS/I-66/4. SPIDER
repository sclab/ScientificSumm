The expected value for this joint policy is used to prune out the nodes in level 2 (the ones with upper bounds < 234) When creating policies for each non-leaf agent i, SPIDER  potentially performs two steps: 1.
These best response policies from the leaf agents yield an actual expected value of 234 for the complete joint policy.
The brute-force technique for computing an optimal policy would be to examine the expected values for all possible joint policies.
To reiterate this in terms of the agents in DFS tree interaction structure, we assume full  observability for the agents in Tree(i) and for fixed policies of the agents in {Ancestors(i) ∪ i}, we compute the joint value ˆv[πi+ , πi− ] .
The key idea in SPIDER is to avoid  computation of expected values for the entire space of joint policies, by utilizing upper bounds on the expected values of policies and the interaction structure of the agents.
The only exception being when a joint policy is pruned based on the heuristic value.
Obtaining upper bounds and sorting: In this step, agent i computes upper bounds on the expected values, ˆv[πi, πi− ] of the joint policies πi+ corresponding to each of its policy πi and fixed ancestor policies.
SPIDER begins with no policy assigned to any of the agents (shown in the level 1 of the search tree).
We show the proof for l ∈ Ei− and similar reasoning can be adopted to prove for l ∈ Ei+ .
Thus,  exploration of a policy πi yields actual expected value of a joint policy, πi+ represented as v[πi+ , πi− ].
This best response computation for a non-leaf agent i includes: (a)  Sorting of policies (in descending order) based on heuristic values on line 11; (b) Computing best response policies at each of the  children for fixed policies of agent i in lines 16-20; and (c) Maintaining 824 The Sixth Intl.
Exploration in SPIDER-ABS has the same definition as in SPIDER if the policy being explored has a horizon of policy computation which is equal to the actual time horizon and if all the nodes of the policy have an action associated with them (lines 25-30).
For HBA, there are two parts to heuristic computation: (a) Computing the upper bound for the horizon of the abstract  policy.
This  maximum possible reward (for one time step) is obtained by iterating through all the actions of all the agents in Tree(i) and computing the maximum joint reward for any joint action.
on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Figure 3: Example of abstraction for (a) HBA (Horizon Based Abstraction) and (b) NBA (Node Based Abstraction) idea in this technique is to prune out joint a policy if the following condition is satisfied: threshold + > ˆv[πi , πi− ].
Πi and summing two quantities for each policy: (i) the best response for all of i"s children (obtained by  performing steps 1 and 2 at each of the child nodes); (ii) the expected value obtained by i for fixed policies of ancestors.
The  subtree of agents is a distributed POMDP in itself and the idea here is to construct a centralized MDP corresponding to the (sub-tree) distributed POMDP and obtain the expected value of the optimal policy for this centralized MDP.
A policy, πi need not be explored if the upper bound for that policy, ˆv[πi, πi− ] is less than the threshold.
Exploration of these policies (in step 2 below) are performed in this descending order.
πi+ ⇒ joint policy of all agents in Tree(i) ∪ i. πi− ⇒ joint policy of agents that are in Ancestors(i).
We provide an approach to possibly circumvent the exploration of a group of policies based on heuristic computation for one abstract policy, thus leading to an improvement in runtime performance (without loss in solution quality).
Each of this children is of depth ≤ d, and hence from the assumption, the solution quality in the kth child is at least δ 100 v[πk+,∗ , πk− ] for PAX.
Error bound on the solution quality for VAX (implemented over SPIDER-ABS) with an approximation  parameter of is ρ , where ρ is the number of leaf nodes in the DFS tree.
Irrespective of whether l ∈ Ei− or l ∈ Ei+ , ˆrt l is computed by maximizing over all actions of the agents in Tree(i), while rt l is computed for fixed policies of the same agents.
A policy is pruned if the following condition is satisfied: threshold > δ 100 ˆv[πi , πi− ].
Like in VAX, the only difference  between PAX and SPIDER/SPIDER-ABS is this pruning condition.
Akin to some of the algorithms for DCOP [8, 12], SPIDER has a pre-processing step that constructs a DFS tree corresponding to the given interaction structure.
We use the following notation for presenting the equations for computing upper bounds/heuristic values (for agents i and k): Let Ei− denote the set of links between agents in {Ancestors(i)∪ i} and Tree(i), Ei+ denote the set of links between agents in Tree(i).
Also, if l ∈ Ei− , then l1 is the agent in {Ancestors(i)∪ i} and l2 is the agent in Tree(i), that l connects together.
Hence, with VAX a joint policy is pruned at the root agent if ˆv[πroot, πroot− ] < threshvax + ⇒ ˆv[πroot, πroot− ] < threshspider − (( k ρk) − 1) ≤ threshspider − ( k ρk) ≤ threshspider − ρ .
However with PAX, a joint policy is pruned if δ 100 ˆv[πroot, πroot− ] < k δ 100 v[πk+,∗ , πk− ] ⇒ ˆv[πroot, πroot− ] < k v[πk+,∗ , πk− ].
Thus, we need to show that: For l ∈ Ei+ ∪ Ei− : ˆvt l ≥ vt l (refer to notation in Section 4.2) We use mathematical induction on t to prove this.
on Autonomous Agents and Multi-Agent Systems (AAMAS 07) Algorithm 1 SPIDER(i, πi− , threshold) 1: πi+,∗ ← null 2: Πi ← GET-ALL-POLICIES (horizon, Ai, Ωi) 3: if IS-LEAF(i) then 4: for all πi ∈ Πi do 5: v[πi, πi−] ← JOINT-REWARD (πi, πi−) 6: if v[πi, πi−] > threshold then 7: πi+,∗ ← πi 8: threshold ← v[πi, πi−] 9: else 10: children ← CHILDREN (i) 11: ˆΠi ← UPPER-BOUND-SORT(i, Πi, πi−) 12: for all πi ∈ ˆΠi do 13: ˜πi+ ← πi 14: if ˆv[πi, πi−] < threshold then 15: Go to line 12 16: for all j ∈ children do 17: jThres ← threshold − v[πi, πi−]− Σk∈children,k=j ˆvk[πi, πi−] 18: πj+,∗ ← SPIDER(j, πi πi−, jThres) 19: ˜πi+ ← ˜πi+ πj+,∗ 20: ˆvj[πi, πi−] ← v[πj+,∗, πi πi−] 21: if v[˜πi+, πi−] > threshold then 22: threshold ← v[˜πi+, πi−] 23: πi+,∗ ← ˜πi+ 24: return πi+,∗ Algorithm 2 UPPER-BOUND-SORT(i, Πi, πi− ) 1: children ← CHILDREN (i) 2: ˆΠi ← null /* Stores the sorted list */ 3: for all πi ∈ Πi do 4: ˆv[πi, πi−] ← JOINT-REWARD (πi, πi−) 5: for all j ∈ children do 6: ˆvj[πi, πi−] ← UPPER-BOUND(i, j, πi πi−) 7: ˆv[πi, πi−] + ← ˆvj[πi, πi−] 8: ˆΠi ← INSERT-INTO-SORTED (πi, ˆΠi) 9: return ˆΠi best expected value, joint policy in lines 21-23.
4.3 Abstraction Algorithm 5 SPIDER-ABS(i, πi− , threshold) 1: πi+,∗ ← null 2: Πi ← GET-POLICIES (<>, 1) 3: if IS-LEAF(i) then 4: for all πi ∈ Πi do 5: absHeuristic ← GET-ABS-HEURISTIC (πi, πi−) 6: absHeuristic ∗ ← (timeHorizon − πi.horizon) 7: if πi.horizon = timeHorizon and πi.absNodes = 0 then 8: v[πi, πi−] ← JOINT-REWARD (πi, πi−) 9: if v[πi, πi−] > threshold then 10: πi+,∗ ← πi; threshold ← v[πi, πi−] 11: else if v[πi, πi−] + absHeuristic > threshold then 12: ˆΠi ← EXTEND-POLICY (πi, πi.absNodes + 1) 13: Πi + ← INSERT-SORTED-POLICIES ( ˆΠi) 14: REMOVE(πi) 15: else 16: children ← CHILDREN (i) 17: Πi ← UPPER-BOUND-SORT(i, Πi, πi−) 18: for all πi ∈ Πi do 19: ˜πi+ ← πi 20: absHeuristic ← GET-ABS-HEURISTIC (πi, πi−) 21: absHeuristic ∗ ← (timeHorizon − πi.horizon) 22: if πi.horizon = timeHorizon and πi.absNodes = 0 then 23: if ˆv[πi, πi−] < threshold and πi.absNodes = 0 then 24: Go to line 19 25: for all j ∈ children do 26: jThres ← threshold − v[πi, πi−]− Σk∈children,k=j ˆvk[πi, πi−] 27: πj+,∗ ← SPIDER(j, πi πi−, jThres) 28: ˜πi+ ← ˜πi+ πj+,∗; ˆvj[πi, πi−] ← v[πj+,∗, πi πi−] 29: if v[˜πi+, πi−] > threshold then 30: threshold ← v[˜πi+, πi−]; πi+,∗ ← ˜πi+ 31: else if ˆv[πi+, πi−] + absHeuristic > threshold then 32: ˆΠi ← EXTEND-POLICY (πi, πi.absNodes + 1) 33: Πi + ← INSERT-SORTED-POLICIES (ˆΠi) 34: REMOVE(πi) 35: return πi+,∗ In SPIDER, the exploration/pruning phase can only begin after the heuristic (or upper bound) computation and sorting for the  policies has ended.
on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 825 computation of the upper bound on a link l, is as follows: IF l ∈ Ei− , ˆvη−1 l =ˆrη−1 l + max al2 ω η l1 ,s η l ˆpη−1 l1 ˆpη−1 l2 pη−1 u ˆvη l IF l ∈ Ei+ , ˆvη−1 l =ˆrη−1 l + max al1 ,al2 s η l ˆpη−1 l1 ˆpη−1 l2 pη−1 u ˆvη l Algorithm 3 and Algorithm 4 provide the algorithm for computing upper bound for child j of agent i, using the equations descirbed above.
We first compact the standard notation: ot k =Ok(st+1 k , st+1 u , πk(ωt k), ωt+1 k ) (1) pt k =Pk(st k, st u, πk(ωt k), st+1 k ) · ot k pt u =P(st u, st+1 u ) st l = st l1 , st l2 , st u ; ωt l = ωt l1 , ωt l2 rt l =Rl(st l , πl1 (ωt l1 ), πl2 (ωt l2 )) vt l =V t πl (st l , st u, ωt l1 , ωt l2 ) Depending on the location of agent k in the agent tree we have the following cases: IF k ∈ {Ancestors(i) ∪ i}, ˆpt k = pt k, (2) IF k ∈ Tree(i), ˆpt k = Pk(st k, st u, πk(ωt k), st+1 k ) IF l ∈ Ei− , ˆrt l = max {al2 } Rl(st l , πl1 (ωt l1 ), al2 ) IF l ∈ Ei+ , ˆrt l = max {al1 ,al2 } Rl(st l , al1 , al2 ) The value function for an agent i executing the joint policy πi+ at time η − 1 is provided by the equation: V η−1 πi+ (sη−1 , ωη−1 ) = l∈Ei− vη−1 l + l∈Ei+ vη−1 l (3) where vη−1 l = rη−1 l + ω η l ,sη pη−1 l1 pη−1 l2 pη−1 u vη l Algorithm 3 UPPER-BOUND (i, j, πj− ) 1: val ← 0 2: for all l ∈ Ej− ∪ Ej+ do 3: if l ∈ Ej− then πl1 ← φ 4: for all s0 l do 5: val + ← startBel[s0 l ]· UPPER-BOUND-TIME (i, s0 l , j, πl1 , ) 6: return val Algorithm 4 UPPER-BOUND-TIME (i, st l , j, πl1 , ωt l1 ) 1: maxV al ← −∞ 2: for all al1 , al2 do 3: if l ∈ Ei− and l ∈ Ej− then al1 ← πl1 (ωt l1 ) 4: val ← GET-REWARD(st l , al1 , al2 ) 5: if t < πi.horizon − 1 then 6: for all st+1 l , ωt+1 l1 do 7: futV al←pt u ˆpt l1 ˆpt l2 8: futV al ∗ ← UPPER-BOUND-TIME(st+1 l , j, πl1 , ωt l1 ωt+1 l1 ) 9: val + ← futV al 10: if val > maxV al then maxV al ← val 11: return maxV al Upper bound on the expected value for a link is computed by modifying the equation 3 to reflect the full observability  assumption.
The heuristic value function for l ∈ Ei− is provided by the following equation: ˆvη−1 l =ˆrη−1 l + max al2 ω η l1 ,s η l ˆpη−1 l1 ˆpη−1 l2 pη−1 u ˆvη l Rewriting the RHS and using Eqn 2 (in Section 4.2) =ˆrη−1 l + max al2 ω η l1 ,s η l pη−1 u pη−1 l1 ˆpη−1 l2 ˆvη l =ˆrη−1 l + ω η l1 ,s η l pη−1 u pη−1 l1 max al2 ˆpη−1 l2 ˆvη l Since maxal2 ˆpη−1 l2 ˆvη l ≥ ωl2 oη−1 l2 ˆpη−1 l2 ˆvη l and pη−1 l2 = oη−1 l2 ˆpη−1 l2 ≥ˆrη−1 l + ω η l1 ,s η l pη−1 u pη−1 l1 ωl2 pη−1 l2 ˆvη l Since ˆvη l ≥ vη l (from the assumption) ≥ˆrη−1 l + ω η l1 ,s η l pη−1 u pη−1 l1 ωl2 pη−1 l2 vη l Since ˆrη−1 l ≥ rη−1 l (by definition) ≥rη−1 l + ω η l1 ,s η l pη−1 u pη−1 l1 ωl2 pη−1 l2 vη l =rη−1 l + (ω η l ,s η l ) pη−1 u pη−1 l1 pη−1 l2 vη l = vη−1 l Thus proved.
On the other hand, when considering a leaf agent, SPIDER  computes the best response policy (and consequently its expected value) corresponding to fixed policies of its ancestors, πi− .
Level 3 shows one SPIDER search node with a complete joint policy (a policy assigned to each of the agents).
Pruning refers to avoiding exploring all policies (or computing expected values) at agent i by using the current best expected value, vmax [πi+ , πi− ].
The abstraction horizon is gradually increased and these abstract policies are then explored in descending order of heuristic values and ones that have heuristic values less than the threshold are pruned (lines 23-24).
If the number is italicized and underlined, it implies that the actual expected value of the joint policy is provided.
Level 2 of the search tree indicates that the joint policies are sorted based on upper bounds computed for root agent"s policies.
The intuition behind sorting and then exploring policies in descending order of upper bounds, is that the policies with higher upper bounds could yield joint policies with higher expected values.
The important steps in this technique are defining the abstract policy and how heuristic values are computated for the abstract policies.
The policy for the left leaf agent is to perform action East at each time step in the policy, while the policy for the right leaf agent is to perform Off at each time step.
ˆv[πi, πi− ] ⇒ upper bound on the expected value for πi+ given πi and policies of ancestor agents i.e.
Output of this technique is a policy with an expected value that is at least δ% of the optimal solution quality.
This tree governs how the search for the optimal joint  polThe Sixth Intl.
Each of this children is of depth ≤ d, and hence from the assumption, the error introduced in kth child is ρk , where ρk is the number of leaf nodes in kth child of the root.
πroot+ ⇒ joint policy of all agents.
With SPIDER-ABS, a joint policy is pruned at the root agent if ˆv[πroot, πroot− ] < k v[πk+,∗ , πk− ].
v[πi+ , πi− ] ⇒ expected value for πi+ given policies of ancestor agents, πi− .
We prove this proposition using mathematical induction on the depth of the DFS tree.
Therefore, ρ = k ρk, where ρ is the number of leaf nodes in the tree.
It can be noted from the example (just discussed) that this kind of pruning can lead to fewer  explorations and hence lead to an improvement in the overall run-time performance.
This is  accomplished by computing expected values for each of the policies  (corresponding to fixed policies of ancestors) and selecting the highest expected value policy.
This approximation parameter is used at each agent for pruning out joint policies.
For NBA, the heuristic computation is similar to that of a normal policy, except in cases where there is no action associated with policy nodes.
Exploration and Pruning: Exploration implies computing the best response joint policy πi+,∗ corresponding to fixed  ancestor policies of agent i, πi− .
All policies of agent i, Πi are then sorted based on these upper bounds (also referred to as heuristic values henceforth) in descending order.
This is same as the heuristic computation defined by the  GETHEURISTIC() algorithm for SPIDER, however with a shorter time horizon (horizon of the abstract policy).
The sum of these two parts yields an upper bound on the expected value for agent i, and line 8 of the algorithm sorts the policies based on these upper bounds.
It represents a group of longer horizon policies that have the same actions as the abstract policy for times less than or equal to the horizon of the abstract policy.
Thus the best response policy computed by VAX would be at most away from the optimal best response.
ˆvj[πi, πi− ] ⇒ upper bound on the expected value for πi+ from the jth child.
The Sixth Intl.
However, with VAX the threshold at the root agent will be (in the worst case), threshvax = k v[πk+ , πk− ]− k ρk .
A bound on the error introduced by this approximate algorithm as a function of , is provided by Proposition 3.
This  algorithm outputs the best joint policy, πi+,∗ (with an expected value greater than threshold) for the agents in Tree(i).
(b) Computing the maximum possible reward that can be  accumulated in one time step (using GET-ABS-HEURISTIC()) and  multiplying it by the number of time steps to time horizon.
vj[πi+ , πi− ] ⇒ expected value for πi+ from the jth child.
We prove this proposition using mathematical induction on the depth of the DFS tree.
A policy,πk is pruned if ˆv[πk, πk− ] < threshold + .
Sum of (a) and (b) is the heuristic value for a HBA abstract policy.
This type of pruning leads to fewer  explorations and hence an improvement in run-time performance, while potentially leading to a loss in quality of the solution.
As mentioned in Section 3.1, an ND-POMDP can be treated as a DCOP, where the goal is to compute a joint policy that maximizes the overall joint reward.
Increased levels of abstraction leads to faster computation of a complete joint policy, πroot+ and also to shorter heuristic computation and exploration, pruning phases.
Algorithm 2 provides the pseudo code for sorting policies based on the upper bounds on the expected values of joint policies.
4.2 MDP based heuristic function The heuristic function quickly provides an upper bound on the expected value obtainable from the agents in Tree(i).
We now have to prove that the proposition holds for t = η − 1.
For the value provided by the heuristic to be admissible, it should be an over estimate of the expected value for a joint policy.
Thus the best response policy computed by PAX would be at least δ 100 times the optimal best response.
Thus, the equation for the The Sixth Intl.
As proved in Proposition 1, the expected value for a joint policy is always an upper bound.
v[πi, πi− ] ⇒ expected value for πi given policies of ancestor agents, πi− .
absNodes  represents the number of nodes at the last level in the policy tree, that do not have an action assigned to them.
As indicated in the level 2 of the search tree (of Figure 2), all the joint policies are sorted based on the heuristic values, indicated in the top right corner of each joint policy.
We  employ the Maximum Constrained Node (MCN) heuristic used in the DCOP algorithm, ADOPT [8], however other heuristics (such as MLSP heuristic from [7]) can also be employed.
In Figure 3(a), a T=1 abstract policy that performs East action, represents a group of T=2 policies, that perform East in the first time step.
A policy,πk is pruned if δ 100 ˆv[πk, πk− ] < threshold.
Without loss of generality, lets assume that the root node of this tree has k children.
In the example of Figure 2, if the heuristic value for the second joint policy (or second search tree node) in level 2 were 238 instead of 232, then that policy could not be be pruned using SPIDER or SPIDER-Abs.
The policy with the highest  expected value is the best response policy.
For computing optimal joint policy with SPIDER-ABS, a non-leaf agent i initially examines all abstract T=1 policies (line 2) and sorts them based on abstract  policy heuristic computations (line 17).
Since the pruning condition at the root agent in PAX is the same as the one in SPIDER-ABS, there is no error  introduced at the root agent and all the error is introduced in the  children.
Apart from the pruning condition, VAX is the same as SPIDER/SPIDER-ABS.
While Algorithm 4 computes the upper bound on a link given the starting state, Algorithm 3 sums the upper bound values computed over each of the links in Ei− ∪ Ei+ .
Before substituting the abstract policy with a group of policies, those policies are sorted based on heuristic values (line 33).
However, this can entail a sacrifice in the quality of the solution because this technique can prune out a candidate  optimal solution.
EXTEND-POLICY() function is also responsible for  initializing the horizon and absNodes of a policy.
The pruning mechanism in SPIDER and SPIDER-Abs dictates that a joint policy be pruned only if the threshold is exactly greater than the heuristic value.
Without loss of generality, lets assume that the root node of this tree has k children.
Line 4  computes the expected value obtained from ancestors of the agent  (using JOINT-REWARD function), while lines 5-7 compute the  heuristic value from the children.
Assumption: Proposition holds for d, where 1 ≤ depth ≤ d. We now have to prove that the proposition holds for d + 1.
Hence, ˆrt l ≥ rt l and also ˆvt l ≥ vt l .
Figure 2 shows an example search tree for the SPIDER  algorithm, using an example of the three agent chain.
Input to this technique is a  parameter, δ that represents the minimum percentage of the optimal solution quality that is desired.
This is performed by iterating through all policies of agent i i.e.
πi ⇒ policy of the ith agent.
Lines 3-8  compute the best response policy of a leaf agent i, while lines 9-23 computes the best response joint policy for agents in Tree(i).
Node Based Abstraction (NBA): Here an abstract policy is  obtained by not associating actions to certain nodes of the policy tree.
Horizon Based Abstraction (HBA): Here, the abstract policy is defined as a shorter horizon policy.
This is because the expected value for the best joint policy attainable for that policy will be less than the threshold.
The input to this technique is an approximation parameter , which determines the difference from the optimal  solution quality.
However, in VAX with an approximation parameter of 5, the joint policy in consideration would also be pruned.
Note that these DFS trees are pseudo trees [12] that allow links between ancestors and children.
Assumption: Proposition holds for d, where 1 ≤ depth ≤ d. We now have to prove that the proposition holds for d + 1.
This involves removing the observational probability term for agents in Tree(i) and maximizing the future value ˆvη l over the actions of those agents (in Tree(i)).
Heuristic provided using the centralized MDP heuristic is admissible.
Thus, each rounded rectange (search tree node) indicates a partial/complete joint policy, a rectangle indicates an agent and the ovals internal to an agent show its policy.
Hence when a joint policy is pruned, it cannot be an optimal solution.
Figure 2: Execution of SPIDER, an example 4.1 Outline of SPIDER SPIDER is based on the idea of branch and bound search, where the nodes in the search tree represent partial/complete joint  policies.
SPIDER exploits the structure of this DFS tree while engaging in its search.
Hence the proposition holds for the base case.
Heuristic or actual expected value for a joint policy is indicated in the top right corner of the rounded rectangle.
For PAX (implemented over SPIDER-ABS) with an input parameter of δ, the solution quality is at least δ 100 v[πroot+,∗ ], where v[πroot+,∗ ] denotes the optimal solution quality.
Algorithm 5 provides the  algorithm for this abstraction technique.
Hence the proposition holds for the base case.
SPIDER examines all possible joint policies given the interaction structure of the agents.
total number of policy nodes possible at πi.horizon) , then πi.absNodes is set to zero and πi.horizon is increased by 1.
Again in Figure 2, if the heuristic value for the second search tree node in level 2 were 238 instead of 232, then PAX with an  input parameter of 98% would be able to prune that search tree node (since 98 100 ∗238 < 234).
Thus, as long as a candidate optimal policy is not pruned, SPIDER will return an optimal policy.
In this paper, we employ the following notation to denote policies and expected values: Ancestors(i) ⇒ agents from i to the root (not including i).
The algorithms presented in this paper are easily extendable to hyper-trees, however for expository purposes, we assume binary trees.
Before SPIDER begins its search we create a DFS tree (i.e.
Base case: depth = 1 (i.e.
Thus, overall solution quality is at least δ 100 of the optimal solution.
Expected value for an agent i consists of two parts: value obtained from ancestors and value obtained from its children.
Thus, this function combines both HBA and NBA by using the policy variables, horizon and absNodes.
Base case: depth = 1 (i.e.
Best response is  computed by iterating through all policies, Πk.
MCN heuristic tries to place agents with more number of constraints at the top of the tree.
Best response is  computed by iterating through all policies, Πk.
Unlike in HBA, this implies multiple levels of abstraction.
We combine both the abstraction techniques mentioned above into one technique, SPIDER-ABS.
239 would have been greater than the heuristic value for that joint policy (238).
Similar type of abstraction based best response computation is adopted at leaf agents (lines 3-14).
Joint Conf.
Tree(i) ⇒ agents in the sub-tree (not including i) for which i is the root.
An MDP based heuristic is used to compute these upper bounds on the expected values.
SPIDER is an algorithm for centralized planning and distributed execution in distributed POMDPs.
Joint Conf.
pseudo tree) from the three agent chain, with the middle agent as the root of this tree.
Note that in our example figure, each agent is assigned a policy with T=2.
In this paper, we propose two types of abstraction: 1.
Joint Conf.
In Figure 2, SPIDER assigns best response policies to leaf agents at level 3.
However, if those  conditions are not met, then it is substituted by a group of policies that it represents (using EXTEND-POLICY () function) (lines 31-32).
Joint Conf.
4.5 Percentage ApproXimation (PAX) In this section, we present the second approximation  enhancement over SPIDER called PAX.
Joint Conf.
This is because the threshold (234) at that juncture plus the approximation parameter (5), i.e.
4.4 Value ApproXimation (VAX) In this section, we present an approximate enhancement to  SPIDER called VAX.
However, the 826 The Sixth Intl.
Henceforth, this vmax [πi+ , πi− ] will be referred to as threshold.
These incomplete T=2  policies are abstractions for T=2 complete policies.
In such cases, the immediate reward is taken as Rmax (maximum reward for any action).
This is illustrated in Figure 3(b), where there are T=2 policies that do not have an action for observation ‘TP".
Proposition 4 provides the bound on quality loss.
In SPIDER-ABS, threshold at the root agent, thresspider = k v[πk+ , πk− ].
SPIDER provides an optimal solution.
Detailed description about this MDP heuristic is provided in Section 4.2.
Assumption: Proposition holds for t = η, where 1 ≤ η < T − 1.
If πi.absNodes = |Ωi|πi.horizon−1 (i.e.
Hence proved. 
Hence proved.
Algorithm 1 provides the pseudo code for SPIDER.
one node).
one node).
Otherwise, πi.absNodes is increased by 1.
4.6 Theoretical Results PROPOSITION 1.
on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 827 Proof.
PROPOSITION 2.
on Autonomous Agents and Multi-Agent Systems (AAMAS 07) 823 icy proceeds in SPIDER.
PROPOSITION 4.
PROPOSITION 3.
Base case: t = T − 1.
