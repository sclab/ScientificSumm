This algorithm is based on the combination of a classical heuristic search algorithm, Aâˆ— and decentralized control theory.
[4] approximate POSGs as a series of one-step Bayesian games using heuristics to approximate future value,  trading off limited lookahead for computational efficiency, resulting in locally optimal policies (with respect to the selected heuristic).
easier scale-up to larger number of agents); (ii) using branch and bound search with an MDP based heuristic function; (iii) utilizing abstraction to improve  runtime performance without sacrificing solution quality; (iv)  providing a priori percentage bounds on quality of solutions using PAX; and (v) providing expected value bounds on the quality of solutions using VAX.
[11] and Bernstein et al.
The views and  conclusions contained in this document are those of the authors, and should not be interpreted as representing the official policies, either expressed or implied, of the Defense Advanced Research Projects Agency or the U.S. Government. 
This paper presents four algorithms SPIDER, SPIDER-ABS, PAX and VAX that provide a novel combination of features for  policy search in distributed POMDPs: (i) exploiting agent interaction structure given a network of agents (i.e.
The key differences between SPIDER and MAA* are: (a) Enhancements to SPIDER (VAX and PAX) provide for quality guaranteed approximations, while MAA* is a global optimal algorithm and hence involves significant  computational complexity; (b) Due to MAA*"s inability to exploit  interaction structure, it was illustrated only with two agents.
[13] provide an optimal heuristic search method for solving Decentralized POMDPs.
Though all the above techniques improve the efficiency of policy  computation considerably, they are unable to provide error bounds on the quality of the solution.
Peshkin et al.
Researchers have typically employed two types of techniques for solving distributed POMDPs.
This material is based upon work  supported by the Defense Advanced Research Projects Agency (DARPA), through the Department of the Interior, NBC, Acquisition Services Division under Contract No.
Nair et al.
The first set of techniques  compute global optimal solutions.
EmeryMontemerlo et al.
[5] present an  algorithm based on dynamic programming and iterated elimination of dominant policies, that provides optimal solutions for distributed POMDPs.
Szer et al.
[2] are examples of policy search techniques that search for locally optimal policies.
Hansen et al.
The second set of techniques seek approximate policies.
This aspect of quality bounds differentiates SPIDER from all the above techniques.
Experimental results show orders of magnitude  improvement in performance over previous global optimal algorithms.
These features allow for systematic tradeoff of solution quality for run-time in networks of agents operating under  uncertainty.
[9]"s JESP algorithm uses dynamic programming to reach a local optimum solution for finite horizon decentralized POMDPs.
However, SPIDER has been illustrated for networks of agents; and (c)  SPIDER explores the joint policy one agent at a time, while MAA*  expands it one time step at a time (simultaneously for all the agents).
