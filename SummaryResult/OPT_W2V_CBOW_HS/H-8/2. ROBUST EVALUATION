For the confidence estimates to be  accurate, we need at least α · |Zα| of these pairs to actually have ΔMAP = −1 after we have judged every document.
where xm is the set of relevance judgments defined above, Xn is the full set of documents that we wish to estimate the relevance of, and I is some information about the documents (unspecified as of now).
This says that the better the estimates of relevance, the more accurate the evaluation.
By that we mean that if we have made some relevance judgments and have, for example, 75% confidence that system A is better than system B, we would like there to be no more than 25% chance that our  assessment of the relative quality of the systems will change as we continue to judge documents.
This random variable has an  expectation, a variance, confidence intervals, and a certain probability of being less than or equal to a given value.
Let Zα be the subset of pairs in Z for which we predict that ΔMAP = −1 with confidence α given the judgments xm .
Suppose we have a set of m relevance judgments xm = {x1, x2, ..., xm} (using small x rather than capital X to distinguish between judged and unjudged  documents); these are the judgments against which we compute confidence.
To see why this is true, consider a toy example: a list of 3 documents with relevant documents B, C at ranks 1 and 3 and  nonrelevant document A at rank 2.
The probability estimates are entirely indepedent of the measure we are interested in.
Summing over all possibilities, we can compute  expectation and variance: E[AP] ≈ 1 pi aiipi + j>i aij pipj V ar[AP] ≈ 1 ( pi)2 n i a2 iipiqi + j>i a2 ijpipj(1 − pipj) + i=j 2aiiaijpipj(1 − pi) + k>j=i 2aijaikpipjpk(1 − pi) AP asymptotically converges to a normal distribution with expectation and variance as defined above.1 For our comparative evaluation task we are interested in the sign of the difference in two average precisions: ΔAP = AP1 − AP2.
The entropy of a random variable X with distribution p(X) is defined as H(p) = − i p(X = i) log p(X = i).
We argue that the best possible distribution of relevance p(Xi) is the one that explains all of the data (all of the observations made about the retrieval systems) while at the same time making no unwarranted assumptions.
Average precision then becomes the quadratic equation AP = 1 Xi n i=1 Xi/i i j=1 Xj = 1 Xi n i=1 j≥i aijXiXj where aij = 1/ max{r(i), r(j)}.
As described above, we want it to mean that if we continue to judge documents, there will only be a 25% chance that our assessment will change.
P(AP = 0) = 0.2 · 0.6 · 0.3 = 0.036, or P(AP = 1 2 ) = 0.2 · 0.4 · 0.7 = 0.056.
Since AP is normal, ΔAP is normal as well, meaning we can use the normal cumulative density function to  determine the confidence that a difference in AP is less than zero.
Average precision will be 1 2 (1 1 x2 B+ 1 2 xBxA+ 1 3 xBxC + 1 2 x2 A+ 1 3 xAxC + 1 3 x2 C) = 1 2 1 + 2 3 because xA = 0, xB = 1, xC = 1.
All of these are dependent on the probability that  document i is relevant: pi = p(Xi = 1).
It is typically written as the mean precision at the ranks of relevant documents: AP = 1 |R| i∈R prec@r(i) where R is the set of relevant documents and r(i) is the rank of document i.
We can now see average precision itself is a random  variable with a distribution over all possible assignments of  relevance to all documents.
Our calculation of confidence rested on an assumption about the probability of relevance of unjudged documents, specifically that each unjudged document was equally likely to be relevant or nonrelevant.
The following theorem shows the utility of a maximum entropy distribution for relevance when estimating confidence.
Let Z be the set of all pairs of ranked results for a  common set of topics.
MAP is also normally distributed; its expectation and variance are: EMAP = 1 T t∈T E[APt] (1) VMAP = 1 T2 t∈T V ar[APt] ΔMAP = MAP1 − MAP2 Confidence can then be estimated by calculating the  expectation and variance and using the normal density function to find P(ΔMAP < 0).
In our previous work, we defined confidence as the  probability that the difference in an evaluation measure calculated for two systems is less than zero [8].
If this is what it means, we can trust the confidence estimates.
Furthermore, we could assume that the relevance of  documents i and j is independent and achieve the same result, which we state as a corollary: Corollary 1.
Using aij instead of 1/i  allows us to number the documents arbitrarily.
We therefore see confidence as a probability estimate.
Above we gave an intuitive definition of reusability: a collection is reusable if we can trust our estimates of  confidence in an evaluation.
If our confidence estimates are based on unrealistic  assumptions, we cannot expect them to be accurate.
Let Xi be a random variable indicating the relevance of document i.
If they do, we can trust the confidence estimates; our evaluation will be robust to missing judgments.
The theorem implies that the closer we get to the maximum entropy distribution of relevance, the closer we get to robustness. 
As we showed in our previous work, ΔAP has a closed form when documents are ordered arbitrarily: ΔAP = 1 Xi n i=1 j≥i cij XiXj cij = aij − bij where bij is defined analogously to aij for the second  ranking.
This means the same probability estimates can tell us about average precision as well as precision, recall, bpref, etc.
One of the questions we must ask about a probability estimate is what it means.
2.2 Confidence and Robustness Having defined confidence, we turn back to the issue of trust in confidence estimates, and show how it ties into the robustness of the collection to missing judgments.
If documents are ordered by rank, we can express precision as prec@i = 1/i i j=1 Xj .
Since topics are independent, we can easily extend this to mean average precision (MAP).
Other  evaluation tasks could be defined; estimating the magnitude of the difference or the values of the measures themselves are examples that entail different notions of confidence.
As it turns out, it is this assumption that determines whether the  confidence estimates can eb trusted.
The assumptions they are based on are the probabilities of  relevance pi.
This notion of  confidence is defined in the context of a particular evaluation task that we call comparative evaluation: determining the sign of the difference in an evaluation measure.
1 These are actually approximations to the true expectation and variance, but the error is a negligible O(n2−n ).
2.1 Estimating Confidence Average precision (AP) is a standard evaluation metric that captures both the ability of a system to rank relevant documents highly (precision) as well as its ability to retrieve relevant documents (recall).
The task therefore becomes the imputation of the missing values of relevance.
The task of creating a reusable test collection thus becomes the task of estimating the  relevance of unjudged documents.
Though the ordering B, A, C is different from the labeling A, B, C, it does not affect the computation.
What does it mean to have 75% confidence that system A is better than system B?
If p(Xi|I, xm ) = argmaxpH(p), confidence estimates will be accurate.
Before elaborating on this, we formally define confidence.
But do we know it has that meaning?
The maximum entropy distribution is the one that maximizes H. This distribution is unique and has an exponential form.
We forgo the proof for the time being, but it is quite simple.
If p(Xn |I, xm ) = argmaxpH(p), confidence estimates will be accurate.
We need these to be realistic.
The theorem and its proof say nothing whatsoever about the evaluation metric.
We can then compute e.g.
Our evaluation should be robust to missing judgments.
This is known as the principle of maximum entropy [13].
Suppose in our  previous example we do not know the relevance judgments, but we believe pA = 0.4, pB = 0.8, pC = 0.7.
This has found a wide array of uses in computer science and information retrieval.
This assumption is almost certainly not realistic in most IR applications.
Theorem 1.
