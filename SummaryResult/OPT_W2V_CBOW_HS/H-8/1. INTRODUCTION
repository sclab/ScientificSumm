Evaluation is an important aspect of information retrieval research, but it is only a semi-solved problem: for most retrieval tasks, it is impossible to judge the relevance of every document; there are simply too many of them.
Research groups would be able to share the relevance judgments they have done in-house for pilot studies, new tasks, or new topics.
This method creates large sets of judgments that are reusable for training or evaluating new systems that did not contribute to the pool [21].
Specifically, the question of reusability is not how  accurately we can evaluate new systems.
While we can say that it was right 75% of the time, or that it had a rank correlation of 0.8, these numbers do not have any  predictive power: they do not tell us which systems are likely to be wrong or how confident we should be in any one.
In previous work,  reusability has been tested by simply assessing the accuracy of a set of relevance judgments at evaluating unseen systems.
Even if confidence is not high, as long as we can trust it, we can identify which systems need more  judgments in order to increase confidence.
Can they evaluate without more relevance judgments?
Small, reusable test collections could have a huge impact on information retrieval research.
[4], among others.
As we will see, the judgments these methods produce can significantly bias the evaluation of a new set of systems.
Consider an information retrieval researcher who has  invented a new retrieval task.
The real question is how much confidence we have in our evaluations, and, more importantly, whether we can trust our estimates of confidence.
She can only judge the documents that seem to be the most  informative and stop when she has a reasonable degree of confidence in her conclusions.
She has built a system to  perform the task and wants to evaluate it.
Or another research group decides to implement a system to perform the task?
The solution used by NIST at TREC (Text REtrieval Conference) is the pooling method [19, 20]: all competing systems contribute N documents to a pool, and every document in that pool is judged.
We need a more careful definition of reusability.
As a result, there have been a slew of recent papers on reducing annotator effort in producing test collections: Cormack et al.
But what happens when she develops a new system and needs to evaluate it?
[8], and Aslam et al.
Since the task is new, it is unlikely that there are any extant relevance  judgments.
Any set of judgments, no matter how small, becomes reusable to some degree.
A malicious  adversary can always produce a new ranked list that has not retrieved any of the judged documents.
The pooling method gives thousands of relevance judgments, but it requires many hours of (paid) annotator time.
She does not have the time or resources to judge every document, or even every retrieved document.
[11], Zobel [21], Sanderson and Joho [17], Carterette et al.
Returning to our hypothetical resesarcher, can she reuse her relevance judgments?
Can they reliably reuse the original judgments?
This solution is not adequate for our hypothetical  researcher.
First we must formally define what it means to be reusable.
The amount of data available to researchers would grow  exponentially over time. 
